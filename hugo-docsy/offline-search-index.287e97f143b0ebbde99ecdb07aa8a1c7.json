





































































































































































































































[{"body":" This is a placeholder page that shows you how to use this template site.\n The Overview is where your users find out about your project. Depending on the size of your docset, you can have a separate overview page (like this one) or put your overview contents in the Documentation landing page (like in the Docsy User Guide).\nTry answering these questions for your user in this page:\nWhat is it? Introduce your project, including what it does or lets you do, why you would use it, and its primary goal (and how it achieves it). This should be similar to your README description, though you can go into a little more detail here if you want.\nWhy do I want it? Help your user know if your project will help them. Useful information can include:\n  What is it good for?: What types of problems does your project solve? What are the benefits of using it?\n  What is it not good for?: For example, point out situations that might intuitively seem suited for your project, but aren’t for some reason. Also mention known limitations, scaling issues, or anything else that might let your users know if the project is not for them.\n  What is it not yet good for?: Highlight any useful features that are coming soon.\n  Where should I go next? Give your users next steps from the Overview. For example:\n Getting Started: Get started with $project Examples: Check out some example code!  ","categories":"","description":"Here's where your user finds out if your project is for them.\n","excerpt":"Here's where your user finds out if your project is for them.\n","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/overview/","tags":"","title":"Overview"},{"body":" This is a placeholder page. Replace it with your own content.\n Text can be bold, italic, or strikethrough. Links should be blue with no underlines (unless hovered over).\nThere should be whitespace between paragraphs. Vape migas chillwave sriracha poutine try-hard distillery. Tattooed shabby chic small batch, pabst art party heirloom letterpress air plant pop-up. Sustainable chia skateboard art party banjo cardigan normcore affogato vexillologist quinoa meggings man bun master cleanse shoreditch readymade. Yuccie prism four dollar toast tbh cardigan iPhone, tumblr listicle live-edge VHS. Pug lyft normcore hot chicken biodiesel, actually keffiyeh thundercats photo booth pour-over twee fam food truck microdosing banh mi. Vice activated charcoal raclette unicorn live-edge post-ironic. Heirloom vexillologist coloring book, beard deep v letterpress echo park humblebrag tilde.\n90’s four loko seitan photo booth gochujang freegan tumeric listicle fam ugh humblebrag. Bespoke leggings gastropub, biodiesel brunch pug fashion axe meh swag art party neutra deep v chia. Enamel pin fanny pack knausgaard tofu, artisan cronut hammock meditation occupy master cleanse chartreuse lumbersexual. Kombucha kogi viral truffaut synth distillery single-origin coffee ugh slow-carb marfa selfies. Pitchfork schlitz semiotics fanny pack, ugh artisan vegan vaporware hexagon. Polaroid fixie post-ironic venmo wolf ramps kale chips.\n There should be no margin above this first sentence.\nBlockquotes should be a lighter gray with a border along the left side in the secondary color.\nThere should be no margin below this final sentence.\n First Header 2 This is a normal paragraph following a header. Knausgaard kale chips snackwave microdosing cronut copper mug swag synth bitters letterpress glossier craft beer. Mumblecore bushwick authentic gochujang vegan chambray meditation jean shorts irony. Viral farm-to-table kale chips, pork belly palo santo distillery activated charcoal aesthetic jianbing air plant woke lomo VHS organic. Tattooed locavore succulents heirloom, small batch sriracha echo park DIY af. Shaman you probably haven’t heard of them copper mug, crucifix green juice vape single-origin coffee brunch actually. Mustache etsy vexillologist raclette authentic fam. Tousled beard humblebrag asymmetrical. I love turkey, I love my job, I love my friends, I love Chardonnay!\nDeae legum paulatimque terra, non vos mutata tacet: dic. Vocant docuique me plumas fila quin afuerunt copia haec o neque.\nOn big screens, paragraphs and headings should not take up the full container width, but we want tables, code blocks and similar to take the full width.\nScenester tumeric pickled, authentic crucifix post-ironic fam freegan VHS pork belly 8-bit yuccie PBR\u0026B. I love this life we live in.\nSecond Header 2  This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\n Header 3 This is a code block following a header.  Next level leggings before they sold out, PBR\u0026B church-key shaman echo park. Kale chips occupy godard whatever pop-up freegan pork belly selfies. Gastropub Belinda subway tile woke post-ironic seitan. Shabby chic man bun semiotics vape, chia messenger bag plaid cardigan.\nHeader 4  This is an unordered list following a header. This is an unordered list following a header. This is an unordered list following a header.  Header 5  This is an ordered list following a header. This is an ordered list following a header. This is an ordered list following a header.  Header 6    What Follows     A table A header   A table A header   A table A header     There’s a horizontal rule above and below this.\n Here is an unordered list:\n Liverpool F.C. Chelsea F.C. Manchester United F.C.  And an ordered list:\n Michael Brecker Seamus Blake Branford Marsalis  And an unordered task list:\n Create a Hugo theme Add task lists to it Take a vacation  And a “mixed” task list:\n Pack bags ? Travel!  And a nested list:\n Jackson 5  Michael Tito Jackie Marlon Jermaine   TMNT  Leonardo Michelangelo Donatello Raphael    Definition lists can be used with Markdown syntax. Definition headers are bold.\n Name Godzilla Born 1952 Birthplace Japan Color Green   Tables should have bold headings and alternating shaded rows.\n   Artist Album Year     Michael Jackson Thriller 1982   Prince Purple Rain 1984   Beastie Boys License to Ill 1986    If a table is too wide, it should scroll horizontally.\n   Artist Album Year Label Awards Songs     Michael Jackson Thriller 1982 Epic Records Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical Wanna Be Startin' Somethin', Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life   Prince Purple Rain 1984 Warner Brothers Records Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal Let’s Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I’m a Star, Purple Rain   Beastie Boys License to Ill 1986 Mercury Records noawardsbutthistablecelliswide Rhymin \u0026 Stealin, The New Style, She’s Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill     Code snippets like var foo = \"bar\"; can be shown inline.\nAlso, this should vertically align with this and this.\nCode can also be shown in a block element.\nfoo := \"bar\"; bar := \"foo\";  Code can also use syntax highlighting.\nfunc main() { input := `var foo = \"bar\";` lexer := lexers.Get(\"javascript\") iterator, _ := lexer.Tokenise(nil, input) style := styles.Get(\"github\") formatter := html.New(html.WithLineNumbers()) var buff bytes.Buffer formatter.Format(\u0026buff, style, iterator) fmt.Println(buff.String()) }  Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.  Inline code inside table cells should still be distinguishable.\n   Language Code     Javascript var foo = \"bar\";   Ruby foo = \"bar\"{     Small images should be shown at their actual size.\nLarge images should always scale down and fit in the content container.\nThe photo above of the Spruce Picea abies shoot with foliage buds: Bjørn Erik Pedersen, CC-BY-SA.\nComponents Alerts  This is an alert.  Note This is an alert with a title.  Note This is an alert with a title and Markdown.  This is a successful alert.  This is a warning.  Warning This is a warning with a title.  Another Heading Add some sections here to see how the ToC looks like. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nThis Document Inguina genus: Anaphen post: lingua violente voce suae meus aetate diversi. Orbis unam nec flammaeque status deam Silenum erat et a ferrea. Excitus rigidum ait: vestro et Herculis convicia: nitidae deseruit coniuge Proteaque adiciam eripitur? Sitim noceat signa probat quidem. Sua longis fugatis quidem genae.\nPixel Count Tilde photo booth wayfarers cliche lomo intelligentsia man braid kombucha vaporware farm-to-table mixtape portland. PBR\u0026B pickled cornhole ugh try-hard ethical subway tile. Fixie paleo intelligentsia pabst. Ennui waistcoat vinyl gochujang. Poutine salvia authentic affogato, chambray lumbersexual shabby chic.\nContact Info Plaid hell of cred microdosing, succulents tilde pour-over. Offal shabby chic 3 wolf moon blue bottle raw denim normcore poutine pork belly.\nExternal Links Stumptown PBR\u0026B keytar plaid street art, forage XOXO pitchfork selvage affogato green juice listicle pickled everyday carry hashtag. Organic sustainable letterpress sartorial scenester intelligentsia swag bushwick. Put a bird on it stumptown neutra locavore. IPhone typewriter messenger bag narwhal. Ennui cold-pressed seitan flannel keytar, single-origin coffee adaptogen occupy yuccie williamsburg chillwave shoreditch forage waistcoat.\nThis is the final element on the page and there should be no margin below this.  ","categories":"","description":"A short lead description about this content page. It can be **bold** or _italic_ and can be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. It can be **bold** …","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/tasks/beds/","tags":"","title":"Bed and Chair Metrics"},{"body":" This is a placeholder page. Replace it with your own content.\n Text can be bold, italic, or strikethrough. Links should be blue with no underlines (unless hovered over).\nThere should be whitespace between paragraphs. Vape migas chillwave sriracha poutine try-hard distillery. Tattooed shabby chic small batch, pabst art party heirloom letterpress air plant pop-up. Sustainable chia skateboard art party banjo cardigan normcore affogato vexillologist quinoa meggings man bun master cleanse shoreditch readymade. Yuccie prism four dollar toast tbh cardigan iPhone, tumblr listicle live-edge VHS. Pug lyft normcore hot chicken biodiesel, actually keffiyeh thundercats photo booth pour-over twee fam food truck microdosing banh mi. Vice activated charcoal raclette unicorn live-edge post-ironic. Heirloom vexillologist coloring book, beard deep v letterpress echo park humblebrag tilde.\n90’s four loko seitan photo booth gochujang freegan tumeric listicle fam ugh humblebrag. Bespoke leggings gastropub, biodiesel brunch pug fashion axe meh swag art party neutra deep v chia. Enamel pin fanny pack knausgaard tofu, artisan cronut hammock meditation occupy master cleanse chartreuse lumbersexual. Kombucha kogi viral truffaut synth distillery single-origin coffee ugh slow-carb marfa selfies. Pitchfork schlitz semiotics fanny pack, ugh artisan vegan vaporware hexagon. Polaroid fixie post-ironic venmo wolf ramps kale chips.\n There should be no margin above this first sentence.\nBlockquotes should be a lighter gray with a border along the left side in the secondary color.\nThere should be no margin below this final sentence.\n First Header 2 This is a normal paragraph following a header. Knausgaard kale chips snackwave microdosing cronut copper mug swag synth bitters letterpress glossier craft beer. Mumblecore bushwick authentic gochujang vegan chambray meditation jean shorts irony. Viral farm-to-table kale chips, pork belly palo santo distillery activated charcoal aesthetic jianbing air plant woke lomo VHS organic. Tattooed locavore succulents heirloom, small batch sriracha echo park DIY af. Shaman you probably haven’t heard of them copper mug, crucifix green juice vape single-origin coffee brunch actually. Mustache etsy vexillologist raclette authentic fam. Tousled beard humblebrag asymmetrical. I love turkey, I love my job, I love my friends, I love Chardonnay!\nDeae legum paulatimque terra, non vos mutata tacet: dic. Vocant docuique me plumas fila quin afuerunt copia haec o neque.\nOn big screens, paragraphs and headings should not take up the full container width, but we want tables, code blocks and similar to take the full width.\nScenester tumeric pickled, authentic crucifix post-ironic fam freegan VHS pork belly 8-bit yuccie PBR\u0026B. I love this life we live in.\nSecond Header 2  This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\n Header 3 This is a code block following a header.  Next level leggings before they sold out, PBR\u0026B church-key shaman echo park. Kale chips occupy godard whatever pop-up freegan pork belly selfies. Gastropub Belinda subway tile woke post-ironic seitan. Shabby chic man bun semiotics vape, chia messenger bag plaid cardigan.\nHeader 4  This is an unordered list following a header. This is an unordered list following a header. This is an unordered list following a header.  Header 5  This is an ordered list following a header. This is an ordered list following a header. This is an ordered list following a header.  Header 6    What Follows     A table A header   A table A header   A table A header     There’s a horizontal rule above and below this.\n Here is an unordered list:\n Liverpool F.C. Chelsea F.C. Manchester United F.C.  And an ordered list:\n Michael Brecker Seamus Blake Branford Marsalis  And an unordered task list:\n Create a Hugo theme Add task lists to it Take a vacation  And a “mixed” task list:\n Pack bags ? Travel!  And a nested list:\n Jackson 5  Michael Tito Jackie Marlon Jermaine   TMNT  Leonardo Michelangelo Donatello Raphael    Definition lists can be used with Markdown syntax. Definition headers are bold.\n Name Godzilla Born 1952 Birthplace Japan Color Green   Tables should have bold headings and alternating shaded rows.\n   Artist Album Year     Michael Jackson Thriller 1982   Prince Purple Rain 1984   Beastie Boys License to Ill 1986    If a table is too wide, it should scroll horizontally.\n   Artist Album Year Label Awards Songs     Michael Jackson Thriller 1982 Epic Records Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical Wanna Be Startin' Somethin', Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life   Prince Purple Rain 1984 Warner Brothers Records Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal Let’s Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I’m a Star, Purple Rain   Beastie Boys License to Ill 1986 Mercury Records noawardsbutthistablecelliswide Rhymin \u0026 Stealin, The New Style, She’s Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill     Code snippets like var foo = \"bar\"; can be shown inline.\nAlso, this should vertically align with this and this.\nCode can also be shown in a block element.\nfoo := \"bar\"; bar := \"foo\";  Code can also use syntax highlighting.\nfunc main() { input := `var foo = \"bar\";` lexer := lexers.Get(\"javascript\") iterator, _ := lexer.Tokenise(nil, input) style := styles.Get(\"github\") formatter := html.New(html.WithLineNumbers()) var buff bytes.Buffer formatter.Format(\u0026buff, style, iterator) fmt.Println(buff.String()) }  Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.  Inline code inside table cells should still be distinguishable.\n   Language Code     Javascript var foo = \"bar\";   Ruby foo = \"bar\"{     Small images should be shown at their actual size.\nLarge images should always scale down and fit in the content container.\nThe photo above of the Spruce Picea abies shoot with foliage buds: Bjørn Erik Pedersen, CC-BY-SA.\nComponents Alerts  This is an alert.  Note This is an alert with a title.  Note This is an alert with a title and Markdown.  This is a successful alert.  This is a warning.  Warning This is a warning with a title.  Another Heading Add some sections here to see how the ToC looks like. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nThis Document Inguina genus: Anaphen post: lingua violente voce suae meus aetate diversi. Orbis unam nec flammaeque status deam Silenum erat et a ferrea. Excitus rigidum ait: vestro et Herculis convicia: nitidae deseruit coniuge Proteaque adiciam eripitur? Sitim noceat signa probat quidem. Sua longis fugatis quidem genae.\nPixel Count Tilde photo booth wayfarers cliche lomo intelligentsia man braid kombucha vaporware farm-to-table mixtape portland. PBR\u0026B pickled cornhole ugh try-hard ethical subway tile. Fixie paleo intelligentsia pabst. Ennui waistcoat vinyl gochujang. Poutine salvia authentic affogato, chambray lumbersexual shabby chic.\nContact Info Plaid hell of cred microdosing, succulents tilde pour-over. Offal shabby chic 3 wolf moon blue bottle raw denim normcore poutine pork belly.\nExternal Links Stumptown PBR\u0026B keytar plaid street art, forage XOXO pitchfork selvage affogato green juice listicle pickled everyday carry hashtag. Organic sustainable letterpress sartorial scenester intelligentsia swag bushwick. Put a bird on it stumptown neutra locavore. IPhone typewriter messenger bag narwhal. Ennui cold-pressed seitan flannel keytar, single-origin coffee adaptogen occupy yuccie williamsburg chillwave shoreditch forage waistcoat.\nThis is the final element on the page and there should be no margin below this.  ","categories":"","description":"A short lead description about this content page. It can be **bold** or _italic_ and can be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. It can be **bold** …","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/tasks/ponycopters/configuring-ponycopters/","tags":"","title":"Configuring Ponycopters"},{"body":" This is a placeholder page that shows you how to use this template site.\n Information in this section helps your user try your project themselves.\n  What do your users need to do to start using your project? This could include downloading/installation instructions, including any prerequisites or system requirements.\n  Introductory “Hello World” example, if appropriate. More complex tutorials should live in the Tutorials section.\n  Consider using the headings below for your getting started page. You can delete any that are not applicable to your project.\nPrerequisites Are there any system requirements for using your project? What languages are supported (if any)? Do users need to already have any software or tools installed?\nInstallation Where can your user find your project code? How can they install it (binaries, installable package, build from source)? Are there multiple options/versions they can install and how should they choose the right one for them?\nSetup Is there any initial setup users need to do after installation to try your project?\nTry it out! Can your users test their installation, for example by running a command or deploying a Hello World example?\n","categories":"","description":"What does your user need to know to try your project?\n","excerpt":"What does your user need to know to try your project?\n","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/getting-started/","tags":"","title":"Getting Started"},{"body":" This is a placeholder page that shows you how to use this template site.\n Do you have any example applications or code for your users in your repo or elsewhere? Link to your examples here.\n","categories":"","description":"See your project in action!\n","excerpt":"See your project in action!\n","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/examples/","tags":"","title":"Examples"},{"body":" This is a placeholder page. Replace it with your own content.\n Text can be bold, italic, or strikethrough. Links should be blue with no underlines (unless hovered over).\nThere should be whitespace between paragraphs. Vape migas chillwave sriracha poutine try-hard distillery. Tattooed shabby chic small batch, pabst art party heirloom letterpress air plant pop-up. Sustainable chia skateboard art party banjo cardigan normcore affogato vexillologist quinoa meggings man bun master cleanse shoreditch readymade. Yuccie prism four dollar toast tbh cardigan iPhone, tumblr listicle live-edge VHS. Pug lyft normcore hot chicken biodiesel, actually keffiyeh thundercats photo booth pour-over twee fam food truck microdosing banh mi. Vice activated charcoal raclette unicorn live-edge post-ironic. Heirloom vexillologist coloring book, beard deep v letterpress echo park humblebrag tilde.\n90’s four loko seitan photo booth gochujang freegan tumeric listicle fam ugh humblebrag. Bespoke leggings gastropub, biodiesel brunch pug fashion axe meh swag art party neutra deep v chia. Enamel pin fanny pack knausgaard tofu, artisan cronut hammock meditation occupy master cleanse chartreuse lumbersexual. Kombucha kogi viral truffaut synth distillery single-origin coffee ugh slow-carb marfa selfies. Pitchfork schlitz semiotics fanny pack, ugh artisan vegan vaporware hexagon. Polaroid fixie post-ironic venmo wolf ramps kale chips.\n There should be no margin above this first sentence.\nBlockquotes should be a lighter gray with a border along the left side in the secondary color.\nThere should be no margin below this final sentence.\n First Header 2 This is a normal paragraph following a header. Knausgaard kale chips snackwave microdosing cronut copper mug swag synth bitters letterpress glossier craft beer. Mumblecore bushwick authentic gochujang vegan chambray meditation jean shorts irony. Viral farm-to-table kale chips, pork belly palo santo distillery activated charcoal aesthetic jianbing air plant woke lomo VHS organic. Tattooed locavore succulents heirloom, small batch sriracha echo park DIY af. Shaman you probably haven’t heard of them copper mug, crucifix green juice vape single-origin coffee brunch actually. Mustache etsy vexillologist raclette authentic fam. Tousled beard humblebrag asymmetrical. I love turkey, I love my job, I love my friends, I love Chardonnay!\nDeae legum paulatimque terra, non vos mutata tacet: dic. Vocant docuique me plumas fila quin afuerunt copia haec o neque.\nOn big screens, paragraphs and headings should not take up the full container width, but we want tables, code blocks and similar to take the full width.\nScenester tumeric pickled, authentic crucifix post-ironic fam freegan VHS pork belly 8-bit yuccie PBR\u0026B. I love this life we live in.\nSecond Header 2  This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\n Header 3 This is a code block following a header.  Next level leggings before they sold out, PBR\u0026B church-key shaman echo park. Kale chips occupy godard whatever pop-up freegan pork belly selfies. Gastropub Belinda subway tile woke post-ironic seitan. Shabby chic man bun semiotics vape, chia messenger bag plaid cardigan.\nHeader 4  This is an unordered list following a header. This is an unordered list following a header. This is an unordered list following a header.  Header 5  This is an ordered list following a header. This is an ordered list following a header. This is an ordered list following a header.  Header 6    What Follows     A table A header   A table A header   A table A header     There’s a horizontal rule above and below this.\n Here is an unordered list:\n Liverpool F.C. Chelsea F.C. Manchester United F.C.  And an ordered list:\n Michael Brecker Seamus Blake Branford Marsalis  And an unordered task list:\n Create a Hugo theme Add task lists to it Take a vacation  And a “mixed” task list:\n Pack bags ? Travel!  And a nested list:\n Jackson 5  Michael Tito Jackie Marlon Jermaine   TMNT  Leonardo Michelangelo Donatello Raphael    Definition lists can be used with Markdown syntax. Definition headers are bold.\n Name Godzilla Born 1952 Birthplace Japan Color Green   Tables should have bold headings and alternating shaded rows.\n   Artist Album Year     Michael Jackson Thriller 1982   Prince Purple Rain 1984   Beastie Boys License to Ill 1986    If a table is too wide, it should scroll horizontally.\n   Artist Album Year Label Awards Songs     Michael Jackson Thriller 1982 Epic Records Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical Wanna Be Startin' Somethin', Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life   Prince Purple Rain 1984 Warner Brothers Records Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal Let’s Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I’m a Star, Purple Rain   Beastie Boys License to Ill 1986 Mercury Records noawardsbutthistablecelliswide Rhymin \u0026 Stealin, The New Style, She’s Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill     Code snippets like var foo = \"bar\"; can be shown inline.\nAlso, this should vertically align with this and this.\nCode can also be shown in a block element.\nfoo := \"bar\"; bar := \"foo\";  Code can also use syntax highlighting.\nfunc main() { input := `var foo = \"bar\";` lexer := lexers.Get(\"javascript\") iterator, _ := lexer.Tokenise(nil, input) style := styles.Get(\"github\") formatter := html.New(html.WithLineNumbers()) var buff bytes.Buffer formatter.Format(\u0026buff, style, iterator) fmt.Println(buff.String()) }  Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.  Inline code inside table cells should still be distinguishable.\n   Language Code     Javascript var foo = \"bar\";   Ruby foo = \"bar\"{     Small images should be shown at their actual size.\nLarge images should always scale down and fit in the content container.\nThe photo above of the Spruce Picea abies shoot with foliage buds: Bjørn Erik Pedersen, CC-BY-SA.\nComponents Alerts  This is an alert.  Note This is an alert with a title.  Note This is an alert with a title and Markdown.  This is a successful alert.  This is a warning.  Warning This is a warning with a title.  Another Heading Add some sections here to see how the ToC looks like. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nThis Document Inguina genus: Anaphen post: lingua violente voce suae meus aetate diversi. Orbis unam nec flammaeque status deam Silenum erat et a ferrea. Excitus rigidum ait: vestro et Herculis convicia: nitidae deseruit coniuge Proteaque adiciam eripitur? Sitim noceat signa probat quidem. Sua longis fugatis quidem genae.\nPixel Count Tilde photo booth wayfarers cliche lomo intelligentsia man braid kombucha vaporware farm-to-table mixtape portland. PBR\u0026B pickled cornhole ugh try-hard ethical subway tile. Fixie paleo intelligentsia pabst. Ennui waistcoat vinyl gochujang. Poutine salvia authentic affogato, chambray lumbersexual shabby chic.\nContact Info Plaid hell of cred microdosing, succulents tilde pour-over. Offal shabby chic 3 wolf moon blue bottle raw denim normcore poutine pork belly.\nExternal Links Stumptown PBR\u0026B keytar plaid street art, forage XOXO pitchfork selvage affogato green juice listicle pickled everyday carry hashtag. Organic sustainable letterpress sartorial scenester intelligentsia swag bushwick. Put a bird on it stumptown neutra locavore. IPhone typewriter messenger bag narwhal. Ennui cold-pressed seitan flannel keytar, single-origin coffee adaptogen occupy yuccie williamsburg chillwave shoreditch forage waistcoat.\nThis is the final element on the page and there should be no margin below this.  ","categories":"","description":"A short lead description about this content page. It can be **bold** or _italic_ and can be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. It can be **bold** …","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/tasks/ponycopters/launching-ponycopters/","tags":"","title":"Launching Ponycopters"},{"body":" This is a placeholder page. Replace it with your own content.\n Text can be bold, italic, or strikethrough. Links should be blue with no underlines (unless hovered over).\nThere should be whitespace between paragraphs. Vape migas chillwave sriracha poutine try-hard distillery. Tattooed shabby chic small batch, pabst art party heirloom letterpress air plant pop-up. Sustainable chia skateboard art party banjo cardigan normcore affogato vexillologist quinoa meggings man bun master cleanse shoreditch readymade. Yuccie prism four dollar toast tbh cardigan iPhone, tumblr listicle live-edge VHS. Pug lyft normcore hot chicken biodiesel, actually keffiyeh thundercats photo booth pour-over twee fam food truck microdosing banh mi. Vice activated charcoal raclette unicorn live-edge post-ironic. Heirloom vexillologist coloring book, beard deep v letterpress echo park humblebrag tilde.\n90’s four loko seitan photo booth gochujang freegan tumeric listicle fam ugh humblebrag. Bespoke leggings gastropub, biodiesel brunch pug fashion axe meh swag art party neutra deep v chia. Enamel pin fanny pack knausgaard tofu, artisan cronut hammock meditation occupy master cleanse chartreuse lumbersexual. Kombucha kogi viral truffaut synth distillery single-origin coffee ugh slow-carb marfa selfies. Pitchfork schlitz semiotics fanny pack, ugh artisan vegan vaporware hexagon. Polaroid fixie post-ironic venmo wolf ramps kale chips.\n There should be no margin above this first sentence.\nBlockquotes should be a lighter gray with a border along the left side in the secondary color.\nThere should be no margin below this final sentence.\n First Header 2 This is a normal paragraph following a header. Knausgaard kale chips snackwave microdosing cronut copper mug swag synth bitters letterpress glossier craft beer. Mumblecore bushwick authentic gochujang vegan chambray meditation jean shorts irony. Viral farm-to-table kale chips, pork belly palo santo distillery activated charcoal aesthetic jianbing air plant woke lomo VHS organic. Tattooed locavore succulents heirloom, small batch sriracha echo park DIY af. Shaman you probably haven’t heard of them copper mug, crucifix green juice vape single-origin coffee brunch actually. Mustache etsy vexillologist raclette authentic fam. Tousled beard humblebrag asymmetrical. I love turkey, I love my job, I love my friends, I love Chardonnay!\nDeae legum paulatimque terra, non vos mutata tacet: dic. Vocant docuique me plumas fila quin afuerunt copia haec o neque.\nOn big screens, paragraphs and headings should not take up the full container width, but we want tables, code blocks and similar to take the full width.\nScenester tumeric pickled, authentic crucifix post-ironic fam freegan VHS pork belly 8-bit yuccie PBR\u0026B. I love this life we live in.\nSecond Header 2  This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\n Header 3 This is a code block following a header.  Next level leggings before they sold out, PBR\u0026B church-key shaman echo park. Kale chips occupy godard whatever pop-up freegan pork belly selfies. Gastropub Belinda subway tile woke post-ironic seitan. Shabby chic man bun semiotics vape, chia messenger bag plaid cardigan.\nHeader 4  This is an unordered list following a header. This is an unordered list following a header. This is an unordered list following a header.  Header 5  This is an ordered list following a header. This is an ordered list following a header. This is an ordered list following a header.  Header 6    What Follows     A table A header   A table A header   A table A header     There’s a horizontal rule above and below this.\n Here is an unordered list:\n Liverpool F.C. Chelsea F.C. Manchester United F.C.  And an ordered list:\n Michael Brecker Seamus Blake Branford Marsalis  And an unordered task list:\n Create a Hugo theme Add task lists to it Take a vacation  And a “mixed” task list:\n Pack bags ? Travel!  And a nested list:\n Jackson 5  Michael Tito Jackie Marlon Jermaine   TMNT  Leonardo Michelangelo Donatello Raphael    Definition lists can be used with Markdown syntax. Definition headers are bold.\n Name Godzilla Born 1952 Birthplace Japan Color Green   Tables should have bold headings and alternating shaded rows.\n   Artist Album Year     Michael Jackson Thriller 1982   Prince Purple Rain 1984   Beastie Boys License to Ill 1986    If a table is too wide, it should scroll horizontally.\n   Artist Album Year Label Awards Songs     Michael Jackson Thriller 1982 Epic Records Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical Wanna Be Startin' Somethin', Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life   Prince Purple Rain 1984 Warner Brothers Records Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal Let’s Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I’m a Star, Purple Rain   Beastie Boys License to Ill 1986 Mercury Records noawardsbutthistablecelliswide Rhymin \u0026 Stealin, The New Style, She’s Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill     Code snippets like var foo = \"bar\"; can be shown inline.\nAlso, this should vertically align with this and this.\nCode can also be shown in a block element.\nfoo := \"bar\"; bar := \"foo\";  Code can also use syntax highlighting.\nfunc main() { input := `var foo = \"bar\";` lexer := lexers.Get(\"javascript\") iterator, _ := lexer.Tokenise(nil, input) style := styles.Get(\"github\") formatter := html.New(html.WithLineNumbers()) var buff bytes.Buffer formatter.Format(\u0026buff, style, iterator) fmt.Println(buff.String()) }  Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.  Inline code inside table cells should still be distinguishable.\n   Language Code     Javascript var foo = \"bar\";   Ruby foo = \"bar\"{     Small images should be shown at their actual size.\nLarge images should always scale down and fit in the content container.\nThe photo above of the Spruce Picea abies shoot with foliage buds: Bjørn Erik Pedersen, CC-BY-SA.\nComponents Alerts  This is an alert.  Note This is an alert with a title.  Note This is an alert with a title and Markdown.  This is a successful alert.  This is a warning.  Warning This is a warning with a title.  Another Heading Add some sections here to see how the ToC looks like. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nThis Document Inguina genus: Anaphen post: lingua violente voce suae meus aetate diversi. Orbis unam nec flammaeque status deam Silenum erat et a ferrea. Excitus rigidum ait: vestro et Herculis convicia: nitidae deseruit coniuge Proteaque adiciam eripitur? Sitim noceat signa probat quidem. Sua longis fugatis quidem genae.\nPixel Count Tilde photo booth wayfarers cliche lomo intelligentsia man braid kombucha vaporware farm-to-table mixtape portland. PBR\u0026B pickled cornhole ugh try-hard ethical subway tile. Fixie paleo intelligentsia pabst. Ennui waistcoat vinyl gochujang. Poutine salvia authentic affogato, chambray lumbersexual shabby chic.\nContact Info Plaid hell of cred microdosing, succulents tilde pour-over. Offal shabby chic 3 wolf moon blue bottle raw denim normcore poutine pork belly.\nExternal Links Stumptown PBR\u0026B keytar plaid street art, forage XOXO pitchfork selvage affogato green juice listicle pickled everyday carry hashtag. Organic sustainable letterpress sartorial scenester intelligentsia swag bushwick. Put a bird on it stumptown neutra locavore. IPhone typewriter messenger bag narwhal. Ennui cold-pressed seitan flannel keytar, single-origin coffee adaptogen occupy yuccie williamsburg chillwave shoreditch forage waistcoat.\nThis is the final element on the page and there should be no margin below this.  ","categories":"","description":"A short lead description about this content page. It can be **bold** or _italic_ and can be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. It can be **bold** …","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/tutorials/multi-bear/","tags":"","title":"Multi-Bear Domicile Setup"},{"body":" This is a placeholder page. Replace it with your own content.\n Text can be bold, italic, or strikethrough. Links should be blue with no underlines (unless hovered over).\nThere should be whitespace between paragraphs. Vape migas chillwave sriracha poutine try-hard distillery. Tattooed shabby chic small batch, pabst art party heirloom letterpress air plant pop-up. Sustainable chia skateboard art party banjo cardigan normcore affogato vexillologist quinoa meggings man bun master cleanse shoreditch readymade. Yuccie prism four dollar toast tbh cardigan iPhone, tumblr listicle live-edge VHS. Pug lyft normcore hot chicken biodiesel, actually keffiyeh thundercats photo booth pour-over twee fam food truck microdosing banh mi. Vice activated charcoal raclette unicorn live-edge post-ironic. Heirloom vexillologist coloring book, beard deep v letterpress echo park humblebrag tilde.\n90’s four loko seitan photo booth gochujang freegan tumeric listicle fam ugh humblebrag. Bespoke leggings gastropub, biodiesel brunch pug fashion axe meh swag art party neutra deep v chia. Enamel pin fanny pack knausgaard tofu, artisan cronut hammock meditation occupy master cleanse chartreuse lumbersexual. Kombucha kogi viral truffaut synth distillery single-origin coffee ugh slow-carb marfa selfies. Pitchfork schlitz semiotics fanny pack, ugh artisan vegan vaporware hexagon. Polaroid fixie post-ironic venmo wolf ramps kale chips.\n There should be no margin above this first sentence.\nBlockquotes should be a lighter gray with a border along the left side in the secondary color.\nThere should be no margin below this final sentence.\n First Header 2 This is a normal paragraph following a header. Knausgaard kale chips snackwave microdosing cronut copper mug swag synth bitters letterpress glossier craft beer. Mumblecore bushwick authentic gochujang vegan chambray meditation jean shorts irony. Viral farm-to-table kale chips, pork belly palo santo distillery activated charcoal aesthetic jianbing air plant woke lomo VHS organic. Tattooed locavore succulents heirloom, small batch sriracha echo park DIY af. Shaman you probably haven’t heard of them copper mug, crucifix green juice vape single-origin coffee brunch actually. Mustache etsy vexillologist raclette authentic fam. Tousled beard humblebrag asymmetrical. I love turkey, I love my job, I love my friends, I love Chardonnay!\nDeae legum paulatimque terra, non vos mutata tacet: dic. Vocant docuique me plumas fila quin afuerunt copia haec o neque.\nOn big screens, paragraphs and headings should not take up the full container width, but we want tables, code blocks and similar to take the full width.\nScenester tumeric pickled, authentic crucifix post-ironic fam freegan VHS pork belly 8-bit yuccie PBR\u0026B. I love this life we live in.\nSecond Header 2  This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\n Header 3 This is a code block following a header.  Next level leggings before they sold out, PBR\u0026B church-key shaman echo park. Kale chips occupy godard whatever pop-up freegan pork belly selfies. Gastropub Belinda subway tile woke post-ironic seitan. Shabby chic man bun semiotics vape, chia messenger bag plaid cardigan.\nHeader 4  This is an unordered list following a header. This is an unordered list following a header. This is an unordered list following a header.  Header 5  This is an ordered list following a header. This is an ordered list following a header. This is an ordered list following a header.  Header 6    What Follows     A table A header   A table A header   A table A header     There’s a horizontal rule above and below this.\n Here is an unordered list:\n Liverpool F.C. Chelsea F.C. Manchester United F.C.  And an ordered list:\n Michael Brecker Seamus Blake Branford Marsalis  And an unordered task list:\n Create a Hugo theme Add task lists to it Take a vacation  And a “mixed” task list:\n Pack bags ? Travel!  And a nested list:\n Jackson 5  Michael Tito Jackie Marlon Jermaine   TMNT  Leonardo Michelangelo Donatello Raphael    Definition lists can be used with Markdown syntax. Definition headers are bold.\n Name Godzilla Born 1952 Birthplace Japan Color Green   Tables should have bold headings and alternating shaded rows.\n   Artist Album Year     Michael Jackson Thriller 1982   Prince Purple Rain 1984   Beastie Boys License to Ill 1986    If a table is too wide, it should scroll horizontally.\n   Artist Album Year Label Awards Songs     Michael Jackson Thriller 1982 Epic Records Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical Wanna Be Startin' Somethin', Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life   Prince Purple Rain 1984 Warner Brothers Records Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal Let’s Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I’m a Star, Purple Rain   Beastie Boys License to Ill 1986 Mercury Records noawardsbutthistablecelliswide Rhymin \u0026 Stealin, The New Style, She’s Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill     Code snippets like var foo = \"bar\"; can be shown inline.\nAlso, this should vertically align with this and this.\nCode can also be shown in a block element.\nfoo := \"bar\"; bar := \"foo\";  Code can also use syntax highlighting.\nfunc main() { input := `var foo = \"bar\";` lexer := lexers.Get(\"javascript\") iterator, _ := lexer.Tokenise(nil, input) style := styles.Get(\"github\") formatter := html.New(html.WithLineNumbers()) var buff bytes.Buffer formatter.Format(\u0026buff, style, iterator) fmt.Println(buff.String()) }  Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.  Inline code inside table cells should still be distinguishable.\n   Language Code     Javascript var foo = \"bar\";   Ruby foo = \"bar\"{     Small images should be shown at their actual size.\nLarge images should always scale down and fit in the content container.\nThe photo above of the Spruce Picea abies shoot with foliage buds: Bjørn Erik Pedersen, CC-BY-SA.\nComponents Alerts  This is an alert.  Note This is an alert with a title.  Note This is an alert with a title and Markdown.  This is a successful alert.  This is a warning.  Warning This is a warning with a title.  Another Heading Add some sections here to see how the ToC looks like. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nThis Document Inguina genus: Anaphen post: lingua violente voce suae meus aetate diversi. Orbis unam nec flammaeque status deam Silenum erat et a ferrea. Excitus rigidum ait: vestro et Herculis convicia: nitidae deseruit coniuge Proteaque adiciam eripitur? Sitim noceat signa probat quidem. Sua longis fugatis quidem genae.\nPixel Count Tilde photo booth wayfarers cliche lomo intelligentsia man braid kombucha vaporware farm-to-table mixtape portland. PBR\u0026B pickled cornhole ugh try-hard ethical subway tile. Fixie paleo intelligentsia pabst. Ennui waistcoat vinyl gochujang. Poutine salvia authentic affogato, chambray lumbersexual shabby chic.\nContact Info Plaid hell of cred microdosing, succulents tilde pour-over. Offal shabby chic 3 wolf moon blue bottle raw denim normcore poutine pork belly.\nExternal Links Stumptown PBR\u0026B keytar plaid street art, forage XOXO pitchfork selvage affogato green juice listicle pickled everyday carry hashtag. Organic sustainable letterpress sartorial scenester intelligentsia swag bushwick. Put a bird on it stumptown neutra locavore. IPhone typewriter messenger bag narwhal. Ennui cold-pressed seitan flannel keytar, single-origin coffee adaptogen occupy yuccie williamsburg chillwave shoreditch forage waistcoat.\nThis is the final element on the page and there should be no margin below this.  ","categories":"","description":"A short lead description about this content page. It can be **bold** or _italic_ and can be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. It can be **bold** …","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/tasks/porridge/","tags":"","title":"Porridge Assessment"},{"body":" This is a placeholder page that shows you how to use this template site.\n For many projects, users may not need much information beyond the information in the Overview, so this section is optional. However if there are areas where your users will need a more detailed understanding of a given term or feature in order to do anything useful with your project (or to not make mistakes when using it) put that information in this section. For example, you may want to add some conceptual pages if you have a large project with many components and a complex architecture.\nRemember to focus on what the user needs to know, not just what you think is interesting about your project! If they don’t need to understand your original design decisions to use or contribute to the project, don’t put them in, or include your design docs in your repo and link to them. Similarly, most users will probably need to know more about how features work when in use rather than how they are implemented. Consider a separate architecture page for more detailed implementation and system design information that potential project contributors can consult.\n","categories":"","description":"What does your user need to understand about your project in order to use it - or potentially contribute to it? \n","excerpt":"What does your user need to understand about your project in order to …","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/concepts/","tags":"","title":"Concepts"},{"body":" This is a placeholder page. Replace it with your own content.\n Text can be bold, italic, or strikethrough. Links should be blue with no underlines (unless hovered over).\nThere should be whitespace between paragraphs. Vape migas chillwave sriracha poutine try-hard distillery. Tattooed shabby chic small batch, pabst art party heirloom letterpress air plant pop-up. Sustainable chia skateboard art party banjo cardigan normcore affogato vexillologist quinoa meggings man bun master cleanse shoreditch readymade. Yuccie prism four dollar toast tbh cardigan iPhone, tumblr listicle live-edge VHS. Pug lyft normcore hot chicken biodiesel, actually keffiyeh thundercats photo booth pour-over twee fam food truck microdosing banh mi. Vice activated charcoal raclette unicorn live-edge post-ironic. Heirloom vexillologist coloring book, beard deep v letterpress echo park humblebrag tilde.\n90’s four loko seitan photo booth gochujang freegan tumeric listicle fam ugh humblebrag. Bespoke leggings gastropub, biodiesel brunch pug fashion axe meh swag art party neutra deep v chia. Enamel pin fanny pack knausgaard tofu, artisan cronut hammock meditation occupy master cleanse chartreuse lumbersexual. Kombucha kogi viral truffaut synth distillery single-origin coffee ugh slow-carb marfa selfies. Pitchfork schlitz semiotics fanny pack, ugh artisan vegan vaporware hexagon. Polaroid fixie post-ironic venmo wolf ramps kale chips.\n There should be no margin above this first sentence.\nBlockquotes should be a lighter gray with a border along the left side in the secondary color.\nThere should be no margin below this final sentence.\n First Header 2 This is a normal paragraph following a header. Knausgaard kale chips snackwave microdosing cronut copper mug swag synth bitters letterpress glossier craft beer. Mumblecore bushwick authentic gochujang vegan chambray meditation jean shorts irony. Viral farm-to-table kale chips, pork belly palo santo distillery activated charcoal aesthetic jianbing air plant woke lomo VHS organic. Tattooed locavore succulents heirloom, small batch sriracha echo park DIY af. Shaman you probably haven’t heard of them copper mug, crucifix green juice vape single-origin coffee brunch actually. Mustache etsy vexillologist raclette authentic fam. Tousled beard humblebrag asymmetrical. I love turkey, I love my job, I love my friends, I love Chardonnay!\nDeae legum paulatimque terra, non vos mutata tacet: dic. Vocant docuique me plumas fila quin afuerunt copia haec o neque.\nOn big screens, paragraphs and headings should not take up the full container width, but we want tables, code blocks and similar to take the full width.\nScenester tumeric pickled, authentic crucifix post-ironic fam freegan VHS pork belly 8-bit yuccie PBR\u0026B. I love this life we live in.\nSecond Header 2  This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\n Header 3 This is a code block following a header.  Next level leggings before they sold out, PBR\u0026B church-key shaman echo park. Kale chips occupy godard whatever pop-up freegan pork belly selfies. Gastropub Belinda subway tile woke post-ironic seitan. Shabby chic man bun semiotics vape, chia messenger bag plaid cardigan.\nHeader 4  This is an unordered list following a header. This is an unordered list following a header. This is an unordered list following a header.  Header 5  This is an ordered list following a header. This is an ordered list following a header. This is an ordered list following a header.  Header 6    What Follows     A table A header   A table A header   A table A header     There’s a horizontal rule above and below this.\n Here is an unordered list:\n Liverpool F.C. Chelsea F.C. Manchester United F.C.  And an ordered list:\n Michael Brecker Seamus Blake Branford Marsalis  And an unordered task list:\n Create a Hugo theme Add task lists to it Take a vacation  And a “mixed” task list:\n Pack bags ? Travel!  And a nested list:\n Jackson 5  Michael Tito Jackie Marlon Jermaine   TMNT  Leonardo Michelangelo Donatello Raphael    Definition lists can be used with Markdown syntax. Definition headers are bold.\n Name Godzilla Born 1952 Birthplace Japan Color Green   Tables should have bold headings and alternating shaded rows.\n   Artist Album Year     Michael Jackson Thriller 1982   Prince Purple Rain 1984   Beastie Boys License to Ill 1986    If a table is too wide, it should scroll horizontally.\n   Artist Album Year Label Awards Songs     Michael Jackson Thriller 1982 Epic Records Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical Wanna Be Startin' Somethin', Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life   Prince Purple Rain 1984 Warner Brothers Records Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal Let’s Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I’m a Star, Purple Rain   Beastie Boys License to Ill 1986 Mercury Records noawardsbutthistablecelliswide Rhymin \u0026 Stealin, The New Style, She’s Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill     Code snippets like var foo = \"bar\"; can be shown inline.\nAlso, this should vertically align with this and this.\nCode can also be shown in a block element.\nfoo := \"bar\"; bar := \"foo\";  Code can also use syntax highlighting.\nfunc main() { input := `var foo = \"bar\";` lexer := lexers.Get(\"javascript\") iterator, _ := lexer.Tokenise(nil, input) style := styles.Get(\"github\") formatter := html.New(html.WithLineNumbers()) var buff bytes.Buffer formatter.Format(\u0026buff, style, iterator) fmt.Println(buff.String()) }  Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.  Inline code inside table cells should still be distinguishable.\n   Language Code     Javascript var foo = \"bar\";   Ruby foo = \"bar\"{     Small images should be shown at their actual size.\nLarge images should always scale down and fit in the content container.\nThe photo above of the Spruce Picea abies shoot with foliage buds: Bjørn Erik Pedersen, CC-BY-SA.\nComponents Alerts  This is an alert.  Note This is an alert with a title.  Note This is an alert with a title and Markdown.  This is a successful alert.  This is a warning.  Warning This is a warning with a title.  Another Heading Add some sections here to see how the ToC looks like. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nThis Document Inguina genus: Anaphen post: lingua violente voce suae meus aetate diversi. Orbis unam nec flammaeque status deam Silenum erat et a ferrea. Excitus rigidum ait: vestro et Herculis convicia: nitidae deseruit coniuge Proteaque adiciam eripitur? Sitim noceat signa probat quidem. Sua longis fugatis quidem genae.\nPixel Count Tilde photo booth wayfarers cliche lomo intelligentsia man braid kombucha vaporware farm-to-table mixtape portland. PBR\u0026B pickled cornhole ugh try-hard ethical subway tile. Fixie paleo intelligentsia pabst. Ennui waistcoat vinyl gochujang. Poutine salvia authentic affogato, chambray lumbersexual shabby chic.\nContact Info Plaid hell of cred microdosing, succulents tilde pour-over. Offal shabby chic 3 wolf moon blue bottle raw denim normcore poutine pork belly.\nExternal Links Stumptown PBR\u0026B keytar plaid street art, forage XOXO pitchfork selvage affogato green juice listicle pickled everyday carry hashtag. Organic sustainable letterpress sartorial scenester intelligentsia swag bushwick. Put a bird on it stumptown neutra locavore. IPhone typewriter messenger bag narwhal. Ennui cold-pressed seitan flannel keytar, single-origin coffee adaptogen occupy yuccie williamsburg chillwave shoreditch forage waistcoat.\nThis is the final element on the page and there should be no margin below this.  ","categories":"","description":"A short lead description about this content page. It can be **bold** or _italic_ and can be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. It can be **bold** …","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/tasks/task/","tags":"","title":"Another Task"},{"body":" This is a placeholder page. Replace it with your own content.\n Text can be bold, italic, or strikethrough. Links should be blue with no underlines (unless hovered over).\nThere should be whitespace between paragraphs. Vape migas chillwave sriracha poutine try-hard distillery. Tattooed shabby chic small batch, pabst art party heirloom letterpress air plant pop-up. Sustainable chia skateboard art party banjo cardigan normcore affogato vexillologist quinoa meggings man bun master cleanse shoreditch readymade. Yuccie prism four dollar toast tbh cardigan iPhone, tumblr listicle live-edge VHS. Pug lyft normcore hot chicken biodiesel, actually keffiyeh thundercats photo booth pour-over twee fam food truck microdosing banh mi. Vice activated charcoal raclette unicorn live-edge post-ironic. Heirloom vexillologist coloring book, beard deep v letterpress echo park humblebrag tilde.\n90’s four loko seitan photo booth gochujang freegan tumeric listicle fam ugh humblebrag. Bespoke leggings gastropub, biodiesel brunch pug fashion axe meh swag art party neutra deep v chia. Enamel pin fanny pack knausgaard tofu, artisan cronut hammock meditation occupy master cleanse chartreuse lumbersexual. Kombucha kogi viral truffaut synth distillery single-origin coffee ugh slow-carb marfa selfies. Pitchfork schlitz semiotics fanny pack, ugh artisan vegan vaporware hexagon. Polaroid fixie post-ironic venmo wolf ramps kale chips.\n There should be no margin above this first sentence.\nBlockquotes should be a lighter gray with a border along the left side in the secondary color.\nThere should be no margin below this final sentence.\n First Header 2 This is a normal paragraph following a header. Knausgaard kale chips snackwave microdosing cronut copper mug swag synth bitters letterpress glossier craft beer. Mumblecore bushwick authentic gochujang vegan chambray meditation jean shorts irony. Viral farm-to-table kale chips, pork belly palo santo distillery activated charcoal aesthetic jianbing air plant woke lomo VHS organic. Tattooed locavore succulents heirloom, small batch sriracha echo park DIY af. Shaman you probably haven’t heard of them copper mug, crucifix green juice vape single-origin coffee brunch actually. Mustache etsy vexillologist raclette authentic fam. Tousled beard humblebrag asymmetrical. I love turkey, I love my job, I love my friends, I love Chardonnay!\nDeae legum paulatimque terra, non vos mutata tacet: dic. Vocant docuique me plumas fila quin afuerunt copia haec o neque.\nOn big screens, paragraphs and headings should not take up the full container width, but we want tables, code blocks and similar to take the full width.\nScenester tumeric pickled, authentic crucifix post-ironic fam freegan VHS pork belly 8-bit yuccie PBR\u0026B. I love this life we live in.\nSecond Header 2  This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\n Header 3 This is a code block following a header.  Next level leggings before they sold out, PBR\u0026B church-key shaman echo park. Kale chips occupy godard whatever pop-up freegan pork belly selfies. Gastropub Belinda subway tile woke post-ironic seitan. Shabby chic man bun semiotics vape, chia messenger bag plaid cardigan.\nHeader 4  This is an unordered list following a header. This is an unordered list following a header. This is an unordered list following a header.  Header 5  This is an ordered list following a header. This is an ordered list following a header. This is an ordered list following a header.  Header 6    What Follows     A table A header   A table A header   A table A header     There’s a horizontal rule above and below this.\n Here is an unordered list:\n Liverpool F.C. Chelsea F.C. Manchester United F.C.  And an ordered list:\n Michael Brecker Seamus Blake Branford Marsalis  And an unordered task list:\n Create a Hugo theme Add task lists to it Take a vacation  And a “mixed” task list:\n Pack bags ? Travel!  And a nested list:\n Jackson 5  Michael Tito Jackie Marlon Jermaine   TMNT  Leonardo Michelangelo Donatello Raphael    Definition lists can be used with Markdown syntax. Definition headers are bold.\n Name Godzilla Born 1952 Birthplace Japan Color Green   Tables should have bold headings and alternating shaded rows.\n   Artist Album Year     Michael Jackson Thriller 1982   Prince Purple Rain 1984   Beastie Boys License to Ill 1986    If a table is too wide, it should scroll horizontally.\n   Artist Album Year Label Awards Songs     Michael Jackson Thriller 1982 Epic Records Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical Wanna Be Startin' Somethin', Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life   Prince Purple Rain 1984 Warner Brothers Records Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal Let’s Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I’m a Star, Purple Rain   Beastie Boys License to Ill 1986 Mercury Records noawardsbutthistablecelliswide Rhymin \u0026 Stealin, The New Style, She’s Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill     Code snippets like var foo = \"bar\"; can be shown inline.\nAlso, this should vertically align with this and this.\nCode can also be shown in a block element.\nfoo := \"bar\"; bar := \"foo\";  Code can also use syntax highlighting.\nfunc main() { input := `var foo = \"bar\";` lexer := lexers.Get(\"javascript\") iterator, _ := lexer.Tokenise(nil, input) style := styles.Get(\"github\") formatter := html.New(html.WithLineNumbers()) var buff bytes.Buffer formatter.Format(\u0026buff, style, iterator) fmt.Println(buff.String()) }  Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.  Inline code inside table cells should still be distinguishable.\n   Language Code     Javascript var foo = \"bar\";   Ruby foo = \"bar\"{     Small images should be shown at their actual size.\nLarge images should always scale down and fit in the content container.\nThe photo above of the Spruce Picea abies shoot with foliage buds: Bjørn Erik Pedersen, CC-BY-SA.\nComponents Alerts  This is an alert.  Note This is an alert with a title.  Note This is an alert with a title and Markdown.  This is a successful alert.  This is a warning.  Warning This is a warning with a title.  Another Heading Add some sections here to see how the ToC looks like. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nThis Document Inguina genus: Anaphen post: lingua violente voce suae meus aetate diversi. Orbis unam nec flammaeque status deam Silenum erat et a ferrea. Excitus rigidum ait: vestro et Herculis convicia: nitidae deseruit coniuge Proteaque adiciam eripitur? Sitim noceat signa probat quidem. Sua longis fugatis quidem genae.\nPixel Count Tilde photo booth wayfarers cliche lomo intelligentsia man braid kombucha vaporware farm-to-table mixtape portland. PBR\u0026B pickled cornhole ugh try-hard ethical subway tile. Fixie paleo intelligentsia pabst. Ennui waistcoat vinyl gochujang. Poutine salvia authentic affogato, chambray lumbersexual shabby chic.\nContact Info Plaid hell of cred microdosing, succulents tilde pour-over. Offal shabby chic 3 wolf moon blue bottle raw denim normcore poutine pork belly.\nExternal Links Stumptown PBR\u0026B keytar plaid street art, forage XOXO pitchfork selvage affogato green juice listicle pickled everyday carry hashtag. Organic sustainable letterpress sartorial scenester intelligentsia swag bushwick. Put a bird on it stumptown neutra locavore. IPhone typewriter messenger bag narwhal. Ennui cold-pressed seitan flannel keytar, single-origin coffee adaptogen occupy yuccie williamsburg chillwave shoreditch forage waistcoat.\nThis is the final element on the page and there should be no margin below this.  ","categories":"","description":"A short lead description about this content page. It can be **bold** or _italic_ and can be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. It can be **bold** …","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/tutorials/tutorial2/","tags":"","title":"Another Tutorial"},{"body":" Warning This document refers to v5.\n Stroom Proxy defaults to listening for HTTP on port 9080. It is recommended that Apache is used to listen on the standard HTTP port 80 and forward requests on via the Apache mod_jk module and the AJP protocol (on 9009). Apache can also perform HTTPS on port 443 and pass over requests to Tomcat using the same AJP protocol.\nIt is additionally recommended that Stroom Proxy is used to front data ingest and so Apache is configured to route traffic to http(s)://server/stroom/datafeed to Stroom Proxy.\nPrerequisites  tomcat-connectors-1.2.31-src.tar.gz  Setup Apache  As root Patch mod_jk  cd ~/tmp tar -xvzf tomcat-connectors-1.2.31-src.tar.gz cd tomcat-connectors-1.2.31-src/native ./configure --with-apxs=/usr/sbin/apxs make sudo cp apache-2.0/mod_jk.so /etc/httpd/modules/ cd   Put the web server cert, private key, and CA cert into the web servers conf directory /etc/httpd/conf. E.g.  [user@node1 stroom-doc]$ ls -al /etc/httpd/conf .... -rw-r--r-- 1 root root 1729 Aug 27 2013 host.crt -rw-r--r-- 1 root root 1675 Aug 27 2013 host.key -rw-r--r-- 1 root root 1289 Aug 27 2013 CA.crt ....   Make changes to /etc/http/conf.d/ssl.conf as per below  JkMount /stroom/datafeed* loadbalancer_proxy JkMount /stroom* loadbalancer_proxy  JkOptions +ForwardKeySize +ForwardURICompat +ForwardSSLCertChain -ForwardDirectories SSLCertificateFile /etc/httpd/conf/[YOUR SERVER].crt SSLCertificateKeyFile /etc/httpd/conf/[YOUR SERVER].key SSLCertificateChainFile /etc/httpd/conf/[YOUR CA].crt SSLCACertificateFile /etc/httpd/conf/[YOUR CA APPENDED LIST].crt   Remove /etc/httpd/conf.d/nss.conf to avoid a 8443 port clash  rm /etc/httpd/conf.d/nss.conf   Create a /etc/httpd/conf.d/mod_jk.conf configuration  LoadModule jk_module modules/mod_jk.so JkWorkersFile conf/workers.properties JkLogFile logs/mod_jk.log JkLogLevel info JkLogStampFormat \"[%a %b %d %H:%M:%S %Y]\" JkOptions +ForwardKeySize +ForwardURICompat +ForwardSSLCertChain -ForwardDirectories JkRequestLogFormat \"%w %V %T\"  JkMount /stroom/datafeed* loadbalancer_proxy JkMount /stroom* loadbalancer_proxy  JkShmFile logs/jk.shm \u003cLocation /jkstatus/\u003e JkMount status Order deny,allow Deny from all Allow from 127.0.0.1 \u003c/Location\u003e   Setup stroom-setup/cluster.txt, generate the workers file and copy into Apache. (as root and replace stroomuser with your processing user)  /home/stroomuser/stroom-setup/workers.properties.sh --cluster=/home/stroomuser/cluster.txt \u003e /etc/httpd/conf/workers.properties   Inspect /etc/httpd/conf/workers.properties to make sure it looks as you expect for your cluster  worker.list=loadbalancer_proxy,local_proxy worker.stroom_1_proxy.port=9009 worker.stroom_1_proxy.host=localhost worker.stroom_1_proxy.type=ajp13 worker.stroom_1_proxy.lbfactor=1 worker.stroom_1_proxy.max_packet_size=65536 .... .... worker.loadbalancer_proxy.type=lb worker.loadbalancer_proxy.balance_workers=stroom_1_proxy,stroom_2_proxy worker.loadbalancer_proxy.sticky_session=1 worker.local_proxy.type=lb worker.local_proxy.balance_workers=stroom_1_proxy worker.local_proxy.sticky_session=1   Create a simple redirect page to the stroom web app for the root URL (e.g. DocumentRoot “/var/www/html”, index.html)  \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;meta http-equiv=\"Refresh\" content=\"0; URL=stroom\"\u0026gt;\u0026lt;/head\u0026gt;\u0026lt;/html\u0026gt;   Restart Apache and then test default http / https access.  sudo /etc/init.d/httpd restart  ","categories":"","description":"","excerpt":" Warning This document refers to v5.\n Stroom Proxy defaults to …","ref":"/stroom-docs/hugo-docsy/docs/proxy/apache-forwarding/","tags":"","title":"Apache Forwarding"},{"body":" TODO This document is out of date and needs updating to refer to the stroom-proxy docker stack.  Clone and build stroom-proxy git clone https://github.com/gchq/stroom-proxy.git mvn clean install  Unpack the stroom-proxy distribution in preparation for building the docker iamge cd stroom-proxy-distribution unzip target/stroom-proxy-distribution-\u003cversion\u003e-bin.zip -d target  Building and running the docker image Here you need to say where you want data to be sent by stroom-proxy. This is done using a --build-arg (external link) parameter of STROOM_PROXY_TYPE. These values can be forward, store, or store_nodb. See the stroom-proxy section in the stroom documentation (external link) documentation for more details about these options.\ndocker stop stroom-proxy docker rm stroom-proxy docker rmi stroom-proxy docker build --build-arg STROOM_PROXY_TYPE=store_nodb --tag=stroom-proxy:latest target/stroom-proxy docker run -p 8080:8080 --name=stroom-proxy stroom-proxy  ","categories":"","description":"","excerpt":" TODO This document is out of date and needs updating to refer to the …","ref":"/stroom-docs/hugo-docsy/docs/proxy/docker/","tags":["proxy","docker"],"title":"Running with docker"},{"body":" Warning This document refers to v5.\n Prerequisites  Linux Server’s with at least 4GB RAM Install files stroom-proxy-X-Y-Z-distribution.zip, stroom-deploy-X-Y-Z-distribution.zip Temporarily allow port 9080 if not relying on Apache Forwarding (see below)  Processing User Setup See Processing User Setup.\nInstalling Stroom Proxy As the processing user unpack the stroom-proxy-X-Y-Z-distribution.zip installation files in the processing users home directory.\nunzip stroom-proxy-X-Y-Z-distribution.zip  Stroom Proxy can be setup as follows:\n forward - as an aggregation point to store and forwarding onto another Stroom or Stroom / Proxy store - to front Stroom for data ingest  Stroom Proxy - forward In forward mode you need to know the server address that data is being sent onto. Run the setup script to capture the basic settings required to run Stroom Proxy in forward mode.\n @@ NODE @@ - Each Stroom instance in the cluster needs a unique name, if this is a reinstall ensure you use the previous deployment. This name needs match the name used in your worker.properties (e.g. ‘node1’ in the case ‘node1.my.org’) @@ PORT PREFIX @@ - By default Stroom Proxy will run on port 9080  [stroomuser@node1 ~]$ ./stroom-proxy/bin/setup.sh forward [stroomuser@dev1 ~]$ ./stroom-proxy/bin/setup.sh forward ... Parameters ========== @@NODE@@ : Unique node name for install [node1 ] : node1 @@PORT_PREFIX@@ : HTTP prefix to use [90 ] : 90 @@REPO_DIR@@ : Stroom Proxy Repository Dir [/stroomdata/stroom-proxy ] : /home/stroomuser/stroom-proxy-repo @@FORWARD_SERVER@@ : Server to forward data on to [hostname ] : audit.my.org @@JAVA_OPTS@@ : Optional tomcat JVM settings [\"-Xms512m -Xmx1g\" ] : ...  Stroom Proxy - store In store mode you need to know the mysql details to validate incoming data with.\n[stroomuser@node1 ~]$ ./stroom-proxy-app/bin/setup.sh store ... @@NODE@@ : Unique node name for install [node ] : @@PORT_PREFIX@@ : HTTP prefix to use [90 ] : 72 @@REPO_DIR@@ : Stroom Proxy Repository Dir [/stroomdata/stroom-proxy ] : /home/stroomuser/stroom-proxy-repo-2 @@JDBC_CLASSNAME@@ : JDBC class name [com.mysql.jdbc.Driver ] : @@JDBC_URL@@ : JDBC URL (jdbc:mysql://[HOST]/[DBNAME]) [jdbc:mysql://localhost/stroom ] : @@DB_USERNAME@@ : Database username [ ] : stroomuser @@DB_PASSWORD@@ : Database password [ ] : @@JAVA_OPTS@@ : Optional tomcat JVM settings [\"-Xms512m -Xmx1g\" ] :  Install Check Start the installed instance:\n./stroom-deploy/start.sh  Inspect the logs:\ntail -f stroom-proxy-app/instance/logs/stroom.log  Stroom Proxy Properties The following properties can be configured in the stroom.properties file.\nTODO - Could do with a column indicating which proxy mode these properties apply to, e.g. store/forward\n   Property Name Description     repoDir Optional Repository DIR. If set any incoming request will be written to the file system.   logRequest Optional log line with header attributes output as defined by this property   bufferSize Override default (8192) JDK buffer size to use   forwardUrl Optional The URL’s to forward onto This is pass-through mode if repoDir is not set   forwardThreadCount Number of threads to forward with   forwardTimeoutMs Time out when forwarding   forwardChunkSize Chunk size to use over http(s) if not set requires buffer to be fully loaded into memory   rollCron Interval to roll any writing repositories.   readCron Cron style interval (e.g. every hour ‘0 * *’, every half hour ‘0,30 * *') to read any ready repositories (if not defined we read all the time).   maxAggregation Aggregate size to break at when building an aggregate.   zipFilenameDelimiter The delimiter used to separate the id ihe zip filename from the templated part   zipFilenameTemplate A template for naming the zip files in the repository where files will be named nnn!zipFilenameTemplate.zip where nnn is the id prefix, ! is the configurable delimiter and zipFilenameTemplate will be something like ‘${feed}!${headerMapKey1}!${headerMapKey2}’. The naem of each variable must exactly match a key in the meta data else it will resolve to ‘’.   requestDelayMs Sleep time used to aid with testing   forwardDelayMs Debug setting to add a delay   dbRequestValidatorContext Database Feed Validator - Data base JDBC context   dbRequestValidatorJndiName Database Feed Validator - Data base JDBC JNDI name   dbRequestValidatorFeedQuery Database Feed Validator - SQL to check feed status   dbRequestValidatorAuthQuery Database Feed Validator - SQL to check authorisation required   remotingUrl Url to use for remoting services   remotingReadTimeoutMs Change from the default JVM settings.   remotingConnectTimeoutMs Change from the default JVM settings.   maxStreamSize Stream size to break at when building an aggregate.   maxFileScan Max number of files to scan over during forwarding. Once this limit is it it will wait until next read interval   cacheTimeToIdleSeconds Time to idle settings to used for validating feed information   cacheTimeToLiveSeconds Time to live settings to used for validating feed information    Apache Forwarding See Apache Forwarding.\nJava Key Store Setup If you require that Stroom Proxy communicates over 2-way https you will need to set up Java Key Stores.\nSee Java Key Store Setup.\nSecuring Stroom See Securing Stroom.\n","categories":"","description":"","excerpt":" Warning This document refers to v5.\n Prerequisites  Linux Server’s …","ref":"/stroom-docs/hugo-docsy/docs/proxy/install/","tags":["proxy","v5"],"title":"Stroom Proxy Installation"},{"body":" This is a placeholder page that shows you how to use this template site.\n Think about your project’s features and use cases. Use these to choose your core tasks. Each granular use case (enable x, configure y) should have a corresponding tasks page or tasks page section. Users should be able to quickly refer to your core tasks when they need to find out how to do one specific thing, rather than having to look for the instructions in a bigger tutorial or example. Think of your tasks pages as a cookbook with different procedures your users can combine to create something more substantial.\nYou can give each task a page, or you can group related tasks together in a page, such as tasks related to a particular feature. As well as grouping related tasks in single pages, you can also group task pages in nested folders with an index page as an overview, as seen in this example site. Or if you have a small docset like the Docsy User Guide with no Tutorials or Concepts pages, consider adding your feature-specific pages at the top level of your docs rather than in a Tasks section.\nEach task should give the user\n The prerequisites for this task, if any (this can be specified at the top of a multi-task page if they’re the same for all the page’s tasks. “All these tasks assume that you understand….and that you have already….\"). What this task accomplishes. Instructions for the task. If it involves editing a file, running a command, or writing code, provide code-formatted example snippets to show the user what to do! If there are multiple steps, provide them as a numbered list. If appropriate, links to related concept, tutorial, or example pages.  ","categories":"","description":"What can your user do with your project?\n","excerpt":"What can your user do with your project?\n","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/tasks/","tags":"","title":"Core Tasks"},{"body":" This is a placeholder page that shows you how to use this template site.\n Tutorials are complete worked examples made up of multiple tasks that guide the user through a relatively simple but realistic scenario: building an application that uses some of your project’s features, for example. If you have already created some Examples for your project you can base Tutorials on them. This section is optional. However, remember that although you may not need this section at first, having tutorials can be useful to help your users engage with your example code, especially if there are aspects that need more explanation than you can easily provide in code comments.\n","categories":"","description":"Show your user how to work through some end to end examples.\n","excerpt":"Show your user how to work through some end to end examples.\n","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/tutorials/","tags":"","title":"Tutorials"},{"body":" This is a placeholder page that shows you how to use this template site.\n If your project has an API, configuration, or other reference - anything that users need to look up that’s at an even lower level than a single task - put (or link to it) here. You can serve and link to generated reference docs created using Doxygen, Javadoc, or other doc generation tools by putting them in your static/ directory. Find out more in Adding static content. For OpenAPI reference, Docsy also provides a Swagger UI layout and shortcode that renders Swagger UI using any OpenAPI YAML or JSON file as source.\n","categories":"","description":"Low level reference docs for your project.\n","excerpt":"Low level reference docs for your project.\n","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/reference/","tags":"","title":"Reference"},{"body":"The following CSV data will be split up into separate fields using Data Splitter.\n01/01/2010,00:00:00,192.168.1.100,SOMEHOST.SOMEWHERE.COM,user1,logon, 01/01/2010,00:01:00,192.168.1.100,SOMEHOST.SOMEWHERE.COM,user1,create,c:\\test.txt 01/01/2010,00:02:00,192.168.1.100,SOMEHOST.SOMEWHERE.COM,user1,logoff,  The first thing we need to do is match each record. Each record in a CSV file is delimited by a new line character. The following configuration will split the data into records using ‘\\n’ as a delimiter:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\"\u003e \u003c!-- Match each line using a new line character as the delimiter --\u003e \u003csplit delimiter=\"\\n\"/\u003e \u003c/dataSplitter\u003e  In the above example the ‘split’ tokenizer matches all of the supplied content up to the end of each line ready to pass each line of content on for further treatment.\nWe can now add a \u003cgroup\u003e element within \u003csplit\u003e to take content matched by the tokenizer.\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\"\u003e \u003c!-- Match each line using a new line character as the delimiter --\u003e \u003csplit delimiter=\"\\n\"\u003e \u003c!-- Take the matched line (using group 1 ignores the delimiters, without this each match would include the new line character) --\u003e \u003cgroup value=\"$1\"\u003e \u003c/group\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  The \u003cgroup\u003e within the \u003csplit\u003e chooses to take the content from the \u003csplit\u003e without including the new line ‘\\n’ delimiter by using match group 1, see expression match references for details.\n01/01/2010,00:00:00,192.168.1.100,SOMEHOST.SOMEWHERE.COM,user1,logon,  The content selected by the \u003cgroup\u003e from its parent match can then be passed onto sub expressions for further matching:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\"\u003e \u003c!-- Match each line using a new line character as the delimiter --\u003e \u003csplit delimiter=\"\\n\"\u003e \u003c!-- Take the matched line (using group 1 ignores the delimiters, without this each match would include the new line character) --\u003e \u003cgroup value=\"$1\"\u003e \u003c!-- Match each value separated by a comma as the delimiter --\u003e \u003csplit delimiter=\",\"\u003e \u003c/split\u003e \u003c/group\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  In the above example the additional \u003csplit\u003e element within the \u003cgroup\u003e will match the content provided by the group repeatedly until it has used all of the group content.\nThe content matched by the inner \u003csplit\u003e element can be passed to a \u003cdata\u003e element to emit XML content.\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\"\u003e \u003c!-- Match each line using a new line character as the delimiter --\u003e \u003csplit delimiter=\"\\n\"\u003e \u003c!-- Take the matched line (using group 1 ignores the delimiters, without this each match would include the new line character) --\u003e \u003cgroup value=\"$1\"\u003e \u003c!-- Match each value separated by a comma as the delimiter --\u003e \u003csplit delimiter=\",\"\u003e \u003c!-- Output the value from group 1 (as above using group 1 ignores the delimiters, without this each value would include the comma) --\u003e \u003cdata value=\"$1\" /\u003e \u003c/split\u003e \u003c/group\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  In the above example each match from the inner \u003csplit\u003e is made available to the inner \u003cdata\u003e element that chooses to output content from match group 1, see expression match references for details.\nThe above configuration results in the following XML output for the whole input:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003crecords xmlns=\"records:2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"records:2 file://records-v2.0.xsd\" version=\"3.0\"\u003e \u003crecord\u003e \u003cdata value=\"01/01/2010\" /\u003e \u003cdata value=\"00:00:00\" /\u003e \u003cdata value=\"192.168.1.100\" /\u003e \u003cdata value=\"SOMEHOST.SOMEWHERE.COM\" /\u003e \u003cdata value=\"user1\" /\u003e \u003cdata value=\"logon\" /\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata value=\"01/01/2010\" /\u003e \u003cdata value=\"00:01:00\" /\u003e \u003cdata value=\"192.168.1.100\" /\u003e \u003cdata value=\"SOMEHOST.SOMEWHERE.COM\" /\u003e \u003cdata value=\"user1\" /\u003e \u003cdata value=\"create\" /\u003e \u003cdata value=\"c:\\test.txt\" /\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata value=\"01/01/2010\" /\u003e \u003cdata value=\"00:02:00\" /\u003e \u003cdata value=\"192.168.1.100\" /\u003e \u003cdata value=\"SOMEHOST.SOMEWHERE.COM\" /\u003e \u003cdata value=\"user1\" /\u003e \u003cdata value=\"logoff\" /\u003e \u003c/record\u003e \u003c/records\u003e  ","categories":"","description":"","excerpt":"The following CSV data will be split up into separate fields using …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-splitter/1-1-simple-csv-example/","tags":"","title":"Simple CSV Example"},{"body":"Markdown style conventions Line breaks Sentence per line Each sentence should start on a new line, even in numbered/bulleted lists. This makes it easier to move sentences around or to remove them. When the mrkdown is rendered into HTML/PDF, the sentences will be joined into a single paragraph.\nSee this link (external linnk) for more of the reasons behind sentence per line. Though this link relates to Asciidoc, the same applies to markdown.\nFor example:\nDon’t do this  This is the first sentence of the paragraph. This is the second. This it the third and final one. This is the start of a new paragraph.  Which renders as:\nThis is the first sentence of the paragraph. This is the second. This it the third and final one.\nThis is the start of a new paragraph.\n  Do this  This is the first sentence of the paragraph. This is the second. This it the third and final one. This is the start of a new paragraph.  Which renders as:\nThis is the first sentence of the paragraph. This is the second. This it the third and final one.\nThis is the start of a new paragraph.\n   No hard line breaks. Long lines should not be hard wrapped by adding line breaks. You should instead rely on your editor to soft wrap long lines that cannot fit on the visible screen area. The process of hard wrapping long lines will vary from editor to editor and not all editors support re-wrapping lines after the content has changed. It also relies on each person’s editor being configured to the same wrap column. Adding hard wraps also means a slight change at the start of a paragraph will potentially cause all subsequent lines to be re-wrapped and thus appear as a substantial difference in the commit.\n## Don't do this Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. ## Do this instead Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.  Which both render as:\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nForced line breaks In some circumstances, e.g. a list of items that is not bulleted, you may want to prevent the joining of adjecent lines when rendered. You can force a line break by adding two spaces   at the end of a line.\nDon’t do this  Paragraph 1. One Two Three Paragraph 2.  Which renders as:\nParagraph 1.\nOne Two Three\nParagraph 2.\n  Do this (highlight the text to see the spaces)  Paragraph 1. One Two Three Paragraph 2.  Which renders as:\nParagraph 1.\nOne\nTwo\nThree\nParagraph 2.\n   Blank lines and spacing  A heading line should be preceded by two blank lines and followed by one blank line. This makes the headings clearer in the markdown source. A fenced code block should be surrounded by one blank line. Paragraphs should be separated by one blank line. Bulleted and numbered lists should be surrounded by one blank line. Additional sentences in bulleted/numbered lists should be indented for clarity in the raw markdown.  e.g:\nThe text belonging to the previous heading. ## A Heading ## A sub heading The text of this heading. A second sentence in this paragraph. A new paragraph. A second sentence in this paragraph. Here are some bullets: * Bullet Z. * Bullet A. This is an additional sentence for this bullet point. And so is this. * Sub-bullet AX. This is an additional sentence for this bullet point. And so is this. * Bullet F. Here are some numbered steps: 1. Step 1. 1. Step 2. 1. Step 3. Another random line.  Headings The page title uses heading level one (#  in markdown) so all markdown headings should be \u003e= 2 (## , ### , #### , etc.). Headings should have two blank lines above them for clarity in the raw markdown. The # characters should always be followed by one space character\nThe following is an exmaple of the heading levels.\n# Heading level 1 ## Heading level 2 ### Heading level 3 #### Heading level 4 ##### Heading level 5 ###### Heading level 6  Markdown supports an alternate style for headings, as shown below. Don’t use this style as it is not clear from the symbols what the heading level is.\nHeading level 1 =============== Don't use this style. Heading level 2 --------------- Don't use this style.  Table of contents The page table of contents (right hand pane) is controlled by this in config.toml.\n[markup] [markup.tableOfContents] endLevel = 4 ordered = false startLevel = 2  The maximum depth of the table of contents can be controlled with endLevel.\nHeading example (level 2) This is an example of a level 2 heading.\nHeading example (level 3) This is an example of a level 3 heading.\nHeading example (level 4) This is an example of a level 4 heading.\nHeading example (level 5) This is an example of a level 5 heading.\nHeading example (level 6) This is an example of a level 6 heading.\nBlock quotes A simple paragraph block quote.\n This is a simple block quote. This is the second sentence.\n A pair of spaces at the end of a line can be used to force line breaks, e.g.:\n This is a multi line block quote.\nThis is the second line.\nThis is the third line.\n Lists Bulleted list  Fruit - Make sure you get your five-a-day. Meat  Beef Chicken   Vegetables.  Numbered List Numbered list items should all be numbered with number 1 so that the markdown render handles the consecutive numbering. This makes the file easier to edit and means the addition of one item in the middle does not cause a change to all the lines after it, e.g:\n1. Item one. This is some extra content for step 1. 1. Item two. 1. Sub-item A. 1. Sub-item B. 1. Item three.   Item one. This is some extra content for step 1. Item two.  Sub-item A. Sub-item B.   Item three.  Check List  Item 1. This is some extra content for item 1. Item 2.  Item 2a. Item 2b.   Item 3.  Definition list  Name Godzilla Birthplace Japan Color Green  Tables Tables should ideally have its columns aligned in the markdown for clarity in the raw markdown.\n## Don't do this | Artist | Album | Year | |-|-|-| | Michael Jackson | Thriller | 1982 | | Prince | Purple Rain | 1984 | | Beastie Boys | License to Ill | 1986 | ## Do this | Artist | Album | Year | |-----------------|----------------|------| | Michael Jackson | Thriller | 1982 | | Prince | Purple Rain | 1984 | | Beastie Boys | License to Ill | 1986 |  However both will produce the same result.\n   Artist Album Year     Michael Jackson Thriller 1982   Prince Purple Rain 1984   Beastie Boys License to Ill 1986    Links Links can be added using either standard markdown link syntax or using a Hugo shortcode. The advantage of the shortcode is that hugo will check for broken links when building the site so there are preferred.\nLinks to external sites, i.e. on the internet, should have  (external link) appended to the link title. This makes it clear to readers which links are local and which are external and therefore possibly not available if there is no access to the internet.\nAnchors You can link to headings on a page using its anchor. The anchor for a heading is the heading text with:\n All non-alphanumeric characters removed Spaces replaced with a - All characters made lower case Multiple consequtive - characters, e.g. --- are replaced with a single -  For example the heading Mr O'Neil's 1st Event (something) becomes as an anchor #mr-oneils-1st-event-something.\nShortcode link examples   A link to a heading anchor on this page.\n[link]({{\u003c ref \"#alerts\" \u003e}})    A link to a heading anchor on another page.\n[link]({{\u003c ref \"docs/style-guide/using-images#captions\" \u003e}})    A link to a heading anchor on another page, using only the page name. This will only work if the page name is unique\n[link]({{\u003c ref \"docs/style-guide/using-images#captions\" \u003e}})    A link to a heading anchor on page above this one, using a relative path.\n[link]({{\u003c ref \"../proxy/install.md#prerequisites\" \u003e}})    A link to a heading anchor on page above this one, using an absolute path, that will be converted to a relative one.\n[link]({{\u003c relref \"/docs/proxy/install.md#prerequisites\" \u003e}})    A link to a branch in the document tree, i.e. to the _index.md.\n[link]({{\u003c relref \"docs/HOWTOs\" \u003e}})    Markdown link examples Warning Avoid using markdown style links as they can’t be verified at site build time like the short code links can.  Markdown style links should not contain the .md extension as this will be stripped when the site is generated, e.g. for the following content:\n/content /en /docs /section-x /sub-section-a _index.md page1.md page2.md /sub-section-b _index.md page1.md page2.md  This will become:\n/docs /section-x /sub-section-a /page1 /page2 /sub-section-b /page1 /page2  in the rendered site.\n  A link to a heading anchor on this page.\n[link](#alerts)    A link to a heading anchor on another page, using a relative link.\n[link](../using-images#captions)    A link to a heading anchor on another page, using an absolute link.\n[link](/docs/style-guide/using-images#captions)    Code highlighting Inline code Inline code looks like this.\nCode blocks Code blocks should be surrounded with fences ```language-type and ``` with the language type always specified to ensure correct syntax highlighting. If the language type is not supplied then styling will be different to fenced blocks with a language.\nThis is a markdown example of a fenced code block containing XML content.\n```xml \u003croot\u003e \u003cchild attr=\"xxx\"\u003esome val\u003c/child\u003e \u003c/root\u003e ```  Some language types commonly used in this documentation are:\n bash java json text xml yaml  The list of supported languages can be found here (external).\nNote The fenced code block MUST have a language specified. This ensures the correct default styling is used and makes it explicitly clear to anyone editing the markdown what the content of the block is. If the content of the fenced block has no supported language or is just plain text then use language text.  The following are some example of rendered code blocks:\nPlain Text  id,date,time 1,6/2/2018,10:18 2,12/6/2017,5:58 3,6/7/2018,11:58    YAML  --- root: someKey: \"value\"    XML  \u003croot\u003e \u003cchild attr=\"xxx\"\u003e some val \u003c/child\u003e \u003c/root\u003e    bash  echo \"${VAR}\"     Alerts Warning block Quote Warning This is a warning that can contain markdown.  Page level warning Warning This is a warning that can contain markdown.\n Note block Quote Note This is a note that can contain markdown.  Page level info This is some info that can contain markdown.\n TODO block Quote TODO This is a TODO that can contain markdown.  Cards Cards can be used to display related conent side by side. Each card can contain markdown and/or Hugo short codes. The cards will be displayed horizopntally across the page.\nYAML  --- root: someKey: \"value\"    XML  \u003croot\u003e \u003cchild attr=\"xxx\"\u003esome val\u003c/child\u003e \u003c/root\u003e     ","categories":"","description":"Examples of various markdown and Hugo page elements.\n","excerpt":"Examples of various markdown and Hugo page elements.\n","ref":"/stroom-docs/hugo-docsy/docs/style-guide/basic-elements/","tags":["style"],"title":"Basic Page Elements"},{"body":"In this quick-start guide you will learn how to use Stroom to get from this CSV, which looks like this:\nid,guid,from_ip,to_ip,application 1,10990cde-1084-4006-aaf3-7fe52b62ce06,159.161.108.105,217.151.32.69,Tres-Zap 2,633aa1a8-04ff-442d-ad9a-03ce9166a63a,210.14.34.58,133.136.48.23,Sub-Ex ...  To this XML:\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\"?\u003e \u003cEvents xmlns:stroom=\"stroom\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\u003e \u003cEvent\u003e \u003cId\u003e1\u003c/Id\u003e \u003cGuid\u003e10990cde-1084-4006-aaf3-7fe52b62ce06\u003c/Guid\u003e \u003cFromIp\u003e159.161.108.105\u003c/FromIp\u003e \u003cToIp\u003e217.151.32.69\u003c/ToIp\u003e \u003cApplication\u003eTres-Zap\u003c/Application\u003e \u003c/Event\u003e \u003cEvent\u003e \u003cId\u003e2\u003c/Id\u003e \u003cGuid\u003e633aa1a8-04ff-442d-ad9a-03ce9166a63a\u003c/Guid\u003e \u003cFromIp\u003e210.14.34.58\u003c/FromIp\u003e \u003cToIp\u003e133.136.48.23\u003c/ToIp\u003e \u003cApplication\u003eSub-Ex\u003c/Application\u003e \u003c/Event\u003e ...  You will go from a clean vanilla Stroom to having a simple pipeline that takes in CSV data and outputs that data transformed into XML. Stroom is a generic and powerful tool for ingesting and processing data: it’s flexible because it’s generic so if you do want to start processing data we would recommend you follow this tutorial otherwise you’ll find yourself struggling.\nWe’re going to do the following:\n Get, configure, and run Stroom Get some data into Stroom Set up a pipeline to process the data Index the data Show the data on a dashboard  All the things we create here are available as a content pack (external link), so if you just wanted to see it running you could get there quite easily.\n Note: The CSV data used in mock_stroom_data.csv (linked to above) is randomly generated and any association with any real world IP address or name is entirely coincidental.\n ","categories":"","description":"","excerpt":"In this quick-start guide you will learn how to use Stroom to get from …","ref":"/stroom-docs/hugo-docsy/docs/quick-start-guide/","tags":"","title":"Quick Start Guide"},{"body":"Getting and Running Stroom For this quick start you want a simple single-node Stroom. You will want to follow these instructions. They do require Docker and Docker Compose, so make sure you’ve installed those first.\nAt the risk of sowing confusion you should know that there are different ways of running Stroom. Here are the full options:\n Run using Docker Hub images (recommended) Install a stroom v5.x release Install a stroom v6.x release From source you can:  Build and run from IntelliJ    Basic configuration Enable processing of data streams Automatic processing isn’t enabled by default: you might first want to check other settings (for example nodes, properties, and volumes). So we need to enable Stream Processing. This is in Tools -\u003e Jobs menu:: Opening the jobs menu    Next we need to enable Stream Processor jobs:\nEnabling stream processing    Below the list of jobs is the properties pane. The Stream Processor’s properties show the list of nodes. You should have one. You’ll need to enable it by scrolling right:\nEnabling the nodes for the stream processor    So now we’ve done that lets get data into stroom.\n","categories":"","description":"","excerpt":"Getting and Running Stroom For this quick start you want a simple …","ref":"/stroom-docs/hugo-docsy/docs/quick-start-guide/running/","tags":"","title":"Running Stroom"},{"body":" These basic sample guidelines assume that your Docsy site is deployed using Netlify and your files are stored in GitHub. You can use the guidelines “as is” or adapt them with your own instructions: for example, other deployment options, information about your doc project’s file structure, project-specific review guidelines, versioning guidelines, or any other information your users might find useful when updating your site. Kubeflow has a great example.\nDon’t forget to link to your own doc repo rather than our example site! Also make sure users can find these guidelines from your doc repo README: either add them there and link to them from this page, add them here and link to them from the README, or include them in both locations.\n We use Hugo to format and generate our website, the Docsy theme for styling and site structure, and Netlify to manage the deployment of the site. Hugo is an open-source static site generator that provides us with templates, content organisation in a standard directory structure, and a website generation engine. You write the pages in Markdown (or HTML if you want), and Hugo wraps them up into a website.\nAll submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.\nQuick start with Netlify Here’s a quick guide to updating the docs. It assumes you’re familiar with the GitHub workflow and you’re happy to use the automated preview of your doc updates:\n Fork the Goldydocs repo on GitHub. Make your changes and send a pull request (PR). If you’re not yet ready for a review, add “WIP” to the PR name to indicate it’s a work in progress. (Don’t add the Hugo property “draft = true” to the page front matter, because that prevents the auto-deployment of the content preview described in the next point.) Wait for the automated PR workflow to do some checks. When it’s ready, you should see a comment like this: deploy/netlify — Deploy preview ready! Click Details to the right of “Deploy preview ready” to see a preview of your updates. Continue updating your doc and pushing your changes until you’re happy with the content. When you’re ready for a review, add a comment to the PR, and remove any “WIP” markers.  Updating a single page If you’ve just spotted something you’d like to change while using the docs, Docsy has a shortcut for you:\n Click Edit this page in the top right hand corner of the page. If you don’t already have an up to date fork of the project repo, you are prompted to get one - click Fork this repository and propose changes or Update your Fork to get an up to date version of the project to edit. The appropriate page in your fork is displayed in edit mode. Follow the rest of the Quick start with Netlify process above to make, preview, and propose your changes.  Previewing your changes locally If you want to run your own local Hugo server to preview your changes as you work:\n  Follow the instructions in Getting started to install Hugo and any other tools you need. You’ll need at least Hugo version 0.45 (we recommend using the most recent available version), and it must be the extended version, which supports SCSS.\n  Fork the Goldydocs repo repo into your own project, then create a local copy using git clone. Don’t forget to use --recurse-submodules or you won’t pull down some of the code you need to generate a working site.\ngit clone --recurse-submodules --depth 1 https://github.com/google/docsy-example.git    Run hugo server in the site root directory. By default your site will be available at http://localhost:1313/. Now that you’re serving your site locally, Hugo will watch for changes to the content and automatically refresh your site.\n  Continue with the usual GitHub workflow to edit files, commit them, push the changes up to your fork, and create a pull request.\n  Creating an issue If you’ve found a problem in the docs, but you’re not sure how to fix it yourself, please create an issue in the Goldydocs repo. You can also create an issue about a specific page by clicking the Create Issue button in the top right hand corner of the page.\nUseful resources  Docsy user guide: All about Docsy, including how it manages navigation, look and feel, and multi-language support. Hugo documentation: Comprehensive reference for Hugo. Github Hello World!: A basic introduction to GitHub concepts and workflow.  ","categories":"","description":"How to contribute to the docs\n","excerpt":"How to contribute to the docs\n","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/contribution-guidelines/","tags":"","title":"Contribution Guidelines"},{"body":"","categories":"","description":"This section describes how to install Stroom, its dependencies and related applications.\n","excerpt":"This section describes how to install Stroom, its dependencies and …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/","tags":"","title":"Installation Guide"},{"body":"In addition to referencing content produced by a parent element it is often desirable to store content and reference it later. The following example of a CSV with a heading demonstrates how content can be stored in a variable and then referenced later on.\nInput This example will use a similar input to the one in the previous CSV example but also adds a heading line.\nDate,Time,IPAddress,HostName,User,EventType,Detail 01/01/2010,00:00:00,192.168.1.100,SOMEHOST.SOMEWHERE.COM,user1,logon, 01/01/2010,00:01:00,192.168.1.100,SOMEHOST.SOMEWHERE.COM,user1,create,c:\\test.txt 01/01/2010,00:02:00,192.168.1.100,SOMEHOST.SOMEWHERE.COM,user1,logoff,  Configuration \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\"\u003e \u003c!-- Match heading line (note that maxMatch=\"1\" means that only the first line will be matched by this splitter) --\u003e \u003csplit delimiter=\"\\n\" maxMatch=\"1\"\u003e \u003c!-- Store each heading in a named list --\u003e \u003cgroup\u003e \u003csplit delimiter=\",\"\u003e \u003cvar id=\"heading\" /\u003e \u003c/split\u003e \u003c/group\u003e \u003c/split\u003e \u003c!-- Match each record --\u003e \u003csplit delimiter=\"\\n\"\u003e \u003c!-- Take the matched line --\u003e \u003cgroup value=\"$1\"\u003e \u003c!-- Split the line up --\u003e \u003csplit delimiter=\",\"\u003e \u003c!-- Output the stored heading for each iteration and the value from group 1 --\u003e \u003cdata name=\"$heading$1\" value=\"$1\" /\u003e \u003c/split\u003e \u003c/group\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  Output \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003crecords xmlns=\"records:2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"records:2 file://records-v2.0.xsd\" version=\"3.0\"\u003e \u003crecord\u003e \u003cdata name=\"Date\" value=\"01/01/2010\" /\u003e \u003cdata name=\"Time\" value=\"00:00:00\" /\u003e \u003cdata name=\"IPAddress\" value=\"192.168.1.100\" /\u003e \u003cdata name=\"HostName\" value=\"SOMEHOST.SOMEWHERE.COM\" /\u003e \u003cdata name=\"User\" value=\"user1\" /\u003e \u003cdata name=\"EventType\" value=\"logon\" /\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata name=\"Date\" value=\"01/01/2010\" /\u003e \u003cdata name=\"Time\" value=\"00:01:00\" /\u003e \u003cdata name=\"IPAddress\" value=\"192.168.1.100\" /\u003e \u003cdata name=\"HostName\" value=\"SOMEHOST.SOMEWHERE.COM\" /\u003e \u003cdata name=\"User\" value=\"user1\" /\u003e \u003cdata name=\"EventType\" value=\"create\" /\u003e \u003cdata name=\"Detail\" value=\"c:\\test.txt\" /\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata name=\"Date\" value=\"01/01/2010\" /\u003e \u003cdata name=\"Time\" value=\"00:02:00\" /\u003e \u003cdata name=\"IPAdress\" value=\"192.168.1.100\" /\u003e \u003cdata name=\"HostName\" value=\"SOMEHOST.SOMEWHERE.COM\" /\u003e \u003cdata name=\"User\" value=\"user1\" /\u003e \u003cdata name=\"EventType\" value=\"logoff\" /\u003e \u003c/record\u003e \u003c/records\u003e  ","categories":"","description":"","excerpt":"In addition to referencing content produced by a parent element it is …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-splitter/1-2-simple-csv-example-with-heading/","tags":"","title":"Simple CSV example with heading"},{"body":"","categories":"","description":"Reference documentation for how to use Stroom.\n","excerpt":"Reference documentation for how to use Stroom.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/","tags":"","title":"User Guide"},{"body":"Getting data into Stroom Create the feed In real life you might configure Stroom to watch for new files in a directory. In this tutorial we’ll be uploading data but the result will be the same: raw event data sitting on a feed.\n  A lot of Stroom’s functionality is available through right-click context menus. If you right-click System in the tree you can create new things. Create a new folder and call it something like Stroom 101:\nCreating a folder      Right-click again and create a feed. The name needs to be capitalised, e.g. CSV_IN.\nCreating a feed      This will open a new tab for the feed. We want to add some data to the feed so click on Data at the top of the tab.\nThe Data section of a feed      Then click the green up arrow to get the file upload dialog.\n  We’re going to be putting in unprocessed events, known in Stroom as raw events. That’s the type of stream this feed will contain, so that’s the Stream Type you need to select.\nSetting the Stream Type      Download this file, then click choose file from the dialog, select the file, and then ok everything until you’re back at the feed.\n  That’s it, there’s now data in Stroom. You should be able to see it in the data table (you might need to click the refresh button:\nThe data on a feed    Now you can do all sorts of things with the data: transform it, visualise it, index it. It’s Pipelines that allow all these things to happen.\n","categories":"","description":"How to get data into Stroom. \n","excerpt":"How to get data into Stroom. \n","ref":"/stroom-docs/hugo-docsy/docs/quick-start-guide/feed/","tags":["feed"],"title":"Feeds"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/docs/","tags":"","title":"Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/news/news/","tags":"","title":"News About Stroom"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/news/releases/","tags":"","title":"New Releases"},{"body":"The following example uses a real world Apache log and demonstrates the use of regular expressions rather than simple ‘split’ tokenizers. The usage and structure of regular expressions is outside of the scope of this document but Data Splitter uses Java’s standard regular expression library that is POSIX compliant and documented in numerous places.\nThis example also demonstrates that the names and values that are output can be hard coded in the absence of field name information to make XSLT conversion easier later on. Also shown is that any match can be divided into further fields with additional expressions and the ability to nest data elements to provide structure if needed.\nInput 192.168.1.100 - \"-\" [12/Jul/2012:11:57:07 +0000] \"GET /doc.htm HTTP/1.1\" 200 4235 \"-\" \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET4.0C; .NET4.0E; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)\" 192.168.1.100 - \"-\" [12/Jul/2012:11:57:07 +0000] \"GET /default.css HTTP/1.1\" 200 3494 \"http://some.server:8080/doc.htm\" \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET4.0C; .NET4.0E; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)\"  Configuration \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\"\u003e \u003c!-- Standard Apache Format %h - host name should be ok without quotes %l - Remote logname (from identd, if supplied). This will return a dash unless IdentityCheck is set On. \\\"%u\\\" - user name should be quoted to deal with DNs %t - time is added in square brackets so is contained for parsing purposes \\\"%r\\\" - URL is quoted %\u003es - Response code doesn't need to be quoted as it is a single number %b - The size in bytes of the response sent to the client \\\"%{Referer}i\\\" - Referrer is quoted so that’s ok \\\"%{User-Agent}i\\\" - User agent is quoted so also ok LogFormat \"%h %l \\\"%u\\\" %t \\\"%r\\\" %\u003es %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\"\" combined --\u003e \u003c!-- Match line --\u003e \u003csplit delimiter=\"\\n\"\u003e \u003cgroup value=\"$1\"\u003e \u003c!-- Provide a regular expression for the whole line with match groups for each field we want to split out --\u003e \u003cregex pattern=\"^([^ ]+) ([^ ]+) \u0026#34;([^\u0026#34;]+)\u0026#34; \\[([^\\]]+)] \u0026#34;([^\u0026#34;]+)\u0026#34; ([^ ]+) ([^ ]+) \u0026#34;([^\u0026#34;]+)\u0026#34; \u0026#34;([^\u0026#34;]+)\u0026#34;\"\u003e \u003cdata name=\"host\" value=\"$1\" /\u003e \u003cdata name=\"log\" value=\"$2\" /\u003e \u003cdata name=\"user\" value=\"$3\" /\u003e \u003cdata name=\"time\" value=\"$4\" /\u003e \u003cdata name=\"url\" value=\"$5\"\u003e \u003c!-- Take the 5th regular expression group and pass it to another expression to divide into smaller components --\u003e \u003cgroup value=\"$5\"\u003e \u003cregex pattern=\"^([^ ]+) ([^ ]+) ([^ /]*)/([^ ]*)\"\u003e \u003cdata name=\"httpMethod\" value=\"$1\" /\u003e \u003cdata name=\"url\" value=\"$2\" /\u003e \u003cdata name=\"protocol\" value=\"$3\" /\u003e \u003cdata name=\"version\" value=\"$4\" /\u003e \u003c/regex\u003e \u003c/group\u003e \u003c/data\u003e \u003cdata name=\"response\" value=\"$6\" /\u003e \u003cdata name=\"size\" value=\"$7\" /\u003e \u003cdata name=\"referrer\" value=\"$8\" /\u003e \u003cdata name=\"userAgent\" value=\"$9\" /\u003e \u003c/regex\u003e \u003c/group\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  Output \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003crecords xmlns=\"records:2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"records:2 file://records-v2.0.xsd\" version=\"3.0\"\u003e \u003crecord\u003e \u003cdata name=\"host\" value=\"192.168.1.100\" /\u003e \u003cdata name=\"log\" value=\"-\" /\u003e \u003cdata name=\"user\" value=\"-\" /\u003e \u003cdata name=\"time\" value=\"12/Jul/2012:11:57:07 +0000\" /\u003e \u003cdata name=\"url\" value=\"GET /doc.htm HTTP/1.1\"\u003e \u003cdata name=\"httpMethod\" value=\"GET\" /\u003e \u003cdata name=\"url\" value=\"/doc.htm\" /\u003e \u003cdata name=\"protocol\" value=\"HTTP\" /\u003e \u003cdata name=\"version\" value=\"1.1\" /\u003e \u003c/data\u003e \u003cdata name=\"response\" value=\"200\" /\u003e \u003cdata name=\"size\" value=\"4235\" /\u003e \u003cdata name=\"referrer\" value=\"-\" /\u003e \u003cdata name=\"userAgent\" value=\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET4.0C; .NET4.0E; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)\" /\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata name=\"host\" value=\"192.168.1.100\" /\u003e \u003cdata name=\"log\" value=\"-\" /\u003e \u003cdata name=\"user\" value=\"-\" /\u003e \u003cdata name=\"time\" value=\"12/Jul/2012:11:57:07 +0000\" /\u003e \u003cdata name=\"url\" value=\"GET /default.css HTTP/1.1\"\u003e \u003cdata name=\"httpMethod\" value=\"GET\" /\u003e \u003cdata name=\"url\" value=\"/default.css\" /\u003e \u003cdata name=\"protocol\" value=\"HTTP\" /\u003e \u003cdata name=\"version\" value=\"1.1\" /\u003e \u003c/data\u003e \u003cdata name=\"response\" value=\"200\" /\u003e \u003cdata name=\"size\" value=\"3494\" /\u003e \u003cdata name=\"referrer\" value=\"http://some.server:8080/doc.htm\" /\u003e \u003cdata name=\"userAgent\" value=\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET4.0C; .NET4.0E; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)\" /\u003e \u003c/record\u003e \u003c/records\u003e  ","categories":"","description":"","excerpt":"The following example uses a real world Apache log and demonstrates …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-splitter/1-3-complex-example/","tags":"","title":"Complex example with regex and user defined names"},{"body":"File names Ideally files and directories should be named using lower-kebab-case, e.g. site-structure.md.\nDirectory structure All page content, i.e. markdown, is located underneath content/en. This directory has one sub-directory for each of the top nav bar items.\nStroom-Docs top level sections Each of the following sections can have a different styling that is appropriate to its content, e.g. documentation vs blog.\nLanding Page The landing page is located at content/en/_index.html. This is the page that users will initially see, unless visiting via a direct link.\nAbout (about) A single page describing what stroom is\nDocumentation (docs) This is where all the documentation for installing, administering and using stroom is located.\nNews/Releases (news) This is a blog type section that contains a page for each new Stroom release and a set of blog posts for Stroom news items. The pages in the two sub-sections (news and releases) are displayed in chronalogical order based on the date key in the page’s front matter.\nNews (news/news) This sub section has one .md file per news item. Each file should be named in the form YYYYMMDD-\u003cname\u003e.md. The date key should be set in the front matter to match the date of the file. Hugo will use this date key to order the files in the menu. The date should be set in ISO 8601 date format, i.e.\ndate: 2021-07-09  Releases (news/releases) Each new minor version release of Stroom should have a file in this directory. They should be named in the form vXX.YY.md where XX is the zero padded major version and YY is the zero padded minor version, e.g. v07.01. The zero padding is to ensure correct ordering by default withouth having to resort to using weight in the front matter.\nThe front matter should be set along these lines:\n--- title: \"Version 7.0\" linkTitle: \"7.0\" date: 2021-07-07 description: \u003e Key new features and changes present in v7.0 of Stroom and Stroom-Proxy. ---  Community (community) This section provides information for people wanting to contribute to the development of Stroom and its peripheral repositories. This can include developer documentation for building and developing Stroom.\nContent files The docs and news top level sections have a tree structure for the co\nFront matter Note See Hugo Front Matter (external link) for the full list of metadata keys that can be set.  Front matter in Hugo is a set of meta data at the top of each page that controls which menus include the page as well as providing iformation about the page, e.g.\n--- title: \"Site Structure\" linkTitle: \"Site Structure\" #weight: date: 2021-07-20 tags: - style description: \u003e Describes the file and directory structure for the site. ---  Versioning Note See Docsy Versioning (external link) for details on how to configure versioning.  The site will be versioned and the versions will be tied to each minor release of Stroom, e.g 7.0, 7.1, 8.0 etc. Each version will live in its own git branch, e.g. 7.0, 7.1, 8.0, etc.\nThe site will also have a Legacy version which will live on the branch legacy. This will contain the documentation as it was when it was migrated to Hugo. The legacy documentation contains content relevant to multiple versions of Stroom. All subsequent versions will only contain content relevant to that minor version of Stroom, or will be clearly marked as being out of date, e.g.:\nWarning This section is out of date.  ","categories":"","description":"Describes the file and directory structure for the site.\n","excerpt":"Describes the file and directory structure for the site.\n","ref":"/stroom-docs/hugo-docsy/docs/style-guide/site-structure/","tags":"","title":"Site Structure"},{"body":" TODO Add the resources dir to /assets and fix all the img links  The HOWTOs are broken down into different functional concepts or areas of Stroom.\nNOTE: These HOWTOs will match the development of Stroom and as a result, various elements will be updated over time, including screen captures. In some instances, screen captures will contain timestamps and so you may note inconsistent date or time movements within a complete HOWTO, although if a sequence of captures is contained within a section of a document, they all will be replaced.\nInstallation The Installation Scenarios HOWTO is provided to assist users in setting up a number of different Stroom deployments.\nEvent Feed Processing The Event Feed Processing HOWTO is provided to assist users in setting up Stroom to process inbound event logs and transform them into the Stroom Event Logging XML Schema.\nThe Apache HTTPD Event Feed is interwoven into other HOWTOs that utilise this feed as a datasource.\nReference Feeds Reference Feeds are used to provide look up data for a translation. The reference feed HOWTOs illustrate how to create reference feeds and how to use look up reference data maps to enrich the data you are processing.\nSearches and Indexing This section covers Indexing and Searching for data in Stroom\n Search using bash Elasticsearch integration Solr integration  General Raw Source Tracking show how to associate a processed Event with the source line that generated it\nOther topics in this section are\n Feed Management. Tasks  ","categories":"","description":"This is a series of *HOWTOs* that are designed to get one started with Stroom.\n","excerpt":"This is a series of *HOWTOs* that are designed to get one started with …","ref":"/stroom-docs/hugo-docsy/docs/howtos/","tags":"","title":"How Tos"},{"body":"Create a pipeline Pipelines control how data is processed in Stroom. Typically you’re going to want to do a lot of the same stuff for every pipeline, i.e. similar transformations, indexing, writing out data. You can actually create a template pipeline and inherit from it, tweaking what you need to for this or that feed. We’re not doing that now because we want to show how to create one from scratch.\nCreating a pipeline     Create a pipeline by right-clicking our Stroom 101 folder. Call it something like CSV to XML pipeline. Select Structure from the top of the new tab. This is the most important view for the pipeline because it shows what will actually happen on the pipeline. Check Advanced Mode so that we can actually edit things.  Turn on 'Advanced Mode'    We already have a Source element. Unlike most other pipeline elements this isn’t something we need to configure. It’s just there to show the starting point. Data gets into the pipeline via other means - we’ll describe this in detail later.\nAdd a data splitter Data splitters are powerful, and there is a lot we can say about them. Here we’re just going to make a basic one.\nCreate a CSV splitter We have CSV in the following form:\nid,guid,from_ip,to_ip,application 1,10990cde-1084-4006-aaf3-7fe52b62ce06,159.161.108.105,217.151.32.69,Tres-Zap 2,633aa1a8-04ff-442d-ad9a-03ce9166a63a,210.14.34.58,133.136.48.23,Sub-Ex  To process this we need to know if there’s a header row, and what the delimiters are. This is a job for a Data Splitter.\nThe splitter is actually a type of Text Converter, so lets create one of those:\nCreate the CSV splitter    Call it something like CSV splitter. In the new tab you need to tell the Text Converter that it’ll be a Data Splitter:\nConfiguring the data splitter    Now go to the Conversion tab. What you need to put in here is specific to the built-in Data Splitter functionality, so I’m just going to tell you what you’re going to need:\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.1.xsd\" version=\"3.0\"\u003e \u003c!-- The first line contains the field names --\u003e \u003csplit delimiter=\"\\n\" maxMatch=\"1\"\u003e \u003cgroup\u003e \u003csplit delimiter=\",\" containerStart=\"\u0026#34;\" containerEnd=\"\u0026#34;\"\u003e \u003cvar id=\"heading\" /\u003e \u003c/split\u003e \u003c/group\u003e \u003c/split\u003e \u003c!-- All subsequent lines are records --\u003e \u003csplit delimiter=\"\\n\"\u003e \u003cgroup\u003e \u003csplit delimiter=\",\" containerStart=\"\u0026#34;\" containerEnd=\"\u0026#34;\"\u003e \u003cdata name=\"$heading$1\" value=\"$1\" /\u003e \u003c/split\u003e \u003c/group\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  You can see that it uses the data-splitter-v3.0.1.xsd that we imported earlier. Save it by clicking the save icon .\nSo we now have a configured, re-usable data splitter for CSV files that have headers. We need to add this to our pipeline as a filter, so head back to the pipeline’s Structure section and add a DSParser, as below. Call it something like CSV splitter filter: Adding a dsParser    Now we have a pipeline that looks like this:\nPipeline with a CSV splitter    Click on the CSV splitter filter element and the pane below will show it’s properties. We need to tell it to use our newly created CSV splitter. Double click the textConverter property and change Value to the actual CSV splitter:\nConfiguring the CSV splitter    Test the csv splitter So now we have CSV data in Stroom and a pipeline that is configured to process CSV data. We’ve done a fair few things so far and are we sure the pipeline is correctly configured? We can do some debugging and find out.\nIn Stroom you can step through you records and see what the output is at each stage. It’s easy to start doing this. The first thing to do is to open your CSV_FEED feed, then click the big blue stepping button at the bottom right:\nStarting pipeline stepping    You’ll be asked to select a pipeline:\nSelecting a pipeline to step through    Now you get a view that’s similar to your feed view, except it also shows the pipeline:\nDebugging - source data    It also has stepping controls. Click the green step forward icon . You should see something like this:\nStepping through the CSV data    Great! If you don’t see this then there’s something wrong. Click on CSV splitter filter. You’ll see the conversion code and hopefully some errors. Some issues might be: did you remember to import the data splitter schema into Stroom? Did you remember to confgure the Text Converter to be a Data Splitter?\nIf everything went fine then click the step forward button a few more times and you’ll see the yellow selection move down as you process each row.\nWhat we actually want to see is the output from the Text Converter, so click on CSV splitter filter. You’ll see the conversion code we entered earlier and below two panes, one containing the CSV and one containing the split-up text, in XML form:\nThe output from a working data splitter    So here we have some XML in a basic format we call the records format. You can see the schema for records in the XML schemas folder.\nAdd XSLT to transform records format XML into something else Create the XSLT filter This process is very similar to creating the CSV splitter:\n Create the XSLT filter Add it to the pipeline Step through to make sure it’s doing what we expect  Create the XSLT filter, calling it something like XSLT:\nCreate the XSLT filter    On the new tab click on XSLT. This is another big text field but this one accepts XSLT. This one will be very basic and just takes the split up data and puts it into fields. The XSLT for this is below but if you’d like to tinker then go ahead.\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\" ?\u003e \u003cxsl:stylesheet xpath-default-namespace=\"records:2\" xmlns:stroom=\"stroom\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"2.0\"\u003e \u003cxsl:template match=\"records\"\u003e \u003cEvents\u003e \u003cxsl:apply-templates /\u003e \u003c/Events\u003e \u003c/xsl:template\u003e \u003cxsl:template match=\"record\"\u003e \u003cxsl:variable name=\"id\" select=\"data[@name='id']/@value\" /\u003e \u003cxsl:variable name=\"guid\" select=\"data[@name='guid']/@value\" /\u003e \u003cxsl:variable name=\"from_ip\" select=\"data[@name='from_ip']/@value\" /\u003e \u003cxsl:variable name=\"to_ip\" select=\"data[@name='to_ip']/@value\" /\u003e \u003cxsl:variable name=\"application\" select=\"data[@name='application']/@value\" /\u003e \u003cEvent\u003e \u003cId\u003e\u003cxsl:value-of select=\"$id\" /\u003e\u003c/Id\u003e \u003cGuid\u003e\u003cxsl:value-of select=\"$guid\" /\u003e\u003c/Guid\u003e \u003cFromIp\u003e\u003cxsl:value-of select=\"$from_ip\" /\u003e\u003c/FromIp\u003e \u003cToIp\u003e\u003cxsl:value-of select=\"$to_ip\" /\u003e\u003c/ToIp\u003e \u003cApplication\u003e\u003cxsl:value-of select=\"$application\" /\u003e\u003c/Application\u003e \u003c/Event\u003e \u003c/xsl:template\u003e \u003c/xsl:stylesheet\u003e  Make sure you save it.\nGo back to the Structure section of the pipeline and add an XSLTFilter element. Call it something like XSLT filter.\nAdd the XSLT filter    Select the XSLT filter and configure it to use the actual XSLT you just created by double-clicking xslt in the properties:\nConfiguring the XSLT filter    In the dialog make sure the value is the XSLT filter. Save the pipeline.\nTest the XSLT filter We’re going to test this in the same way we tested the CSV splitter, by clicking the big blue button on the feed. Click the step forward button a few times to make sure it’s working then click on the XSLT element. This time you should see the XSLT filter there too, as well as the basic XML being transformed into more useful XML:\nDebugging the XSLT filter    Fantastic! Data converted! Well done if you’ve got this far. Really, there are lots of steps and things that could go wrong and you’ve persevered. There’s a few more things to get this pipeline ready for doing this task for real. We need to get this data to a destination.\nOutputting the transformed data Create the XML writer What’s an XML Writer and why do you need one? The XSLT filter doesn’t actually write XML but instead just passes XML events from one filter to another. In order to write XML out you need an XML writer. You don’t need to create one outside the pipeline (in the way you did with the CSV splitter and the XSLT filter). An XML writer is just added to the pipeline like this:\nAddnig an XML Writer    That’s it, no other configuration necessary.\nCreate the destination We need to do something with the serialised XML. We’ll write it to a stream. To do this we create a stream appender:\nCreating a stream appender    Unlike the Source element this element needs to be configured. We need to configure two things: the streamType and the destination feed.\nSetting the feed We’ll send the output to the CSV_FEED - all data associated with this feed will be in the same place. To do that we edit the feed property and set it to CSV_FEED:\nChange the feed property    We also need to edit the streamType property:We set the streamType to Events:\nChange the streamType property    Setting the streamType    That’s it! Our pipeline is configured!\nTest the destination We can test the XML writer and the streamAppender using the same stepping feature. Make sure you’ve saved the pipeline and set a new stepping session running. If you click on the stream appender you’ll see something like this:\nThe final output from the pipeline    Set the pipeline running Obviously you don’t want to step through your data one by one. This all needs automation, and this is what Processors are for. The processor works in the background to take any unprocessed data from a feed and put it through a pipeline. So far everything on our EXAMPLE_IN feed is unprocessed.\nCreate a processor Processors are created from the Processors section of the pipeline:\nThe processors section of the pipeline    Click the add button and configure the huge dialog. You only need to set the incoming feed and the stream types:\nConfigure the new processor    Now you’ll see a very wide table looking something like this:\nThe new processor    This shows two things:\n The processor for the pipeline The filter that determines what data is passed to the processor  If you scroll all the way over to the right you’ll see the Enabled checkbox:\nThe enabled checkbox    Check enabled for the processor and the filter you’ve just created. This is it, everything we’ve done is about to start working on its own, just like it would in a real configuration.\nIf you keep refreshing this table it will show you the processing status which should change after a few seconds to show that the data you have uploaded is being or has been processed. Once this has happened you should be able to open the destination feed and see the output data (or errors if there were any).\nThe output of the pipeline    You can see that there are the Raw Events and the processed Events. If you click on the Events then you can see all the XML that we’ve produced.\nTroubleshooting If nothing happens you might need to enable stream processing. This was explained earlier.\nNow you’ve processed your data you can go ahead and index it.\n","categories":"","description":"Creating pipelines to process and transform data.\n","excerpt":"Creating pipelines to process and transform data.\n","ref":"/stroom-docs/hugo-docsy/docs/quick-start-guide/process/","tags":["processing","pipeline","data-splitter"],"title":"Pipeline Processing"},{"body":"Example multi line file where records are split over may lines. There are various ways this data could be treated but this example forms a record from data created when some fictitious query starts plus the subsequent query results.\nInput 09/07/2016 14:49:36 User = user1 09/07/2016 14:49:36 Query = some query 09/07/2016 16:34:40 Results: 09/07/2016 16:34:40 Line 1: result1 09/07/2016 16:34:40 Line 2: result2 09/07/2016 16:34:40 Line 3: result3 09/07/2016 16:34:40 Line 4: result4 09/07/2009 16:35:21 User = user2 09/07/2009 16:35:21 Query = some other query 09/07/2009 16:45:36 Results: 09/07/2009 16:45:36 Line 1: result1 09/07/2009 16:45:36 Line 2: result2 09/07/2009 16:45:36 Line 3: result3 09/07/2009 16:45:36 Line 4: result4  Configuration \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\"\u003e \u003c!-- Match each record. We want to treat the query and results as a single event so match the two sets of data separated by a double new line --\u003e \u003cregex pattern=\"\\n*((.*\\n)+?\\n(.*\\n)+?\\n)|\\n*(.*\\n?)+\"\u003e \u003cgroup\u003e \u003c!-- Split the record into query and results --\u003e \u003cregex pattern=\"(.*?)\\n\\n(.*)\" dotAll=\"true\"\u003e \u003c!-- Create a data element to output query data --\u003e \u003cdata name=\"query\"\u003e \u003cgroup value=\"$1\"\u003e \u003c!-- We only want to output the date and time from the first line. --\u003e \u003cregex pattern=\"([^\\t]*)\\t([^\\t]*)[\\t]*([^=:]*)[=:]*(.*)\" maxMatch=\"1\"\u003e \u003cdata name=\"date\" value=\"$1\" /\u003e \u003cdata name=\"time\" value=\"$2\" /\u003e \u003cdata name=\"$3\" value=\"$4\" /\u003e \u003c/regex\u003e \u003c!-- Output all other values --\u003e \u003cregex pattern=\"([^\\t]*)\\t([^\\t]*)[\\t]*([^=:]*)[=:]*(.*)\"\u003e \u003cdata name=\"$3\" value=\"$4\" /\u003e \u003c/regex\u003e \u003c/group\u003e \u003c/data\u003e \u003c!-- Create a data element to output result data --\u003e \u003cdata name=\"results\"\u003e \u003cgroup value=\"$2\"\u003e \u003c!-- We only want to output the date and time from the first line. --\u003e \u003cregex pattern=\"([^\\t]*)\\t([^\\t]*)[\\t]*([^=:]*)[=:]*(.*)\" maxMatch=\"1\"\u003e \u003cdata name=\"date\" value=\"$1\" /\u003e \u003cdata name=\"time\" value=\"$2\" /\u003e \u003cdata name=\"$3\" value=\"$4\" /\u003e \u003c/regex\u003e \u003c!-- Output all other values --\u003e \u003cregex pattern=\"([^\\t]*)\\t([^\\t]*)[\\t]*([^=:]*)[=:]*(.*)\"\u003e \u003cdata name=\"$3\" value=\"$4\" /\u003e \u003c/regex\u003e \u003c/group\u003e \u003c/data\u003e \u003c/regex\u003e \u003c/group\u003e \u003c/regex\u003e \u003c/dataSplitter\u003e  Output \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003crecords xmlns=\"records:2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"records:2 file://records-v2.0.xsd\" version=\"2.0\"\u003e \u003crecord\u003e \u003cdata name=\"query\"\u003e \u003cdata name=\"date\" value=\"09/07/2016\" /\u003e \u003cdata name=\"time\" value=\"14:49:36\" /\u003e \u003cdata name=\"User\" value=\"user1\" /\u003e \u003cdata name=\"Query\" value=\"some query\" /\u003e \u003c/data\u003e \u003cdata name=\"results\"\u003e \u003cdata name=\"date\" value=\"09/07/2016\" /\u003e \u003cdata name=\"time\" value=\"16:34:40\" /\u003e \u003cdata name=\"Results\" /\u003e \u003cdata name=\"Line 1\" value=\"result1\" /\u003e \u003cdata name=\"Line 2\" value=\"result2\" /\u003e \u003cdata name=\"Line 3\" value=\"result3\" /\u003e \u003cdata name=\"Line 4\" value=\"result4\" /\u003e \u003c/data\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata name=\"query\"\u003e \u003cdata name=\"date\" value=\"09/07/2016\" /\u003e \u003cdata name=\"time\" value=\"16:35:21\" /\u003e \u003cdata name=\"User\" value=\"user2\" /\u003e \u003cdata name=\"Query\" value=\"some other query\" /\u003e \u003c/data\u003e \u003cdata name=\"results\"\u003e \u003cdata name=\"date\" value=\"09/07/2016\" /\u003e \u003cdata name=\"time\" value=\"16:45:36\" /\u003e \u003cdata name=\"Results\" /\u003e \u003cdata name=\"Line 1\" value=\"result1\" /\u003e \u003cdata name=\"Line 2\" value=\"result2\" /\u003e \u003cdata name=\"Line 3\" value=\"result3\" /\u003e \u003cdata name=\"Line 4\" value=\"result4\" /\u003e \u003c/data\u003e \u003c/record\u003e \u003c/records\u003e  ","categories":"","description":"","excerpt":"Example multi line file where records are split over may lines. There …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-splitter/1-4-multi-line-example/","tags":"","title":"Multi Line Example"},{"body":"Before you can visualise your data with dashboards you have to index the data; First I opted for creating a specific volume to hold my data, just because I wanted to keep my shards away from the default volumes;\nGo to the Tools ➔ Volumes menu\nVolumes    Once the volumes dialogue opens click the blue plus sign at the top left of the window to add a new one\nVolume list    Select the node where you want the volume to be and the path you want to create, (because we are following the quick-start guide we just have one node and limited size, but we do want to set it as active so we can write documents to it and we want it to be public, because we might want other indexes to use it; your needs might be different.\nVolume edit    Click ok and we’re good to go.\nThen we can create an index by selecting index item in the explorer tree. You do this in the same way you create any of the items. Just select/create a folder that you want to create the new index in and right click, select New Index.\nNew index    Choose a name for your new index\nIndex name    In the settings tab we need to specify the volume where we will store our shards\nNow you need to add fields to this index.\nFirstly there are two mandatory fields that need to be added: StreamId and EventId\nBoth should be of type Id, stored and indexed with the Keyword analyser\nIndex field    If you were following the quick-start instruction on ingesting the mock_stroom_data.csv, we’ll use those fields here.\nOpen the fields tab then create the following fields:\n   Name Type Store Index Positions Analyser Case Sensitive     StreamId Id Yes Yes No Keyword false   EventId Id Yes Yes No Keyword false   Id Text Yes Yes No Keyword false   Guid Text Yes Yes No Alpha numeric false   FromIp Text Yes Yes Yes Keyword false   ToIp Text Yes Yes Yes Keyword false   Application Text Yes Yes Yes Alpha numeric false    We are creating fields in our index to match the fields we have ingested to provide a place for the data to go that Stroom can reference.\nIndex field list    When you’ve done that, save the new index.\nNow create a new XSLT. We are going to convert xml data into something indexable by Stroom.\nNew XSLT    To make things manageable we create our new XSLT with the same name as the index in the same folder. After you’ve set the name just save it and close it, we’ll add some code in there later.\nXSLT name    XSLT settings    Now we get to send data to the index\nCreate a new pipeline called Indexing (we are going to make this a template for future indexing requirements).\nNew pipeline    Edit the structure of the pipeline\nAdd the following element types with the specified names\n   Type Name     XMLParser parser   SplitFilter splitFilter   IdEnrichmentFilter idEnrichmentFilter   XSLTFilter xsltFilter   IndexingFilter indexingFilter    So it looks like this (excluding the ReadRecordCountFilter and WriteRecordCountFilter elements)\nIndexing pipeline    Once the elements have been added you need to set the following property on the elements:\n   Element Property Value     splitFilter splitCount 100    To do this we select the element then double click the property value in the property panel which is below it.\nPipeline element property    The dialogue pops up where you can set the values\nPipeline edit element property    Save the pipeline, using the top left icon , then close the pipeline tab.\nNow create a new pipeline\nPipeline name    Which we will base on our new “Indexing” template pipeline\nOn our structure tab\nPipeline structure    Click in the “Inherit From” window\nPipeline inheritance    Select our Indexing pipeline template that we just created\nPipeline inheritance selection    Now we need to set the XSLT property on the xsltFilter to point at the XSLT we created earlier and set the index on the indexFilter to point to the index we created. This will appear as below (excluding the ReadRecordCountFilter and WriteRecordCountFilter elements)\nPipeline XSLT properties    Once that’s done you can save your new pipeline\nNext we need to create an XSLT that the IndexingFilter understands.\nOpen the feed we created in the quick-start guide if you find some processed data in your feed - i.e. browse the data\nData browsing    Click the stepping button Select your new pipeline\nStepping choose pipeline    Paste the following into your xsltFilter\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003cxsl:stylesheet xmlns=\"records:2\" xmlns:stroom=\"stroom\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"2.0\"\u003e \u003cxsl:template match=\"/Events\"\u003e \u003crecords xsi:schemaLocation=\"records:2 file://records-v2.0.xsd\" version=\"2.0\"\u003e \u003cxsl:apply-templates /\u003e \u003c/records\u003e \u003c/xsl:template\u003e \u003cxsl:template match=\"Event\"\u003e \u003crecord\u003e \u003cdata name=\"StreamId\"\u003e \u003cxsl:attribute name=\"value\" select=\"@StreamId\" /\u003e \u003c/data\u003e \u003cdata name=\"EventId\"\u003e \u003cxsl:attribute name=\"value\" select=\"@EventId\" /\u003e \u003c/data\u003e \u003cxsl:apply-templates select=\"*\" /\u003e \u003c/record\u003e \u003c/xsl:template\u003e \u003c!-- Index the Id --\u003e \u003cxsl:template match=\"Id\"\u003e \u003cdata name=\"Id\"\u003e \u003cxsl:attribute name=\"value\" select=\"text()\" /\u003e \u003c/data\u003e \u003c/xsl:template\u003e \u003c!-- Index the Guid --\u003e \u003cxsl:template match=\"Guid\"\u003e \u003cdata name=\"Guid\"\u003e \u003cxsl:attribute name=\"value\" select=\"text()\" /\u003e \u003c/data\u003e \u003c/xsl:template\u003e \u003c!-- Index the FromIp --\u003e \u003cxsl:template match=\"FromIp\"\u003e \u003cdata name=\"FromIp\"\u003e \u003cxsl:attribute name=\"value\" select=\"text()\" /\u003e \u003c/data\u003e \u003c/xsl:template\u003e \u003c!-- Index the ToIp --\u003e \u003cxsl:template match=\"ToIp\"\u003e \u003cdata name=\"ToIp\"\u003e \u003cxsl:attribute name=\"value\" select=\"text()\" /\u003e \u003c/data\u003e \u003c/xsl:template\u003e \u003c!-- Index the Application --\u003e \u003cxsl:template match=\"Application\"\u003e \u003cdata name=\"Application\"\u003e \u003cxsl:attribute name=\"value\" select=\"text()\" /\u003e \u003c/data\u003e \u003c/xsl:template\u003e \u003c/xsl:stylesheet\u003e  Which should look like this\nStepping edit XSLT    What we are trying to do is turn the data into Stroom record format. This is basically name value pairs that we pass to the index. Step through the data using the top right arrows to ensure the XSLT produces correct output.\nWe’re nearly there for indexing the data - you just need to tell the pipeline to pick up all processed data and index it.\nGo back to your pipeline and go to the processors tab.\nProcessors    Add a filter using and tell it to process all Events data when the filter dialogue opens so it looks like this\nEdit stream filter    Enable the processor and the filter by clicking the enabled tick boxes\nEnable processors    Stroom should then index the data, assuming everything is correct\nIf there are errors you’ll see error streams produced in the data browsing page, i.e. where you would normally see your processed and raw data. If no errors have occurred, there will be no rows in the data page.\nIf it all goes to plan you’ll see index shards appear if you open the index you created and click the shards tab.\nIndex shards    The document count doesn’t update immediately so don’t worry if the count is 0. The count is updated on shard flush and happens in the background.\nNow that we have finished indexing we can display data on a dashboard.\n","categories":"","description":"Indexing the ingested data.\n","excerpt":"Indexing the ingested data.\n","ref":"/stroom-docs/hugo-docsy/docs/quick-start-guide/indexing/","tags":["index"],"title":"Indexing"},{"body":"There are various elements used in a Data Splitter configuration to control behaviour. Each of these elements can be categorised as one of the following:\n","categories":"","description":"","excerpt":"There are various elements used in a Data Splitter configuration to …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-splitter/element-reference/","tags":"","title":"Element Reference"},{"body":"Create a new dashboard the same way you create anything else\nNew dashboard    On the query pane click the settings button on the top right of the panel.\nChoose the index you just created as your data source.\nDashboard settings    Now add a term to the query to get a handle on the data\nDashboard query add term    For our simple example we’re using a wildcard that captures all documents with an Id set.\nDashboard query edit term    Within the table panel we now need to set a few defaults. On the table pane click the settings button on the top right of the panel. Extract Values needs to be ticked. If grouping is to be used and the content of the groups is to be viewed then Show Group Detail should also be ticked. The Maximum Results field may also be changed from default if required to limit the results or if more results are expected than the default value. Be aware that setting this value too high may result in excessive memory being used by the query process though.\nWe now need to select a pipeline to display the results in the table by setting Extraction Pipeline. The simplest way is to create and save a new pipeline based on the existing Search Extraction template pipeline. Within this new pipeline, use either the XSLT used for indexing the data or preferably a copy of this XSLT saved elsewhere. The extraction pipeline and the dashboard itself should then be saved.\nIn the table panel we can add the fields we are interested in, in this case we wanted to sort the application field and count how many time the application name appears.\nDashboard table fields    If at this point, we decide that we’d like to see additional fields in the table extracted from each record then the Extraction Pipeline XSLT can be modified to extract them from the Event:\n... \u003cxsl:template match=\"/xpath/to/usefulField\"\u003e \u003cdata name=\"UsefulField\"\u003e \u003cxsl:attribute name=\"value\" select=\"text()\" /\u003e \u003c/data\u003e \u003c/xsl:template\u003e ...  To be able to select this new field from the table drop-down, it needs to be added back into the list of fields in the original index:\n   Name Type Store Index Positions Analyser Case Sensitive     UsefulField Text No No No Keyword false    If any additionals are made at this point, the index must first be saved and then the dashboard closed and reopened. UsefulField will then be available as a drop-down option in the table.\nStart the query and we should get this\nDashboard table    Then we can add an element from the top again and this time use visualisation\nDashboard add visualisation    In the visualisation panel that has been added to the bottom, click the settings button on the top right of the panel.\nIn our example we have used the Bubble visualisation\nVisualisation settings - basic    Visualisation settings data    Which gives us this visualisation when the query is executed\nBubble visualisation    Where you can hover over elements and get a summary of that representation.\nBubble visualisation legends    ","categories":"","description":"","excerpt":"Create a new dashboard the same way you create anything else\nNew …","ref":"/stroom-docs/hugo-docsy/docs/quick-start-guide/dashboard/","tags":["dashboard","visualisation","query"],"title":"Dashboards"},{"body":"","categories":"","description":"Stroom Proxy acts as a proxy when sending data to a Stroom instance. Stroom Proxy has various modes such as storing, aggregating and forwarding the received data. Stroom Proxies can forward to other Stroom Proxy instances.\n","excerpt":"Stroom Proxy acts as a proxy when sending data to a Stroom instance. …","ref":"/stroom-docs/hugo-docsy/docs/proxy/","tags":["proxy"],"title":"Stroom Proxy"},{"body":"","categories":"","description":"This section is intended for developers contributing to the development of the Stroom software. It describes how to do some common tasks, such as releasing and building documentation.\n","excerpt":"This section is intended for developers contributing to the …","ref":"/stroom-docs/hugo-docsy/docs/dev-guide/","tags":"","title":"Developer Guide"},{"body":"The \u003cgroup\u003e and \u003cdata\u003e elements can reference match groups from parent expressions or from stored matches in variables. In the case of the \u003cgroup\u003e element, referenced values are passed on to child expressions whereas the \u003cdata\u003e element can use match group references for name and value attributes. In the case of both elements the way of specifying references is the same.\n","categories":"","description":"","excerpt":"The \u003cgroup\u003e and \u003cdata\u003e elements can reference match groups from parent …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-splitter/match-reference/","tags":"","title":"Match References, Variables and Fixed Strings"},{"body":"This page can be used as a reference for finding icons and their filenames to use them in the documentation.\nGeneral icons add-above.svg  add-below.svg  add.svg  alert.svg  clear.svg  close-grey.svg  close.svg  collapse-up.svg  copy.svg  database.svg  delete.svg  deleted.svg  dependencies.svg  disable.svg  down.svg  download.svg  drop-down.svg  edit.svg  ellipses-horizontal.svg  ellipses-vertical.svg  error.svg  expand-down.svg  explorer.svg  fast-backward-green.svg  fast-backward.svg  fast-forward-green.svg  fast-forward.svg  fatal.svg  favourites.svg  feed.svg  field.svg  file-formatted.svg  file-raw.svg  file.svg  filter.svg  folder-tree.svg  folder.svg  format.svg  function.svg  generate.svg  help.svg  hide.svg  history.svg  info-deleted.svg  info-warning.svg  info.svg  jobs.svg  key.svg  lock-green.svg  locked-amber.svg  logo.svg  logout.svg  monitoring.svg  move.svg  nodes.svg  oo.svg  operator.svg  password.svg  pause.svg  play-green.svg  play.svg  process.svg  properties.svg  raw.svg  refresh-green.svg  refresh.svg  remove.svg  ruleset.svg  save.svg  saveas.svg  settings-blue.svg  settings-grey.svg  settings.svg  shard-close.svg  shard-flush.svg  show.svg  spinner.svg  step-backward-green.svg  step-backward.svg  step-forward-green.svg  step-forward.svg  step.svg  stepping.svg  stop-red.svg  stop.svg  table-nested.svg  table.svg  tree-closed.svg  tree-leaf.svg  tree-open.svg  undo.svg  unlock-amber.svg  unlocked-green.svg  up.svg  upload.svg  user-disabled.svg  user.svg  users-disabled.svg  users.svg  volumes.svg   Pipeline element icons ElasticSearch.svg  StroomStatsStore.svg  analyticOutput.svg  apache_kafka-icon.svg  file.svg  files.svg  hadoop-elephant-logo.svg  id.svg  index.svg  json.svg  kafka.svg  recordCount.svg  recordOutput.svg  referenceData.svg  search.svg  solr.svg  split.svg  statistics.svg  stream.svg  stroomStats.svg  text.svg  xml.svg  xmlSearch.svg  xsd.svg  xslt.svg   Document type icons AnalyticOutputStore.svg  AnnotationsIndex.svg  Dashboard.svg  Dictionary.svg  ElasticIndex.svg  Feed.svg  Folder.svg  Index.svg  KafkaConfig.svg  Pipeline.svg  ReceiveDataRuleSet.svg  Script.svg  SelectAllOrNone.svg  SolrIndex.svg  StatisticStore.svg  StroomStatsStore.svg  System.svg  TextConverter.svg  Visualisation.svg  XMLSchema.svg  XSLT.svg  searchable.svg   ","categories":"","description":"A gallery of all the icons in use in stroom for reference.\n","excerpt":"A gallery of all the icons in use in stroom for reference.\n","ref":"/stroom-docs/hugo-docsy/docs/style-guide/icon-gallery/","tags":"","title":"Icon Gallery"},{"body":"","categories":"","description":"Example content from docsy-example\n","excerpt":"Example content from docsy-example\n","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/","tags":"","title":"Docsy Examples (DRAFT)"},{"body":" NOTE This document was written for stroom v4/5. It is not applicable for v6+.\n Stroom defaults to listening for HTTP on port 8080. It is recommended that Apache is used to listen on the standard HTTP port 80 and forward requests on via the Apache mod_jk module and the AJP protocol (on 8009). Apache can also perform HTTPS on port 443 and pass over requests to Tomcat using the same AJP protocol.\nIt is additionally recommended that Stroom Proxy is used to front data ingest and so Apache is configured to route traffic to http(s)://server/stroom/datafeed to Stroom Proxy and anything else to Stroom.\nPrerequisites  tomcat-connectors-1.2.31-src.tar.gz  Setup Apache  As root Patch mod_jk  cd ~/tmp tar -xvzf tomcat-connectors-1.2.31-src.tar.gz cd tomcat-connectors-1.2.31-src/native ./configure --with-apxs=/usr/sbin/apxs make sudo cp apache-2.0/mod_jk.so /etc/httpd/modules/ cd   Put the web server cert, private key, and CA cert into the web servers conf directory /etc/httpd/conf. E.g.  [user@node1 stroom-doc]$ ls -al /etc/httpd/conf .... -rw-r--r-- 1 root root 1729 Aug 27 2013 host.crt -rw-r--r-- 1 root root 1675 Aug 27 2013 host.key -rw-r--r-- 1 root root 1289 Aug 27 2013 CA.crt ....   Make changes to /etc/http/conf.d/ssl.conf as per below  JkMount /stroom* local JkMount /stroom/remoting/cluster* local  JkOptions +ForwardKeySize +ForwardURICompat +ForwardSSLCertChain -ForwardDirectories SSLCertificateFile /etc/httpd/conf/[YOUR SERVER].crt SSLCertificateKeyFile /etc/httpd/conf/[YOUR SERVER].key SSLCertificateChainFile /etc/httpd/conf/[YOUR CA].crt SSLCACertificateFile /etc/httpd/conf/[YOUR CA APPENDED LIST].crt SSLOptions +ExportCertData   Remove /etc/httpd/conf.d/nss.conf to avoid a 8443 port clash  rm /etc/httpd/conf.d/nss.conf   Create a /etc/httpd/conf.d/mod_jk.conf configuration  LoadModule jk_module modules/mod_jk.so JkWorkersFile conf/workers.properties JkLogFile logs/mod_jk.log JkLogLevel info JkLogStampFormat \"[%a %b %d %H:%M:%S %Y]\" JkOptions +ForwardKeySize +ForwardURICompat +ForwardSSLCertChain -ForwardDirectories JkRequestLogFormat \"%w %V %T\"  JkMount /stroom* local JkMount /stroom/remoting/cluster* local  JkShmFile logs/jk.shm \u003cLocation /jkstatus/\u003e JkMount status Order deny,allow Deny from all Allow from 127.0.0.1 \u003c/Location\u003e   Setup stroom-setup/cluster.txt, generate the workers file and copy into Apache. (as root and replace stroomuser with your processing user)  /home/stroomuser/stroom-setup/workers.properties.sh --cluster=/home/stroomuser/cluster.txt \u003e /etc/httpd/conf/workers.properties   Inspect /etc/httpd/conf/workers.properties to make sure it looks as you expect for your cluster  worker.list=loadbalancer,local,status worker.stroom_1.port=8009 worker.stroom_1.host=localhost worker.stroom_1.type=ajp13 worker.stroom_1.lbfactor=1 worker.stroom_1.max_packet_size=65536 .... .... worker.loadbalancer.type=lb worker.loadbalancer.balance_workers=stroom_1,stroom_2 worker.loadbalancer.sticky_session=1 worker.local.type=lb worker.local.balance_workers=stroom_1 worker.local.sticky_session=1 worker.status.type=status   Create a simple redirect page to the stroom web app for the root URL (e.g. DocumentRoot “/var/www/html”, index.html)  \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;meta http-equiv=\"Refresh\" content=\"0; URL=stroom\"\u0026gt;\u0026lt;/head\u0026gt;\u0026lt;/html\u0026gt;   Restart Apache and then test default http / https access.  sudo /etc/init.d/httpd restart  Advanced Forwarding Typically Stroom is setup so that traffic sent to /stroom* is routed to Stroom and /stroom/datafeed to Stroom Proxy. It is possible to setup an extra 1 level of datafeed routing so that based on the URL this traffic can be routed differently.\nFor example to route traffic directly to Stroom under the URL /stroom/datafeed/direct (avoiding any aggregation) the following mod_jk setting could be used.\nJkMount /stroom/datafeed/direct* loadbalancer  ","categories":"","description":"","excerpt":" NOTE This document was written for stroom v4/5. It is not applicable …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/setup/apache-forwarding/","tags":"","title":"Apache Forwarding"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/architecture/","tags":"","title":"architecture"},{"body":"Stroom’s documentation is built using GitBook (external link).\nPrerequisites NPM You need NPM to install the GitBook command line toolchain. To get NPM install node (external link).\nGitBook command line tools npm install -g gitbook-cli  Build the book GitBook uses plugins, e.g. anchorjs allows us to create links to headings within a file. You need to install these plugins first. The below commands should be run in the project root.\ngitbook install  You can build the documentation like this:\ngitbook build  Or you can run the GitBook server which will watch your files as you work and server them on localhost:4000.\ngitbook serve  Troubleshooting I get an error when trying to run gitbook serve If you see Errpr: watch /path/to/stroom ENOSPC then run the following: echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf \u0026\u0026 sudo sysctl -p\nLinks don’t work when I load _book/index.html It won’t, because CORS is not and cannot be enabled when viewing local files. You need to run gitbook serve or if you really don’t want to do that try cd _book \u0026\u0026 python -m SimpleHTTPServer.\n","categories":"","description":"","excerpt":"Stroom’s documentation is built using GitBook (external link). …","ref":"/stroom-docs/hugo-docsy/docs/dev-guide/documentation-dev/","tags":["TODO"],"title":"Building the documentation"},{"body":"Some examples of components in Stroom include\n stroom-activity - Component for recording a users actions against a current activity stroom-dictionary - Component for storing lists of words. stroom-statistics - Component for recording statistical data, e.g. amount of data received in X minutes.  In the project structure a component appears as a first level subdirectory of the root project folder. Components have further subdirectories (modules) that make up the various parts of the component, e.g.\n stroom - Root project  stroom-activity - The component  stroom-activity-api - API module for stroom-activity stroom-activity-impl - Implementation of the API and other module implementation code stroom-activity-impl-db - Database persistence implementation used by impl stroom-activity-impl-db-jooq - JOOQ generated classes used by stroom-activity-impl-db stroom-activity-mock - Mock implementation for the stroom-activity API      Dependencies between a modules components The diagram below shows the dependencies between the different modules that make up a component as well as the internal dependencies within the impl module. The actual implementations used at runtime are determined by Guice bindings in whichever Guice modules are loaded by the application. Tests can bind mock implementations of a components API just by using the Guice module within the mock module.\nDependencies between components Typically a component will need to call out to other components to apply security constraints and to log user activity. These typical relationships are shown in the diagram below.\nComponent API, e.g. modules ending in -api API layer All communication between components in stroom must be made via a component’s API. The API provides the minimum surface area for communication between components and decouples dependencies between components to just the API code. For component testing purposes mock implementations of these APIs can be used to limit testing to just a single component.\nComponent API and service implementation, e.g. modules ending in -impl Client interaction - REST services and GWT Action Handlers The uppermost layer of the server side code services requests from the client. The client may make restful calls as is the case for the new UI or will use Actions that are handles with ActionHandlers as is the case for the legacy GWT UI.\nSince this layer deals with all client interaction it should be responsible for creating audit logs for all user activity, e.g. accessing documents, searching etc. No audit logging should need to be performed at a lower level within the application as deeper levels have less knowledge of user intent since they may just be playing a part in the wider request.\nThe client interaction layer adds no logic and asks the underlying service layer to service the encapsulated request away from the REST endpoint wrapping code or GWT action handler code. This allows multiple types of endpoint to use the same underlying service layer. If a request requires the use of multiple services to form a response, this must be handled within the service layer by the primary service which will be responsible for any such orchestration.\nService layer The service layer applies permission constraints to any requests being made so that only calls from identified and permitted users are allowed to proceed. The service layer performs all orchestration and business logic, and is responsible for all mutations of objects that will be persisted by the underlying persistence layer such as stamping objects to be updated with the current user and update time.\nThe service layer provides implementations for any API that the component may have.\nThe service layer provides the DAO (Data Access Object) API for the persistence layer to implement but maintains no knowledge of underlying persistence implementation, e.g. database queries.\nPersistence implementation, e.g. modules ending in -impl-db Persistence layer - DAOs The persistence layer is an implementation of one or more DAOs specified in the service layer. The persistence layer provides no logic, it just stores and retrieves objects in a database or other persistence technology. If serialisation/de-serialisation is required in order to persist the object then that should also be performed by this layer so that no code above this layer has to care about this implementation detail.\nThe persistence layer does not apply security or permissions checking so should not need to reference the security API.\n","categories":"","description":"Stroom is broken down into separate components. Each component encapsulates a specific area of Stroom functionality and aids development by providing a single area of focus for new features and ensures separate components remain as loosely coupled as possible. Components can be tested in isolation and their interaction with other components easily understood by only allowing dependencies via minimal APIs.\n","excerpt":"Stroom is broken down into separate components. Each component …","ref":"/stroom-docs/hugo-docsy/docs/dev-guide/components/","tags":"","title":"Components"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/docker/","tags":"","title":"docker"},{"body":"In order that the java process communicates over https (for example Stroom Proxy forwarding onto Stroom) the JVM requires relevant keystore’s setting up.\nAs the processing user copy the following files to a directory stroom-jks in the processing user home directory :\n CA.crt - Certificate Authority SERVER.crt - Server certificate with client authentication attributes SERVER.key - Server private key  As the processing user perform the following:\n First turn your keys into der format:  cd ~/stroom-jks SERVER=\u003cSERVER crt/key PREFIX\u003e AUTHORITY=CA openssl x509 -in ${SERVER}.crt -inform PEM -out ${SERVER}.crt.der -outform DER openssl pkcs8 -topk8 -nocrypt -in ${SERVER}.key -inform PEM -out ${SERVER}.key.der -outform DER   Import Keys into the Key Stores:  Stroom_UTIL_JAR=`find ~/*app -name 'stroom-util*.jar' -print | head -1` java -cp ${Stroom_UTIL_JAR} stroom.util.cert.ImportKey keystore=${SERVER}.jks keypass=${SERVER} alias=${SERVER} keyfile=${SERVER}.key.der certfile=${SERVER}.crt.der keytool -import -noprompt -alias ${AUTHORITY} -file ${AUTHORITY}.crt -keystore ${AUTHORITY}.jks -storepass ${AUTHORITY}   Update Processing User Global Java Settings:  PWD=`pwd` echo \"export JAVA_OPTS=\\\"-Djavax.net.ssl.trustStore=${PWD}/${AUTHORITY}.jks -Djavax.net.ssl.trustStorePassword=${AUTHORITY} -Djavax.net.ssl.keyStore=${PWD}/${SERVER}.jks -Djavax.net.ssl.keyStorePassword=${SERVER}\\\"\" \u003e\u003e ~/env.sh  Any Stroom or Stroom Proxy instance will now additionally pickup the above JAVA_OPTS settings.\n","categories":"","description":"","excerpt":"In order that the java process communicates over https (for example …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/setup/java-key-store-setup/","tags":"","title":"Java Key Store Setup"},{"body":"Prerequisites  MySQL 5.5.y server installed (e.g. yum install mysql-server) Processing User Setup  A single MySQL database is required for each Stroom instance. You do not need to setup a MySQL instance per node in your cluster.\nCheck Database installed and running [root@stroomdb ~]# /sbin/chkconfig --list mysqld mysqld 0:off 1:off 2:on 3:on 4:on 5:on 6:off [root@stroomdb ~]# mysql --user=root -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. ... mysql\u003e quit  The following commands can be used to auto start mysql if required:\n[root@stroomdb ~]# /sbin/chkconfig –level 345 mysqld on [root@stroomdb ~]# /sbin/service httpd start  Overview MySQL configuration can be simple to complex depending on your requirements.\nFor a very simple configuration you simply need an out-of-the-box mysql install and create a database user account.\nThings get more complicated when considering:\n Security Master Slave Replication Tuning memory usage Running Stroom Stats in a different database to Stroom Performance Monitoring  Simple Install Ensure the database is running, create the database and access to it\n[stroomuser@host stroom-setup]$ mysql --user=root Welcome to the MySQL monitor. Commands end with ; or \\g. ... mysql\u003e create database stroom; Query OK, 1 row affected (0.02 sec) mysql\u003e grant all privileges on stroom.* to 'stroomuser'@'host' identified by 'password'; Query OK, 0 rows affected (0.00 sec) mysql\u003e create database stroom_stats; Query OK, 1 row affected (0.02 sec) mysql\u003e grant all privileges on stroom_stats.* to 'stroomuser'@'host' identified by 'password'; Query OK, 0 rows affected (0.00 sec) mysql\u003e flush privileges; Query OK, 0 rows affected (0.00 sec)  Advanced Security It is recommended to run /usr/bin/mysql_secure_installation to remove test database and accounts.\n./stroom-setup/mysql_grant.sh is a utility script that creates accounts for you to use within a cluster (or single node setup). Run to see the options:\n[stroomuser@host stroom-setup]$ ./mysql_grant.sh usage : --name=\u003cinstance name (defaults to my for /etc/my.cnf)\u003e --user=\u003cthe stroom user for the db\u003e --password=\u003cthe stroom password for the db\u003e --cluster=\u003cthe file with a line per node in the cluster\u003e --user=\u003cdb user\u003e Must be set  N.B. name is used when multiple mysql instances are setup (see below).\nYou need to create a file cluster.txt with a line for each member of your cluster (or single line in the case of a one node Stroom install). Then run the utility script to lock down the server access.\n[stroomuser@host ~]$ hostname \u003e\u003e cluster.txt [stroomuser@host ~]$ ./stroom-setup/mysql_grant.sh --name=mysql56_dev --user=stroomuser --password= --cluster=cluster.txt Enter root mysql password : -------------- flush privileges -------------- -------------- delete from mysql.user where user = 'stroomuser' -------------- ... ... ... -------------- flush privileges -------------- [stroomuser@host ~]$  Advanced Install The below example uses the utility scripts to create 3 custom mysql server instances on 2 servers:\n server1 - master stroom, server2 - slave stroom, stroom_stats  As root on server1:\nyum install \"mysql56-mysql-server\"  Create the master database:\n[root@node1 stroomuser]# ./stroom-setup/mysqld_instance.sh --name=mysqld56_stroom --port=3106 --server=mysqld56 --os=rhel6 --master not set ... assuming master database Wrote base files in tmp (You need to move them as root). cp /tmp/mysqld56_stroom /etc/init.d/mysqld56_stroom; cp /tmp/mysqld56_stroom.cnf /etc/mysqld56_stroom.cnf Run mysql client with mysql --defaults-file=/etc/mysqld56_stroom.cnf [root@node1 stroomuser]# cp /tmp/mysqld56_stroom /etc/init.d/mysqld56_stroom; cp /tmp/mysqld56_stroom.cnf /etc/mysqld56_stroom.cnf [root@node1 stroomuser]# /etc/init.d/mysqld56_stroom start Initializing MySQL database: Installing MySQL system tables... OK Filling help tables... ... ... Starting mysql56-mysqld: [ OK ]  Check Start up Settings Correct\n[root@node2 stroomuser]# chkconfig mysqld off [root@node2 stroomuser]# chkconfig mysql56-mysqld off [root@node1 stroomuser]# chkconfig --add mysqld56_stroom [root@node1 stroomuser]# chkconfig mysqld56_stroom on [root@node2 stroomuser]# chkconfig --list | grep mysql mysql56-mysqld 0:off 1:off 2:off 3:off 4:off 5:off 6:off mysqld 0:off 1:off 2:off 3:off 4:off 5:off 6:off mysqld56_stroom 0:off 1:off 2:on 3:on 4:on 5:on 6:off mysqld56_stats 0:off 1:off 2:on 3:on 4:on 5:on 6:off  Create a text file will all members of the cluster:\n[root@node1 stroomuser]# vi cluster.txt node1.my.org node2.my.org node3.my.org node4.my.org  Create the grants:\n[root@node1 stroomuser]# ./stroom-setup/mysql_grant.sh --name=mysqld56_stroom --user=stroomuser --password=password --cluster=cluster.txt  As root on server2:\n[root@node2 stroomuser]# yum install \"mysql56-mysql-server\" [root@node2 stroomuser]# ./stroom-setup/mysqld_instance.sh --name=mysqld56_stroom --port=3106 --server=mysqld56 --os=rhel6 --master=node1.my.org --user=stroomuser --password=password --master set ... assuming slave database Wrote base files in tmp (You need to move them as root). cp /tmp/mysqld56_stroom /etc/init.d/mysqld56_stroom; cp /tmp/mysqld56_stroom.cnf /etc/mysqld56_stroom.cnf Run mysql client with mysql --defaults-file=/etc/mysqld56_stroom.cnf [root@node2 stroomuser]# cp /tmp/mysqld56_stroom /etc/init.d/mysqld56_stroom; cp /tmp/mysqld56_stroom.cnf /etc/mysqld56_stroom.cnf [root@node1 stroomuser]# /etc/init.d/mysqld56_stroom start Initializing MySQL database: Installing MySQL system tables... OK Filling help tables... ... ... Starting mysql56-mysqld: [ OK ]  Check Start up Settings Correct\n[root@node2 stroomuser]# chkconfig mysqld off [root@node2 stroomuser]# chkconfig mysql56-mysqld off [root@node1 stroomuser]# chkconfig --add mysqld56_stroom [root@node1 stroomuser]# chkconfig mysqld56_stroom on [root@node2 stroomuser]# chkconfig --list | grep mysql mysql56-mysqld 0:off 1:off 2:off 3:off 4:off 5:off 6:off mysqld 0:off 1:off 2:off 3:off 4:off 5:off 6:off mysqld56_stroom 0:off 1:off 2:on 3:on 4:on 5:on 6:off  Create the grants:\n[root@node1 stroomuser]# ./stroom-setup/mysql_grant.sh --name=mysqld56_stroom --user=stroomuser --password=password --cluster=cluster.txt  Make the slave database start to follow:\n[root@node2 stroomuser]# cat /etc/mysqld56_stroom.cnf | grep \"change master\" # change master to MASTER_HOST='node1.my.org', MASTER_PORT=3106, MASTER_USER='stroomuser', MASTER_PASSWORD='password'; [root@node2 stroomuser]# mysql --defaults-file=/etc/mysqld56_stroom.cnf mysql\u003e change master to MASTER_HOST='node1.my.org', MASTER_PORT=3106, MASTER_USER='stroomuser', MASTER_PASSWORD='password'; mysql\u003e start slave;  As processing user on server1:\n[stroomuser@node1 ~]$ mysql --defaults-file=/etc/mysqld56_stroom.cnf --user=stroomuser --password=password mysql\u003e create database stroom; Query OK, 1 row affected (0.00 sec) mysql\u003e use stroom; Database changed mysql\u003e create table test (a int); Query OK, 0 rows affected (0.05 sec)  As processing user on server2 check server replicating OK:\n[stroomuser@node2 ~]$ mysql --defaults-file=/etc/mysqld56_stroom.cnf --user=stroomuser --password=password mysql\u003e show create table test; +-------+----------------------------------------------------------------------------------------+ | Table | Create Table | +-------+----------------------------------------------------------------------------------------+ | test | CREATE TABLE `test` (`a` int(11) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=latin1 | +-------+----------------------------------------------------------------------------------------+ 1 row in set (0.00 sec)  As root on server2:\n[root@node2 stroomuser]# /home/stroomuser/stroom-setup/mysqld_instance.sh --name=mysqld56_stats --port=3206 --server=mysqld56 --os=rhel6 --user=statsuser --password=password [root@node2 stroomuser]# cp /tmp/mysqld56_stats /etc/init.d/mysqld56_stats; cp /tmp/mysqld56_stats.cnf /etc/mysqld56_stats.cnf [root@node2 stroomuser]# /etc/init.d/mysqld56_stats start [root@node2 stroomuser]# chkconfig mysqld56_stats on  Create the grants:\n[root@node2 stroomuser]# ./stroom-setup/mysql_grant.sh --name=mysqld56_stats --database=stats --user=stroomstats --password=password --cluster=cluster.txt  As processing user create the database:\n[stroomuser@node2 ~]$ mysql --defaults-file=/etc/mysqld56_stats.cnf --user=stroomstats --password=password Welcome to the MySQL monitor. Commands end with ; or \\g. .... mysql\u003e create database stats; Query OK, 1 row affected (0.00 sec)  ","categories":"","description":"","excerpt":"Prerequisites  MySQL 5.5.y server installed (e.g. yum install …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/setup/mysql-server-setup/","tags":"","title":"MySQL Setup"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/nginx/","tags":"","title":"nginx"},{"body":" Version Information: Created with Stroom v7.0\nLast Updated: 2021-06-07\nSee Also: Nginx documentation (external link).\n Without Docker The standard way of deploying Nginx with stroom running without docker involves running Nginx as part of the services stack. See below for details of how to configure it. If you want to deploy Nginx without docker then you can but that is outside the scope of the this documentation.\nAs part of a docker stack Nginx is included in all the stroom docker stacks. Nginx is configured using multiple configuration files to aid clarity and allow reuse of sections of configuration. The main file for configuring Nginx is nginx.conf.template and this makes use of other files via include statements.\nThe purpose of the various files is as follows:\n nginx.conf.template - Top level configuration file that orchestrate the other files. logging.conf.template - Configures the logging output, its content and format. server.conf.template - Configures things like SSL settings, timeouts, ports, buffering, etc. Upstream configuration  upstreams.stroom.ui.conf.template - Defines the upstream host(s) for stroom node(s) that are dedicated to serving the user interface. upstreams.stroom.processing.conf.template - Defines the upstream host(s) for stroom node(s) that are dedicated to stream processing and direct data receipt. upstreams.proxy.conf.template - Defines the upstream host(s) for local stroom-proxy node(s).   Location configuration  locations_defaults.conf.template - Defines some default directives (e.g. headers) for configuring stroom paths. proxy_location_defaults.conf.template - Defines some default directives (e.g. headers) for configuring stroom-proxy paths. locations.proxy.conf.template - Defines the various paths (e.g/ /datafeed) that will be reverse proxied to stroom-proxy hosts. locations.stroom.conf.template - Defines the various paths (e.g/ /datafeeddirect) that will be reverse proxied to stroom hosts.    Templating The nginx container has been configured to support using environment variables passed into it to set values in the Nginx configuration files. It should be noted that recent versions of Nginx have templating support built in. The templating mechanism used in stroom’s Nginx container was set up before this existed but achieves the same result.\nAll non-default configuration files for Nginx should be placed in volumes/nginx/conf/ and named with the suffix .template (even if no templating is needed). When the container starts any variables in these templates will be substituted and the resulting files will be copied into /etc/nginx. The result of the template substitution is logged to help with debugging.\nThe files can contain templating of the form:\nssl_certificate /stroom-nginx/certs/\u003c\u003c\u003cNGINX_SSL_CERTIFICATE\u003e\u003e\u003e;  In this example \u003c\u003c\u003cNGINX_SSL_CERTIFICATE\u003e\u003e\u003e will be replaced with the value of environment variable NGINX_SSL_CERTIFICATE when the container starts.\nUpstreams When configuring a multi node cluster you will need to configure the upstream hosts. Nginx acts as a reverse proxy for the applications behind it so the lists of hosts for each application need to be configured.\nFor example if you have a 10 node cluster and 2 of those nodes are dedicated for user interface use then the configuration would look like:\nupstreams.stroom.ui.conf.template\nserver node1.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e server node2.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e  upstreams.stroom.processing.conf.template\nserver node3.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e server node4.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e server node5.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e server node6.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e server node7.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e server node8.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e server node9.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e server node10.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e  upstreams.proxy.conf.template\nserver node3.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e server node4.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e server node5.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e server node6.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e server node7.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e server node8.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e server node9.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e server node10.stroomhosts:\u003c\u003c\u003cSTROOM_PORT\u003e\u003e\u003e  In the above example the port is set using templating as it is the same for all nodes. Nodes 1 and 2 will receive all UI and REST API traffic. Nodes 8-10 will serve all datafeed(direct) requests.\nCertificates The stack comes with a default server certificate/key and CA certificate for demo/test purposes. The files are located in volumes/nginx/certs/. For a production deployment these will need to be changed, see Certificates\nLog rotation The Nginx container makes use of logrotate to rotate Nginx’s log files after a period of time so that rotated logs can be sent to stroom. Logrotate is configured using the file volumes/stroom-log-sender/logrotate.conf.template. This file is templated in the same way as the Nginx configuration files, see above. The number of rotated files that should be kept before deleting them can be controlled using the line.\nrotate 100  This should be set in conjunction with the frequency that logrotate is called, which is controlled by volumes/stroom-log-sender/crontab.txt. This crontab file drives the lograte process and by default is set to run every minute.\n","categories":"","description":"Nginx is the standard web server used by stroom. Its primary role is SSL termination and reverse proxying for stroom and stroom-proxy that sit behind it. It can also load balance incoming requests and ensure traffic from the same source is always route to the same upstream instance. Other web servers can be used if required but their installation/configuration is out of the scope of this documentation.\n","excerpt":"Nginx is the standard web server used by stroom. Its primary role is …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/configuration/configuring-nginx/","tags":["nginx"],"title":"Nginx Configuration"},{"body":"Processing User Setup Stroom / Stroom Proxy should be run under a processing user (we assume stroomuser below).\n Setup this user  /usr/sbin/adduser --system stroomuser    You may want to allow normal accounts to sudo to this account for maintenance (visudo)\n  Create a service script to start/stop on server startup (as root).\n  vi /etc/init.d/stroomuser #!/bin/bash # # stroomuser This shell script takes care of starting and stopping # the stroomuser subsystem (tomcat6, etc) # # chkconfig: - 86 14 # description: stroomuser is the stroomuser sub system Stroom_USER=stroomuser case $1 in start) /bin/su ${Stroom_USER} /home/${Stroom_USER}/stroom-deploy/start.sh ;; stop) /bin/su ${Stroom_USER} /home/${Stroom_USER}/stroom-deploy/stop.sh ;; restart) /bin/su ${Stroom_USER} /home/${Stroom_USER}/stroom-deploy/stop.sh /bin/su ${Stroom_USER} /home/${Stroom_USER}/stroom-deploy/start.sh ;; esac exit 0   Initialise Script  /bin/chmod +x /etc/init.d/stroomuser /sbin/chkconfig --level 345 stroomuser on  Install Java 8 yum install java-1.8.0-openjdk.x86_64 yum install java-1.8.0-openjdk-devel.x86_64  Setup Deployment Scripts  As the processing user unpack the stroom-deploy-X-Y-Z-bin.zip generic deployment scripts in the processing users home directory.  unzip stroom-deploy-5.0.beta1-bin.zip   Setup env.sh to include JAVA_HOME to point to the installed directory of the JDK (this will be platform specific). vi ~/env.sh  # User specific aliases and functions export JAVA_HOME=/usr/lib/jvm/java-1.8.0 export PATH=${JAVA_HOME}/bin:${PATH}   Setup users profile to include the same. vi ~/.bashrc  # User specific aliases and functions . ~/env.sh   Check that java is installed OK  [stroomuser@node1 ~]$ . .bashrc [stroomuser@node1 ~]$ which java /usr/lib/jvm/java-1.8.0/bin/java [stroomuser@node1 ~]$ which javac /usr/lib/jvm/java-1.8.0/bin/javac [stroomuser@node1 ~]$ java -version openjdk version \"1.8.0_65\" OpenJDK Runtime Environment (build 1.8.0_65-b17) OpenJDK 64-Bit Server VM (build 25.65-b01, mixed mode)   Setup auto deployment crontab script as below (crontab -e)  [stroomuser@node1 ~]$ crontab -l # Deploy Script 0,5,10,15,20,25,30,35,40,45,50,55 * * * * /home/stroomuser/stroom-deploy/deploy.sh \u003e\u003e /home/stroomuser/stroom-deploy.log 59 0 * * * rm -f /home/stroomuser/stroom-deploy.log # Clean system 0 0 * * * /home/stroomuser/stroom-deploy/clean.sh \u003e /dev/null  ","categories":"","description":"","excerpt":"Processing User Setup Stroom / Stroom Proxy should be run under a …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/setup/processing-user-setup/","tags":"","title":"Processing Users"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/proxy/","tags":"","title":"proxy"},{"body":"Performing a named release This is a general guide for performing a release of a named version of one of the stroom family of products.\n  Ensure the CHANGELOG has been updated for the new x.y.z version, i.e. ensure there is a section for vx.y.z and the links to git diffs at the bottom are up to date. e.g.\n## [v3.1.1_schema-v3.1.2] - 2017-11-14 ### Added * Add sources and javadoc jars ### Changed * Uplift schema to v3.1.2 * Change build to Gradle  and\n[v3.1.2_schema-v3.1.2]: https://github.com/gchq/event-logging/compare/v3.1.1_schema-v3.1.2...v3.1.2_schema-v3.1.2    If the changes include merges in from other branches, e.g. merging a big fix from v1.0 to v2.0 then add a Merged section to make it clear that the changes in the merge have been applied, e.g.\n### Merged * Merged in [v1.0.1]    Test the named release build locally by adding the argument -Pversion=vx.y.z to the gradle build command.\n  Ensure all code is committed to the master branch, or a release branch, such as v5.0.\n  To perform the release create an annotated git tag as follows:\ngit tag -a vx.y.z\n  Complete the tag’s commit message using the following format\nstroom-something-vx.y.z @@@  Where @@@ is the content of the x.y.z section from the CHANGELOG. GitHub takes the top line as the title for the release. Git will ignore any lines starting with a # so remove them from the markdown headings. Keep the * bullets. E.g.\nevent-logging-v3.1.1_schema-v3.1.2 Added * Add sources and javadoc jars Changed * Uplift schema to v3.1.2 * Change build to Gradle    Push the tag. Travis will run a build and on detecting the tag, will release any build artefacts to GitHUb Releases. If the project has any published Maven artefacts these will be releases to Bintray.\n  SNAPSHOT releases SNAPSHOT releases should not and cannot be released to Bintray. If a development version of a library needs to be shared between projects then you can either use the Gradle task publishToMavenLocal to publish a SNAPSHOT version to your local Maven repository and change your dependency version to SNAPSHOT, or perform a named release along the lines of vx.y.z-alpha.n.\nRelease Versioning conventions Semantic versioning is used, and this should be adhered to, see SemVer (external link). The following are examples of valid version names\n  SNAPSHOT - Used only for local development, never to be published publicly.\n  v3.3.0 - Initial release of v3.3, with an associated v3.3 branch.\n  v3.3.1 - A patch release to v3.3 on the v3.3 branch.\n  v3.4.0-alpha.1 - An alpha release of v3.4, either on master or a v3.4 branch\n  v3.4.0-beta.1 - An beta release of v3.4, either on master or a v3.4 branch\n  To Perform a Local Build Full build: ./gradlew clean build\nBuild without unit tests ./gradlew clean build -x test\nBuild without integration tests ./gradlew clean build -x integrationTest\nBuild without any tests or GWT compilation (GWT compilation applies to stroom only) ./gradlew clean build -x test -x integrationTest -x gwtCompile\n","categories":"","description":"","excerpt":"Performing a named release This is a general guide for performing a …","ref":"/stroom-docs/hugo-docsy/docs/dev-guide/releasing/","tags":"","title":"Release Process"},{"body":" NOTE: The published docker images are intended for small scale testing or evaluation purposes and are currently un-tested in a production environment.\n Stroom v6.x release Prerequisites In order to run Stroom v6.x using Docker you will need the following installed on the machine you intend to run Stroom on:\n A Linux-like shell environment. docker CE (v17.12.0+) - e.g https://docs.docker.com/install/linux/docker-ce/centos/ for Centos docker-compose (v1.21.0+) - https://docs.docker.com/compose/install/ bash (v4+) jq - https://stedolan.github.io/jq/, e.g. sudo yum install jq curl  Install steps This will install the core stack (Stroom and the peripheral services required to run Stroom).\nVisit stroom-resources/releases (external link) and find the latest stroom_core release and copy the link to the associated stroom_core*.tar.gz archive file.\nUsing stroom_core-v6.0.19 as an example:\n# Set the release version to download export STROOM_STACK_VER=\"stroom_core-v6.0.19\" # Make the stack directory mkdir ${STROOM_STACK_VER} # Download and extract the Stroom stack into the directory stroom_core-vX.Y.Z curl -sL https://github.com/gchq/stroom-resources/releases/download/${STROOM_STACK_VER}/${STROOM_STACK_VER}.tar.gz | tar xz -C ${STROOM_STACK_VER} # Navigate into the new stack directory, where xxxx is the directory that has just been created cd ${STROOM_STACK_VER} # Start the stack ./start.sh  Alternatively if you understand the risks of redirecting web sourced content direct to bash, you can get the latest release using:\n# Download and extract the Stroom stack bash \u003c(curl -s https://gchq.github.io/stroom-resources/get_stroom.sh) # Navigate into the new stack directory cd stroom_core_test/stroom_core_test* # Start the stack ./start.sh  On first run stroom will build the database schemas so this can take a minute or two. The start.sh script will provide details of the various URLs that are available.\nOpen a browser (preferably Chrome) at https://localhost/stroom and login with:\n username: admin (NOT an email address) password: admin  The stroom stack comes supplied with self-signed certificates so you may need to accept a prompt warning you about visiting an untrusted site.\nStroom v5.x release Prerequisites In order to run Stroom v5.x using Docker you will need the following installed on the machine you intend to run Stroom on:\n A Linux-like shell environment with bash and GNU sed/grep docker CE (v17.12.0+) docker-compose (v1.21.0+) git  Install steps # Clone the stroom-resources git repository git clone https://github.com/gchq/stroom-resources.git # Navigate to the bin directory in the repository cd stroom-resources/bin # Start the stroom v5.x stack using docker/docker-compose ./bounceIt.sh -f env/stroom5.env  Open a browser (preferably Chrome) at http://localhost:8080/stroom and login with:\n username: admin password: admin  Docker Hub links (external) The Stroom image\nThe Stroom Authentication image\nThe GCHQ organisation\n","categories":"","description":"This is how to run Stroom and its peripheral services using _Docker_.  Running Stroom in _Docker_ is the quickest and easiest way to get Stroom up and running. \n","excerpt":"This is how to run Stroom and its peripheral services using _Docker_. …","ref":"/stroom-docs/hugo-docsy/docs/dev-guide/docker-running/","tags":["docker"],"title":"Running in Docker"},{"body":"We tend to use IntelliJ as our Java IDE of choice. This is a guide for running Stroom in IntelliJ for the purposes of developing/debugging Stroom.\nPrerequisites In order to build/run/debug Stroom you will need the following:\n Java 8 JDK Git Gradle IntelliJ Docker CE Docker Compose  These instructions assume that all servcies will either run in the IDE or in Docker containers.\nEnvironment variables Stroom git repositories To develop stroom you will need to clone/fork multiple git repositories. To quickly clone all of the Stroom repositories you can use the helper script described in stroom-resource (external link).\nDatabase setup Stroom requires a MySQL database to run. You can either point stroom at a MySQL server or preferably at a MySQL Docker containers.\nMySQL in a Docker container See the section below on stroom-resources.\nHost based MySQL server With an instance of MySQL server 5.5 running on your local machine do the following to create the stroom database:\n# log into your MySQL server using your root credentials mysql --user=root --password=myrootpassword  Then run the following commands in the MySQL shell:\ndrop database stroom; create database stroom; grant all privileges on stroom.* to stroomuser@localhost identified by 'stroompassword1'; quit;  Local configuration file When running stroom in an IDE it is advisable to have a local configuration file to allow you to change settings locally without affecting the repository. The local configuration file is expected to live at ~/.stroom/stroom.conf. To create a default version of this file run the script stroom.conf.sh from within the root of the stroom git repository.\nAdd any properties from stroom.properties that you want different values for, e.g.\nstroom-resources As a minimum to develop stroom you will need clones of the stroom and stroom-resources git repositories. stroom-resources provides the docker-compose configuration for running the many docker containers needed.\nHaving cloned stroom-resources navigate to the directory stroom-resources/bin and run the script\n./bounceIt.sh  On first run this will create a default version of the git-ignored file stroom-resources/bin/local.env which is intended for use by developers to configure the docker stacks to run.\nThis file is used to set a number of environment variables that docker compose will use to configure the various containers. The key environment variable in there is SERVICE_LIST. This is a space delimited list of the services for docker-compose to run. The services are all defined in stroom-resources/bin/compose/everything.yml and its dependencies. By default SERVICE_LIST runs a core stroom stack entirely in docker.\nTo run stroom in an IDE stroom needs to be removed from SERVICE_LIST, i.e. by commenting out the line SERVICE_LIST=\"${SERVICE_LIST} stroom\" in local.env. Having done this run stroom’s core dependencies as follows:\n./bounceIt.sh  Verify the Gradle build Before trying to run Stroom in an IDE it is worth performing a Gradle build without the integration tests (as these take ~20mins to run) to verify the code compiles and all dependencies are present.\n./gradlew clean build -x integrationTest  Sample Data Some of the tests are dependant on some sample data and content being present in the database. This sample data/content can also be useful for manually testing the application in development. The sample data/content is generated by a class called SetupSampleData.java. This class assumes that the database being used for Stroom is completely empty.\nFirst you need to create a run configuration for SetupSampleData.java\n Click Run -\u003e Edit Configurations… Click the green + icon to add a new configuration Select Application as the configuration type In the Main Class field enter SetupSampleData In the Use classpath of module field select stroom-integrationtest If you have set the STROOM_TMP environment variable in your .bashrc / .zshrc then ignore this step. In the Environment variables field click the … icon and add STROOM_TMP = ~/tmp/stroom/ (or whatever directory you choose) Click the OK button  Now run SetupSampleData\n Click Run -\u003e Run… Select SetupSampleData  You should now have a database populated with tables and data, providing you with some predefined feeds, data, translations, pipelines, dashboards, etc.\nRunning Stroom from the IDE The user interface for Stroom is built using GWT (see GWT Project for more information or GWT specific documentation). As a result Stroom needs to be started up with GWT Dev Mode. Dev Mode handles the compilation of the Java user interface source into JavaScript and the source map that links client JavaScript back to Java source for client side debugging.\nStroom is run from the main method in Startup.java. Before running this you need to setup the run configuration:\n Click Run -\u003e Edit Configurations… Click the green + icon to add a new configuration Select Application as the configuration type In the Main class field enter Startup In the Programme arguments field enter  -startupUrl stroom.jsp -logLevel INFO -war . -logdir . -gen . -extra . -workDir . stroom.app.AppSuperDevMode\n In the Use classpath of module field select stroom-startup If you have set the STROOM_TMP environment variable in your .bashrc / .zshrc then ignore this step. In the Environment variables field click the … icon and add STROOM_TMP = ~/tmp/stroom/ (or whatever directory you choose) Click the OK button  Now run Startup\n Click Run -\u003e Run… Select Startup  You should eventually see the GWT Dev Mode window appear.\nInitially, some of the buttons shown above will not be visible as it is in the process of starting up. As soon as the Launch Default Browser button appears you are ready to open Stroom in a browser. You have two options:\n Click the Launch Default Browser button Open your preferred browser and enter the URL http://127.0.0.1:8888   NOTE: Stroom has been written with Google’s Chrome browser in mind so has only been tested on Chrome. Behaviour in other browsers may vary. We would like to improve cross-browser support so please let us know about any browser incompatibilities that you find.\n In the browser you will initially see the following:\n Starting Stroom\n  Initialising context…\n Once the context has been initialised you will see the Stroom blue background and a spinner while GWT compiles the front-end code. Once the code has been compiled you will be presented with the Stroom login page. Stroom in development mode uses simple username/password authentication. Enter the following credentials:\n Username: admin Password: admin  Right click behaviour Stroom overrides the defualt right click behaviour in the browser with its own context menu. For UI development it is often required to have access to the browser’s context menu for example to inspect elements. To enable the browser’s context menu you need to uncomment the following property in your stroom.conf file, e.g.\n#Uncomment this to enable browser's right click menu for development stroom.ui.oncontextmenu=  ","categories":"","description":"","excerpt":"We tend to use IntelliJ as our Java IDE of choice. This is a guide for …","ref":"/stroom-docs/hugo-docsy/docs/dev-guide/stroom-in-an-ide/","tags":"","title":"Running Stroom in an IDE"},{"body":" NOTE This document was written for stroom v4/5. Some parts may not be applicable for v6+.\n Securing Stroom Firewall The following firewall configuration is recommended:\n Outside cluster drop all access except ports HTTP 80, HTTPS 443, and any other system ports your require SSH, etc Within cluster allow all access  This will enable nodes within the cluster to communicate on:\n Native tomcat HTTP 8080, 9080 Tomcat AJP 8009, 9009 MySQL 3006  MySQL  It is recommended that you run mysql_secure_installation to set a root password and remove test database:  mysql_secure_installation (provide a root password) - Set root password? [Y/n] Y - Remove anonymous users? [Y/n] Y - Disallow root login remotely? [Y/n] Y - Remove test database and access to it? [Y/n] Y - Reload privilege tables now? [Y/n] Y   stroom-setup includes a version of this script designed to be run on instances create using mysqld_instance.sh (i.e. non standard or multiple instances of mysql)  [stroomuser@stroom_1 stroom-setup]$ ./mysql_secure_installation.sh --name=mysqld_ref1m  ","categories":"","description":"","excerpt":" NOTE This document was written for stroom v4/5. Some parts may not be …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/setup/securing-stroom/","tags":"","title":"Securing Stroom"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/docs/install-guide/setup/","tags":"","title":"Setup"},{"body":" Warning This document was written for stroom v4/5. It is not applicable for v6+.\n Prerequisites  Install file ‘stroom-app-distribution-X-Y-Z-bin.zip’. All the pre-built binaries are available on GitHub (external link) MySQL Server 5.5 JDK8 Temporarily allow port 8080, if not relying on Apache Forwarding.  Installing Stroom Unpack the distribution stroom-app-distribution-X-Y-Z-bin.zip:\nunzip stroom-app-distribution-X-Y-Z-bin.zip  In bin are scripts for configuring and starting and stopping Stroom.\nConfiguring The setup.sh script will ask a series of questions to help you configure Stroom.\n./bin/setup.sh  This script asks a series of questions about configuration parameters. These parameters are:\n TEMP_DIR - This is where Stroom will write some temporary files, e.g. imports/exports. Only change this if you do not want to use ‘/tmp’. NODE - Each Stroom instance in the cluster needs a unique name, if this is a reinstall ensure you use the previous deployment. This name needs match the name used in your worker.properties (e.g. ‘node1’ in the case ‘node1.my.org’) RACK - Used to group nodes together (so for example nodes near each other process near data) PORT_PREFIX - By default Stroom will run on port 8080 JDBC_CLASSNAME, JDBC URL, DB USERNAME, DB PASSWORD - MySQL connection details for the stroom database JPA DIALECT - Leave blank to use MySQL JAVA OPTS - By default this is ‘-Xms1g -Xmx8g’. Stroom performs better if you use most of the servers memory so change the maximum memory setting (Xmx) accordingly, e.g. -Xmx40g will use 40 GB. STROOM_STATISTICS_SQL_JDBC_CLASSNAME, STROOM_STATISTICS_SQL_JDBC URL, STROOM_STATISTICS_SQL_DB USERNAME, STROOM_STATISTICS_SQL_DB PASSWORD - MySQL connection details for the statistics database  Running Start the configured instance:\n./bin/start.sh  Inspect the logs:\ntail -f instance/logs/stroom.log  Other things to configure: You might want to configure some of the following:\n Processing User Setup MySQL Server Setup Java Key Store Setup Apache Forwarding Securing Stroom  ","categories":"","description":"","excerpt":" Warning This document was written for stroom v4/5. It is not …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/stroom-app-install/","tags":"","title":"Stroom 5 Installation"},{"body":"The diagram below shows the logical architecture of Stroom v6. It is not concerned with how/where the various services are deployed. This page describes represents a reference architecture and deployment for stroom but it is possible to deploy the various services in many different ways, e.g. using a different web server to Nginx or introducing load balancers.\nNginx In stroom v6, a central Nginx is key to the whole architecture. It acts in the following capacities:\n A reverse proxy to abstract clients from the multiple service instances. An API gateway for all service traffic. The termination point for client SSL traffic.  Reverse Proxy Nginx is used to reverse proxy all client connections (even those from within the estate) to the various services that sit behind it. For example, a client request to https://nginx-host/stroom will be reverse proxied to http://a-stroom-host:8080/stroom. Nginx is responsible for selecting the upstream server to reverse proxy to. It is possible to use multiple instances of Nginx for redundancy or improved performance, however care needs to be taken to ensure all requests for a session go to the same Nginx instance, i.e. sticky sessions. Some requests are stateful and some are stateless but the Nginx config will reverse proxy them accordingly.\nAPI Gateway Nginx is also used as an API gateway. This means all inter-service calls go via the Nginx gateway so each service only needs to know the location of the Nginx gateway. Nginx will then reverse proxy all requests to the appropriate instance of an upstream service.\nThe grey dashed lines on the diagram attempt to show the effective inter-service connections that are being made if you ignore the Nginx reverse proxying.\nSSL Termination All SSL termination is handled by Nginx. Nginx holds the server and certificate authority certificate and will authenticate the client requests if the client has a certificate. Any client certificate details will be passed on to the service that is being reverse proxied.\nPhysical Deployment Single Node Docker Deployment The simplest deployment of stroom is where all services are on a single host and each service runs in its own docker container. Such a deployment can be achieved by following these instructions.\nThe following diagram shows how a single node deployment would look.\nMulti Node Mixed Deployment The typical deployment for a large scale stroom is where stroom is run on multiple hosts to scale out the processing. In this deployment stroom and MySQL are run directly on the host OS, i.e. without docker. This approach was taken to gradually introduce docker into the stroom deployment strategies.\nThe following diagram shows how a multi node deployment would look.\nMulti Node All docker Deployment The aim in future is to run all services in docker in a multi node deployment. Such a deployment is still under development and will likely involve kubernetes for container orchestration.\n","categories":"","description":"","excerpt":"The diagram below shows the logical architecture of Stroom v6. It is …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/stroom-6-architecture/","tags":["architecture"],"title":"Stroom 6 Architecture \u0026 Depployment"},{"body":"We would welcome feedback on this documentation.\nRunning on a single box Running a release Download a release (external link), for example Stroom Core v6.0 Beta 3 (external link), unpack it, and run the start.sh script. When you’ve given it some time to start up go to http://localhost/stroom. There’s a README.md file inside the tar.gz with more information.\nPost-install hardening Before first run Change database passwords If you don’t do this before the first run of Stroom then the passwords will already be set and you’ll have to change them on the database manually, and then change the .env.\nThis change should be made in the .env configuration file. If the values are not there then this service is not included in your Stroom stack and there is nothing to change.\n  STROOM_DB_PASSWORD\n  STROOM_DB_ROOT_PASSWORD\n  STROOM_STATS_DB_ROOT_PASSWORD\n  STROOM_STATS_DB_PASSWORD\n  STROOM_AUTH_DB_PASSWORD\n  STROOM_AUTH_DB_ROOT_PASSWORD\n  STROOM_ANNOTATIONS_DB_PASSWORD\n  STROOM_ANNOTATIONS_DB_ROOT_PASSWORD\n  On first run Create yourself an account After first logging in as admin you should create yourself a normal account (using your email address) and add yourself to the Administrators group. You should then log out of admin, log in with your new administrator account and then disable the admin account.\nIf you decide to use the admin account as your normal account you might find yourself locked out. The admin account has no associated email address, so the Reset Password feature will not work if your account is locked. It might become locked if you enter your password incorrectly too many times.\nDelete un-used users and API keys  If you’re not using stats you can delete or disable the following:  the user statsServiceUser the API key for statsServiceUser    Change the API keys First generate new API keys. You can generate a new API key using Stroom, under Tools -\u003e API Keys. The following need to be changed:\n  STROOM_SECURITY_API_TOKEN\n This is the API token for user stroomServiceUser.    Then stop Stroom and update the API key in the .env configuration file with the new value.\nTroubleshooting I’m trying to use certificate logins (PKI) but I keep being prompted for the username and password! You need to be sure of several things:\n When a user arrives at Stroom the first thing Stroom does is redirect the user to the authentication service. This is when the certificate is checked. If this redirect doesn’t use HTTPS then nginx will not get the cert and will not send it onwards to the authentication service. Remember that all of this stuff, apart from back-channel/service-to-service chatter, goes through nginx. The env var that needs to use HTTPS is STROOM_AUTHENTICATION_SERVICE_URL. Note that this is the var Stroom looks for, not the var as set in the stack, so you’ll find it in the stack YAML. Are your certs configured properly? If nginx isn’t able to decode the incoming cert for some reason then it won’t pass anything on to the service. Is your browser sending certificates?  ","categories":"","description":"","excerpt":"We would welcome feedback on this documentation.\nRunning on a single …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/stroom-6-installation/","tags":"","title":"Stroom 6 Installation"},{"body":"There are 2 versions of the stroom software availble for building a proxy server.\nThere is an ‘app’ version that runs stroom as a Java ARchive (jar) file locally on the server and has settings contained in a configuration file that controls access to the stroom server and database.\nThe other version runs stroom proxy within docker containers and also has a settings configuration file that controls access to the stroom server and database.\nThe document will cover the installation and configuration of the stroom proxy software for both the docker and ‘app’ versions.\nAssumptions The following assumptions are used in this document.\n the user has reasonable RHEL/CentOS System administration skills. installation is on a fully patched minimal CentOS 7 instance. the Stroom database has been created and resides on the host stroomdb0.strmdev00.org listening on port 3307. the Stroom database user is stroomuser with a password of Stroompassword1@. the application user stroomuser has been created. the user is or has deployed the two node Stroom cluster described here. the user has set up the Stroom processing user as described here. the prerequisite software has been installed. when a screen capture is documented, data entry is identified by the data surrounded by ‘\u003c’ ‘\u003e’ . This excludes enter/return presses.    Stroom Remote Proxy (docker version) The build of a stroom proxy where the stroom applications are running in docker containers.\nThe operating system (OS) build for a ‘dockerised’ stroom proxy is minimal RHEL/CentOS 7 plus the docker-ce \u0026 docker-compose packages.\nNeither of the pre-requisites are available from the CentOS ditribution.\nIt will also be necessary to open additional ports on the system firewall (where appropriate).\nDownload and install docker To download and install - docker-ce - from the internet, a new ‘repo’ file is downloaded first, that provides access to the docker.com repository. e.g. as root user:  wget https://download.docker.com/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo yum install docker-ce.x86_64  The packages - docker-ce docker-ce-cli \u0026 containerd.io - will be installed\n The docker-compose software can de downloaded from github e.g. as root user to download docker-compose version 1.25.4 and save it to - /usr/local/bin/docker-compose\n curl -L https://github.com/docker/compose/releases/download/1.25.4/docker-compose-Linux-x86_64 -o /usr/local/bin/docker-compose chmod 755 /usr/local/bin/docker-compose   Firewall Configuration If you have a firewall running additional ports will need to be opened, to allow the Docker containers to talk to each other.\nCurrently these ports are:\n 3307 8080 8081 8090 8091 8543 5000 2888 443 80  For example on a RHEL/CentOS server using firewalld the commands would be as root user:\nfirewall-cmd --zone=public --permanent --add-port=3307/tcp firewall-cmd --zone=public --permanent --add-port=8080/tcp firewall-cmd --zone=public --permanent --add-port=8081/tcp firewall-cmd --zone=public --permanent --add-port=8090/tcp firewall-cmd --zone=public --permanent --add-port=8091/tcp firewall-cmd --zone=public --permanent --add-port=8099/tcp firewall-cmd --zone=public --permanent --add-port=5000/tcp firewall-cmd --zone=public --permanent --add-port=2888/tcp firewall-cmd --zone=public --permanent --add-port=443/tcp firewall-cmd --zone=public --permanent --add-port=80/tcp firewall-cmd --reload   Download and install Stroom v7 (docker version) The installation example below is for stroom version 7.0.beta.45 - but is applicable to other stroom v7 versions.\nAs a suitable stroom user e.g. stroomuser - download and unpack the stroom software.\n wget https://github.com/gchq/stroom-resources/releases/download/stroom-stacks-v7.0-beta.41/stroom_proxy-v7.0-beta.45.tar.gz tar zxf stroom-stacks…………..   For a stroom proxy, the configuration file - stroom_proxy/stroom_proxy-v7.0-beta.45/stroom_proxy.env\nneeds to be edited, with the connection details of the stroom server that data files will be sent to.\nThe default network port for connection to the stroom server is 8080 The values that need to be set are:\nSTROOM_PROXY_REMOTE_FEED_STATUS_API_KEY\nSTROOM_PROXY_REMOTE_FEED_STATUS_URL\nSTROOM_PROXY_REMOTE_FORWARD_URL\nThe ‘API key’ is generated on the stroom server and is related to a specific user e.g. proxyServiceUser The 2 URL values also refer to the stroom server and can be a fully qualified domain name (fqdn) or the IP Address.\ne.g. if the stroom server was - stroom-serve.somewhere.co.uk - the URL lines would be:\nexport STROOM_PROXY_REMOTE_FEED_STATUS_URL=“http://stroom-serve.somewhere.co.uk:8080/api/feedStatus/v1\"\nexport STROOM_PROXY_REMOTE_FORWARD_URL=“http://stroom-serve.somewhere.co.uk:8080/stroom/datafeed\"\n To Start Stroom Proxy As the stroom user, run the ‘start.sh’ script found in the stroom install:\n cd ~/stroom_proxy/stroom_proxy-v7.0-beta.45/ ./start.sh  The first time the script is ran it will download from github the docker containers for a stroom proxy\nthese are - stroom-proxy-remote, stroom-log-sender and nginx.\nOnce the script has completed the stroom proxy server should be running. There are additional scripts - status.sh - that will show the status of the docker containers (stroom-proxy-remote, stroom-log-sender and nginx) and - logs.sh - that will tail all of the stroom message files to the screen.\n  Stroom Remote Proxy (app version) The build of a stroom proxy server, where the stroom application is running locally as a Java ARchive (jar) file.\nThe operating system (OS) build for an ‘application’ stroom proxy is minimal RHEL/CentOS 7 plus Java.\nThe Java version required for stroom v7 is 12+ This version of Java is not available from the RHEL/CentOS distribution.\nThe version of Java used below is the ‘openJDK’ version as opposed to Oracle’s version.\nThis can be downloaded from the internet.\nVersion 12.0.1\nwget https://download.java.net/java/GA/jdk12.0.1/69cfe15208a647278a19ef0990eea691/12/GPL/openjdk-12.0.1_lin ux-x64_bin.tar.gz Or version 14.0.2 https://download.java.net/java/GA/jdk14.0.2/205943a0976c4ed48cb16f1043c5c647/12/GPL/openjdk-14.0.2_linux-x64_bin.tar.gz The gzipped tar file needs to be untarred and moved to a suitable location.\n tar xvf openjdk-12.0.1_linux-x64_bin.tar.gz mv jdk-12.0.1 /opt/\n  Create a shell script that will define the Java variables\tOR add the statements to .bash_profile e.g. vi /etc/profile.d/jdk12.sh\nexport JAVA_HOME=/opt/jdk-12.0.1\nexport PATH=$PATH:$JAVA_HOME/bin\n  source /etc/profile.d/jdk12.sh\n  echo $JAVA_HOME\n/opt/jdk-12.0.1\n  java –version openjdk version “12.0.1” 2019-04-16\nOpenJDK Runtime Environment (build 12.0.1+12)\nOpenJDK 64-Bit Server VM (build 12.0.1+12, mixed mode, sharing)\n   **Disable selinux to avoid issues with access and file permissions. **\n Firewall Configuration If you have a firewall running additional ports will need to be opened, to allow the Docker containers to talk to each other.\nCurrently these ports are:\n 3307 8080 8081 8090 8091 8543 5000 2888 443 80  For example on a RHEL/CentOS server using firewalld the commands would be as root user:\nfirewall-cmd --zone=public --permanent --add-port=3307/tcp firewall-cmd --zone=public --permanent --add-port=8080/tcp firewall-cmd --zone=public --permanent --add-port=8081/tcp firewall-cmd --zone=public --permanent --add-port=8090/tcp firewall-cmd --zone=public --permanent --add-port=8091/tcp firewall-cmd --zone=public --permanent --add-port=8099/tcp firewall-cmd --zone=public --permanent --add-port=5000/tcp firewall-cmd --zone=public --permanent --add-port=2888/tcp firewall-cmd --zone=public --permanent --add-port=443/tcp firewall-cmd --zone=public --permanent --add-port=80/tcp firewall-cmd --reload   Download and install Stroom v7 (app version) The installation example below is for stroom version 7.0.beta.45 - but is applicable to other stroom v7 versions.\nAs a suitable stroom user e.g. stroomuser - download and unpack the stroom software.\nwget https://github.com/gchq/stroom/releases/download/v7.0-beta.45/stroom-proxy-app-v7.0-beta.45.zip unzip stroom-proxy-app..............   The configuration file – stroom-proxy/config/config.yml – is the principal file to be edited, as it contains\n connection details to the stroom server the locations of the proxy server log files the directory on the proxy server, where data files will be stored prior to forwarding onot stroom the location of the PKI Java keystore (jks) files\n  The log file locations are changed to be relative to where stroom is started i.e. ~stroomuser/stroom-proxy/logs/…..\ncurrentLogFilename: logs/events/event.log\tarchivedLogFilenamePattern: logs/events/event-%d{yyyy-MM-dd'T'HH:mm}.log currentLogFilename: logs/events/event.log archivedLogFilenamePattern: logs/events/event-%d{yyyy-MM-dd'T'HH:mm}.log currentLogFilename: logs/send/send.log archivedLogFilenamePattern: logs/send/send-%d{yyyy-MM-dd'T'HH:mm}.log.gz currentLogFilename: logs/app/app.log archivedLogFilenamePattern: logs/app/app-%d{yyyy-MM-dd'T'HH:mm}.log.gz   An API key created on the stroom server for a special proxy user is added to the configuration file. The API key is used to validate access to the application\nproxyConfig: useDefaultOpenIdCredentials: **false** proxyContentDir: \"/stroom-proxy/content\" #If you want to use a receipt policy then the RuleSet must exist #in Stroom and have the UUID as specified below in receiptPolicyUuid #proxyRequestConfig: #receiptPolicyUuid: \"${RECEIPT_POLICY_UUID:-}\" feedStatus: url: **“http://stroomserver.somewhere.co.uk:8080/api/feedStatus/v1}\"** apiKey: **\" eyJhbGciOiJSUz ……………………….ScdPX0qai5UwlBA\"** forwardStreamConfig: forwardingEnabled: true   The location of the jks files has to be set, or comment all of the lines that have sslConfig: \u0026 tls: sections out to not use jks checking.\nStroom also needs the client \u0026 ca ‘jks’ files and by default are located in - /stroom-proxy/certs/ca.jks \u0026 client.jks Their location can be changed in the config.yml\nkeyStorePath: \"/stroom-proxy/certs/client.jks\" trustStorePath: \"/stroom-proxy/certs/ca.jks\" keyStorePath: \"/stroom-proxy/certs/client.jks\" trustStorePath: \"/stroom-proxy/certs/ca.jks\"  Could be changed to……………….\nkeyStorePath: \"/home/stroomuser/stroom-proxy/certs/client.jks\" trustStorePath: \"/home/stroomuser/stroom-proxy/certs/ca.jks\" keyStorePath: \"/home/stroomuser/stroom-proxy/certs/client.jks\" trustStorePath: \"/home/stroomuser/stroom-proxy/certs/ca.jks\"   Create a directory - /stroom-proxy – and ensure that stroom can write to it\nThis is where the proxy data files are stored - /stroom-proxy/repo\nproxyRepositoryConfig: storingEnabled: true repoDir: **\"/stroom-proxy/repo\"** format: \"${executionUuid}/${year}-${month}-${day}/${feed}/${pathId}/${id}\"   ","categories":"","description":"","excerpt":"There are 2 versions of the stroom software availble for building a …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/stroom-7-proxy-installation/","tags":["proxy"],"title":"Stroom Proxy Installation"},{"body":"Stroom v6+ Stroom is designed to detect the version of the database schema present and to run any migrations necessary to bring it up to the version begin deployed.\nDocker stack deployments TODO\nNon-docker deployments TODO\nMajor version upgrades The following notes are specific for these major version upgrades\n v6 =\u003e v7  Stroom v5 The cleanest way to upgrade or patch is to simply remove the installed content and reinstall. For example:\n./stroom-deploy/stop.sh rm -fr stroom-app* \u003cunzip new builds as per install instructions\u003e \u003crun setup.sh as per install instructions\u003e ./stroom-deploy/start.sh  It is extremely important that you enter the configuration parameters correctly. In particular the node name should match the current node name otherwise Stroom will create a new node in the system thinking it is part of a cluster. It is recommended that you copy the original parameters file values used in the original installation to help with this (e.g. cp stroom-app/bin/~setup.xml /tmp/orig-stroom-app-setup.xml.\nYou should remove and reinstall all components you originally installed i.e. stroom-deploy-X-Y-Z, stroom-app-X-Y-X as required.\nYou should temporary disable the cron auto deploy script if you have it running during the above.\nPatching You can choose for a minor patch (1-2-X) to simply copy the new WAR file into relevant lib directory and run the deploy.sh script (which you may have running on a cron tab). However this would not patch any potential script or tomcat setting changes.\n","categories":"","description":"","excerpt":"Stroom v6+ Stroom is designed to detect the version of the database …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/upgrade-patch/","tags":["upgrade"],"title":"Stroom Upgrades"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/todo/","tags":"","title":"TODO"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/upgrade/","tags":"","title":"upgrade"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/docs/install-guide/upgrades/","tags":"","title":"Upgrades"},{"body":"Average Takes an average value of the arguments\naverage(arg) mean(arg)  Examples\naverage(${val}) ${val} = [10, 20, 30, 40] \u003e 25 mean(${val}) ${val} = [10, 20, 30, 40] \u003e 25  Count Counts the number of records that are passed through it. Doesn’t take any notice of the values of any fields.\ncount()  Examples\nSupplying 3 values... count() \u003e 3  Count Groups This is used to count the number of unique values where there are multiple group levels. For Example, a data set grouped as follows\n Group by Name Group by Type  A groupCount could be used to count the number of distinct values of ‘type’ for each value of ‘name’\nCount Unique This is used to count the number of unique values passed to the function where grouping is used to aggregate values in other columns. For Example, a data set grouped as follows\n Group by Name Group by Type  countUnique() could be used to count the number of distinct values of ‘type’ for each value of ‘name’\nExamples\ncountUnique(${val}) ${val} = ['bill', 'bob', 'fred', 'bill'] \u003e 3  Joining Concatenates all values together into a single string. If a delimter is supplied then the delimiter is placed bewteen each concatenated string. If a limit is supplied then it will only concatenate up to limit values.\njoining(values) joining(values, delimiter) joining(values, delimiter, limit)  Examples\njoining(${val}, ', ') ${val} = ['bill', 'bob', 'fred', 'bill'] \u003e 'bill, bob, fred, bill'  Max Determines the maximum value given in the args\nmax(arg)  Examples\nmax(${val}) ${val} = [100, 30, 45, 109] \u003e 109 # They can be nested max(max(${val}), 40, 67, 89) ${val} = [20, 1002] \u003e 1002  Min Determines the minimum value given in the args\nmin(arg)  Examples\nmin(${val}) ${val} = [100, 30, 45, 109] \u003e 30  They can be nested\nmin(max(${val}), 40, 67, 89) ${val} = [20, 1002] \u003e 20  Standard Deviation Calculate the standard deviation for a set of input values.\nstDev(arg)  Examples\nround(stDev(${val})) ${val} = [600, 470, 170, 430, 300] \u003e 147  Sum Sums all the arguments together\nsum(arg)  Examples\nsum(${val}) ${val} = [89, 12, 3, 45] \u003e 149  Variance Calculate the variance of a set of input values.\nvariance(arg)  Examples\nvariance(${val}) ${val} = [600, 470, 170, 430, 300] \u003e 21704  ","categories":"","description":"Functions that produce aggregates over multiple data points.\n","excerpt":"Functions that produce aggregates over multiple data points.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/expressions/aggregate/","tags":"","title":"Aggregate Functions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/api/","tags":"","title":"api"},{"body":"Stroom has a number of public APIs to allow other systems to interact with Stroom. The APIs are defined by a Swagger spec that can be viewed as json (external link) or yaml (external link). A dynamic user interface (external link) is also available for viewing the APIs.\n","categories":"","description":"Stroom' public APIs for querying and interacting with all aspects of Stroom.\n","excerpt":"Stroom' public APIs for querying and interacting with all aspects of …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/api/","tags":["api"],"title":"Application Programming Interfaces (API)"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/authorisation/","tags":"","title":"authorisation"},{"body":"To Boolean Attempts to convert the passed value to a boolean data type.\ntoBoolean(arg1)  Examples:\ntoBoolean(1) \u003e true toBoolean(0) \u003e false toBoolean('true') \u003e true toBoolean('false') \u003e false  To Double Attempts to convert the passed value to a double data type.\ntoDouble(arg1)  Examples:\ntoDouble('1.2') \u003e 1.2  To Integer Attempts to convert the passed value to a integer data type.\ntoInteger(arg1)  Examples:\ntoInteger('1') \u003e 1  To Long Attempts to convert the passed value to a long data type.\ntoLong(arg1)  Examples:\ntoLong('1') \u003e 1  To String Attempts to convert the passed value to a string data type.\ntoString(arg1)  Examples:\ntoString(1.2) \u003e '1.2'  ","categories":"","description":"A set of functions for converting between different data types or for working with data types.\n","excerpt":"A set of functions for converting between different data types or for …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/expressions/cast/","tags":"","title":"Cast Functions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/cluster/","tags":"","title":"cluster"},{"body":" Version Information: Created with Stroom v7.0\nLast Updated: 10 September 2020\n Stroom has a number of tools that are available from the command line in addition to starting the main application.\nThe basic structure of the command for starting one of stroom’s commands is:\njava -jar /absolute/path/to/stroom-app.jar COMMAND  COMMAND can be a number of values depending on what you want to do. Each command value is described in its own section.\n NOTE: These commands are very powerful and potentially dangerous in the wrong hands, e.g. they allow the changing of user’s passwords. Access to these commands should be strictly limited. Also, each command will run in its own JVM so are not really intended to be run when Stroom is running on the node.\n server java -jar /absolute/path/to/stroom-app.jar server path/to/config.yml  This is the normal command for starting the Stroom application using the supplied YAML configuration file. The example above will start the application as a foreground process. Stroom would typically be started using the start.sh shell script, but the command above is listed for completeness.\nWhen stroom starts it will check the database to see if any migration is required. If migration from an earlier version (including from an empty database) is required then this will happen as part of the application start process.\nmigrate java -jar /absolute/path/to/stroom-app.jar migrate path/to/config.yml  There may be occasions where you want to migrate an old version but not start the application, e.g. during migration testing or to initiate the migration before starting up a cluster. This command will run the process that checks for any required migrations and then performs them. On completion of the process it exits. This runs as a foreground process.\ncreate_account java -jar /absolute/path/to/stroom-app.jar create_account --u USER --p PASSWORD [OPTIONS] path/to/config.yml  Where the named arguments are:\n -u --user - The username for the user. -p --password - The password for the user. -e --email - The email address of the user. -f --firstName - The first name of the user. -s --lastName - The last name of the user. --noPasswordChange - If set do not require a password change on first login. --neverExpires - If set, the account will never expire.  This command will create an account in the internal identity provider within Stroom. Stroom is able to use third party OpenID identity providers such as Google or AWS Cognito but by default will use its own. When configured to use its own (the default) it will auto create an admin account when starting up a fresh instance. There are times when you may wish to create this account manually which this command allows.\nAuthentication Accounts and Stroom Users The user account used for authentication is distinct to the Stroom user entity that is used for authorisation within Stroom. If an external IDP is used then the mechanism for creating the authentication account will be specific to that IDP. If using the default internal Stroom IDP then an account must be created in order to authenticate, either from within the UI if you are already authenticated as a privileged used or using this command. In either case a Stroom user will need to exist with the same username as the authentication account.\nThe command will fail if the user already exists. This command should NOT be run if you are using a third party identity provider.\nThis command will also run any necessary database migrations to ensure it is working with the correct version of the database schema.\nreset_password java -jar /absolute/path/to/stroom-app.jar reset_password --u USER --p PASSWORD path/to/config.yml  Where the named arguments are:\n -u --user - The username for the user. -p --password - The password for the user.  This command is used for changing the password of an existing account in stroom’s internal identity provider. It will also reset all locked/inactive/disabled statuses to ensure the account can be logged into. This command should NOT be run if you are using a third party identity provider. It will fail if the account does not exist.\nThis command will also run any necessary database migrations to ensure it is working with the correct version of the database schema.\nmanage_users java -jar /absolute/path/to/stroom-app.jar manage_users [OPTIONS] path/to/config.yml  Where the named arguments are:\n --createUser USER_NAME - Creates a Stroom user with the supplied username. --greateGroup GROUP_NAME - Creates a Stroom user group with the supplied group name. --addToGroup USER_OR_GROUP_NAME TARGET_GROUP - Adds a user/group to an existing group. --removeFromGroup USER_OR_GROUP_NAME TARGET_GROUP - Removes a user/group from an existing group. --grantPermission USER_OR_GROUP_NAME PERMISSION_NAME - Grants the named application permission to the user/group. --revokePermission USER_OR_GROUP_NAME PERMISSION_NAME - Revokes the named application permission from the user/group. --listPermissions - Lists all the valid permission names.  This command allows you to manage the account permissions within stroom regardless of whether the internal identity provider or a 3rd party one is used. A typical use case for this is when using a third party identity provider. In this instance Stroom has no way of auto creating an admin account when first started so the association between the account on the 3rd party IDP and the stroom user account needs to be made manually. To set up an admin account to enable you to login to stroom you can do:\nThis command is not intended for automation of user management tasks on a running Stroom instance that you can authenticate with. It is only intended for cases where you cannot authenticate with Stroom, i.e. when setting up a new Stroom with a 3rd party IDP or when scripting the creation of a test environment. If you want to automate actions that can be performed in the UI then you can make use of the REST API that is described at /stroom/noauth/swagger-ui.\n See the note above about the distinction between authentication accounts and stroom users.\n java -jar /absolute/path/to/stroom-app.jar manage_users --createUser jbloggs --createGroup Administrators --addToGroup jbloggs Administrators --grantPermission Administrators \"Administrator\" path/to/config.yml  Where jbloggs is the user name of the account on the 3rd party IDP.\nThis command will also run any necessary database migrations to ensure it is working with the correct version of the database schema.\nThe named arguments can be used as many times as you like so you can create multiple users/groups/grants/etc. Regardless of the order of the arguments, the changes are executed in the following order:\n Create users Create groups Add users/groups to a group Remove users/groups from a group Grant permissions to users/groups Revoke permissions from users/groups  ","categories":"","description":"Command line actions for administering Stroom.\n","excerpt":"Command line actions for administering Stroom.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/tools/command-line/","tags":"","title":"Command Line Tools"},{"body":"It is possible to concatenate multiple fixed strings and match group references using the + character. As with all references and fixed strings this can be done in \u003cgroup\u003e value and \u003cdata\u003e name and value attributes. However concatenation does have some performance overhead as new buffers have to be created to store concatenated content.\nA good example of concatenation is the production of ISO8601 date format from data in the previous example:\n01/01/2010,00:00:00  Here the following \u003cregex\u003e could be used to extract the relevant date, time groups:\n\u003cregex pattern=\"(\\d{2})/(\\d{2})/(\\d{4}),(\\d{2}):(\\d{2}):(\\d{2})\"\u003e  The match groups from this expression can be concatenated with the following value output pattern in the data element:\n\u003cdata name=\"dateTime\" value=\"$3+’-‘+$2+’-‘+$1+’-‘+’T’+$4+’:’+$5+’:’+$6+’.000Z’\" /\u003e  Using the original example, this would result in the output:\n\u003cdata name=\"dateTime\" value=\"2010-01-01T00:00:00.000Z\" /\u003e  Note that the value output pattern wraps all fixed strings in single quotes. This is necessary when concatenating strings and references so that Data Splitter can determine which parts are to be treated as fixed strings. This also allows fixed strings to contain $ and + characters.\nAs single quotes are used for this purpose, a single quote needs to be escaped with another single quote if one is desired in a fixed string, e.g.\n‘this ‘’is quoted text’’’  will result in:\nthis ‘is quoted text’  ","categories":"","description":"","excerpt":"It is possible to concatenate multiple fixed strings and match group …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-splitter/match-reference/3-4-concatenation-of-references/","tags":"","title":"Concatenation of references"},{"body":"","categories":"","description":"Describes a number of core concepts involved in using Stroom.\n","excerpt":"Describes a number of core concepts involved in using Stroom.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/concepts/","tags":"","title":"Concepts"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/configuration/","tags":"","title":"configuration"},{"body":"Content providers take some content from the input source or elsewhere (see fixed strings and provide it to one or more expressions. Both the root element \u003cdataSplitter\u003e and \u003cgroup\u003e elements are content providers.\nRoot element \u003cdataSplitter\u003e The root element of a Data Splitter configuration is \u003cdataSplitter\u003e. It supplies content from the input source to one or more expressions defined within it. The way that content is buffered is controlled by the root element and the way that errors are handled as a result of child expressions not matching all of the content it supplies.\nAttributes The following attributes can be added to the \u003cdataSplitter\u003e root element:\n ignoreErrors bufferSize  ignoreErrors Data Splitter generates errors if not all of the content is matched by the regular expressions beneath the \u003cdataSplitter\u003e or within \u003cgroup\u003e elements. The error messages are intended to aid the user in writing good Data Splitter configurations. The intent is to indicate when the input data is not being matched fully and therefore possibly skipping some important data. Despite this, in some cases it is laborious to have to write expressions to match all content. In these cases it is preferable to add this attribute to ignore these errors. However it is often better to write expressions that capture all of the supplied content and discard unwanted characters. This attribute also affects errors generated by the use of the minMatch attribute on \u003cregex\u003e which is described later on.\nTake the following example input:\nName1,Name2,Name3 value1,value2,value3 # a useless comment value1,value2,value3 # a useless comment  This could be matched with the following configuration:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\"\u003e \u003cregex id=\"heading\" pattern=\".+\" maxMatch=\"1\"\u003e … \u003c/regex\u003e \u003cregex id=\"body\" pattern=\"\\n[^#]+\"\u003e … \u003c/regex\u003e \u003c/dataSplitter\u003e  The above configuration would only match up to a comment for each record line, e.g.\nName1,Name2,Name3 value1,value2,value3 # a useless comment value1,value2,value3 # a useless comment  This may well be the desired functionality but if there was useful content within the comment it would be lost. Because of this Data Splitter warns you when expressions are failing to match all of the content presented so that you can make sure that you aren’t missing anything important. In the above example it is obvious that this is the required behaviour but in more complex cases you might be otherwise unaware that your expressions were losing data.\nTo maintain this assurance that you are handling all content it is usually best to write expressions to explicitly match all content even though you may do nothing with some matches, e.g.\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\"\u003e \u003cregex id=\"heading\" pattern=\".+\" maxMatch=\"1\"\u003e … \u003c/regex\u003e \u003cregex id=\"body\" pattern=\"\\n([^#]+)#.+\"\u003e … \u003c/regex\u003e \u003c/dataSplitter\u003e  The above example would match all of the content and would therefore not generate warnings. Sub-expressions of ‘body’ could use match group 1 and ignore the comment.\nHowever as previously stated it might often be difficult to write expressions that will just match content that is to be discarded. In these cases ignoreErrors can be used to suppress errors caused by unmatched content.\nbufferSize (Advanced) This is an optional attribute used to tune the size of the character buffer used by Data Splitter. The default size is 20000 characters and should be fine for most translations. The minimum value that this can be set to is 20000 characters and the maximum is 1000000000. The only reason to specify this attribute is when individual records are bigger than 10000 characters which is rarely the case.\nGroup element \u003cgroup\u003e Groups behave in a similar way to the root element in that they provide content for one or more inner expressions to deal with, e.g.\n\u003cgroup value=\"$1\"\u003e \u003cregex pattern=\"([^\\t]*)\\t([^\\t]*)[\\t]*([^=:]*)[=:]*(.*)\" maxMatch=\"1\"\u003e ... \u003cregex pattern=\"([^\\t]*)\\t([^\\t]*)[\\t]*([^=:]*)[=:]*(.*)\"\u003e ...  Attributes As the \u003cgroup\u003e element is a content provider it also includes the same ‘ignoreErrors’ attribute which behaves in the same way. The complete list of attributes for the \u003cgroup\u003e element is as follows:\n id value ignoreErrors matchOrder reverse  id When Data Splitter reports errors it outputs an XPath to describe the part of the configuration that generated the error, e.g.\nDSParser [2:1] ERROR: Expressions failed to match all of the content provided by group: regex[0]/group[0]/regex[3]/group[1] : \u003cgroup\u003e  It is often a little difficult to identify the configuration element that generated the error by looking at the path and the element description, particularly when multiple elements are the same, e.g. many \u003cgroup\u003e elements without attributes. To make identification easier you can add an ‘id’ attribute to any element in the configuration resulting in error descriptions as follows:\nDSParser [2:1] ERROR: Expressions failed to match all of the content provided by group: regex[0]/group[0]/regex[3]/group[1] : \u003cgroup id=\"myGroupId\"\u003e  value This attribute determines what content to present to child expressions. By default the entire content matched by a group’s parent expression is passed on by the group to child expressions. If required, content from a specific match group in the parent expression can be passed to child expressions using the value attribute, e.g. value=\"$1\". In addition to this content can be composed in the same way as it is for data names and values. see match references for a full description of match references.\nignoreErrors This behaves in the same way as for the root element.\nmatchOrder This is an optional attribute used to control how content is consumed by expression matches. Content can be consumed in sequence or in any order using matchOrder=\"sequence\" or matchOrder=\"any\". If the attribute is not specified, Data Splitter will default to matching in sequence.\nWhen matching in sequence, each match consumes some content and the content position is moved beyond the match ready for the subsequent match. However, in some cases the order of these constructs is not predictable, e.g. we may sometimes be presented with:\nValue1=1 Value2=2  … or sometimes with:\nValue2=2 Value1=1  Using a sequential match order the following example would work to find both values in Value1=1 Value2=2\n\u003cgroup\u003e \u003cregex pattern=\"Value1=([^ ]*)\"\u003e ... \u003cregex pattern=\"Value2=([^ ]*)\"\u003e ...  … but this example would skip over Value2 and only find the value of Value1 if the input was Value2=2 Value1=1.\nTo be able to deal with content that contains these constructs in either order we need to change the match order to any.\nWhen matching in any order, each match removes the matched section from the content rather than moving the position past the match so that all remaining content can be matched by subsequent expressions. In the following example the first expression would match and remove Value1=1 from the supplied content and the second expression would be presented with  Value2=2 which it could also match.\n\u003cgroup matchOrder=\"any\"\u003e \u003cregex pattern=\"Value1=([^ ]*)\"\u003e ... \u003cregex pattern=\"Value2=([^ ]*)\"\u003e ...  If the attribute is omitted by default the match order will be sequential. This is the default behaviour as tokens are most often in sequence and consuming content in this way is more efficient as content does not need to be copied by the parser to chop out sections as is required for matching in any order. It is only necessary to use this feature when fields that are identifiable with a specific match can occur in any order.\nreverse Occasionally it is desirable to reverse the content presented by a group to child expressions. This is because it is sometimes easier to form a pattern by matching content in reverse.\nTake the following example content of name, value pairs delimited by = but with no spaces between names, multiple spaces between values and only a space between subsequent pairs:\nipAddress=123.123.123.123 zones=Zone 1, Zone 2, Zone 3 location=loc1 A user=An end user serverName=bigserver  We could write a pattern that matches each name value pair by matching up to the start of the next name, e.g.\n\u003cregex pattern=\"([^=]+)=(.+?)( [^=]+=)\"\u003e  This would match the following:\nipAddress=123.123.123.123 zones=  Here we are capturing the name and value for each pair in separate groups but the pattern has to also match the name from the next name value pair to find the end of the value. By default Data Splitter will move the content buffer to the end of the match ready for subsequent matches so the next name will not be available for matching.\nIn addition to matching too much content the above example also uses a reluctant qualifier .+?. Use of reluctant qualifiers almost always impacts performance so they are to be avoided if at all possible.\nA better way to match the example content is to match the input in reverse, reading characters from right to left.\nThe following example demonstrates this:\n\u003cgroup reverse=\"true\"\u003e \u003cregex pattern=\"([^=]+)=([^ ]+)\"\u003e \u003cdata name=\"$2\" value=\"$1\" /\u003e \u003c/regex\u003e \u003c/group\u003e  Using the reverse attribute on the parent group causes content to be supplied to all child expressions in reverse order. In the above example this allows the pattern to match values followed by names which enables us to cope with the fact that values have multiple spaces but names have no spaces.\nContent is only presented to child regular expressions in reverse. When referencing values from match groups the content is returned in the correct order, e.g. the above example would return:\n\u003cdata name=\"ipAddress\" value=\"123.123.123.123\" /\u003e \u003cdata name=\"zones\" value=\"Zone 1, Zone 2, Zone 3\" /\u003e \u003cdata name=\"location\" value=\"loc1\" /\u003e \u003cdata name=\"user\" value=\"An end user\" /\u003e \u003cdata name=\"serverName\" value=\"bigserver\" /\u003e  The reverse feature isn’t needed very often but there are a few cases where it really helps produce the desired output without the complexity and performance overhead of a reluctant match.\nAn alternative to using the reverse attribute is to use the original reluctant expression example but tell Data Splitter to make the subsequent name available for the next match by not advancing the content beyond the end of the previous value. This is done by using the advance attribute on the \u003cregex\u003e. However, the reverse attribute represents a better way to solve this particular problem and allows a simpler and more efficient regular expression to be used.\n","categories":"","description":"","excerpt":"Content providers take some content from the input source or elsewhere …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-splitter/element-reference/2-1-content-providers/","tags":"","title":"Content Providers"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/context/","tags":"","title":"context"},{"body":"##Context File\n###Input File:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cSomeData\u003e \u003cSomeEvent\u003e \u003cSomeTime\u003e01/01/2009:12:00:01\u003c/SomeTime\u003e \u003cSomeAction\u003eOPEN\u003c/SomeAction\u003e \u003cSomeUser\u003euserone\u003c/SomeUser\u003e \u003cSomeFile\u003eD:\\TranslationKit\\example\\VerySimple\\OpenFileEvents.txt\u003c/SomeFile\u003e \u003c/SomeEvent\u003e \u003c/SomeData\u003e  ###Context File:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cSomeContext\u003e \u003cMachine\u003eMyMachine\u003c/Machine\u003e \u003c/SomeContext\u003e  ###Context XSLT:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003cxsl:stylesheet xmlns=\"reference-data:2\" xmlns:evt=\"event-logging:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" version=\"2.0\"\u003e \u003cxsl:template match=\"SomeContext\"\u003e \u003creferenceData xsi:schemaLocation=\"event-logging:3 file://event-logging-v3.0.0.xsd reference-data:2 file://reference-data-v2.0.1.xsd\" version=\"2.0.1\"\u003e \u003cxsl:apply-templates/\u003e \u003c/referenceData\u003e \u003c/xsl:template\u003e \u003cxsl:template match=\"Machine\"\u003e \u003creference\u003e \u003cmap\u003eCONTEXT\u003c/map\u003e \u003ckey\u003eMachine\u003c/key\u003e \u003cvalue\u003e\u003cxsl:value-of select=\".\"/\u003e\u003c/value\u003e \u003c/reference\u003e \u003c/xsl:template\u003e \u003c/xsl:stylesheet\u003e  ###Context XML Translation:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003creferenceData xmlns:evt=\"event-logging:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"reference-data:2\" xsi:schemaLocation=\"event-logging:3 file://event-logging-v3.0.0.xsd reference-data:2 file://reference-data-v2.0.1.xsd\" version=\"2.0.1\"\u003e \u003creference\u003e \u003cmap\u003eCONTEXT\u003c/map\u003e \u003ckey\u003eMachine\u003c/key\u003e \u003cvalue\u003eMyMachine\u003c/value\u003e \u003c/reference\u003e \u003c/referenceData\u003e  ###Input File:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cSomeData\u003e \u003cSomeEvent\u003e \u003cSomeTime\u003e01/01/2009:12:00:01\u003c/SomeTime\u003e \u003cSomeAction\u003eOPEN\u003c/SomeAction\u003e \u003cSomeUser\u003euserone\u003c/SomeUser\u003e \u003cSomeFile\u003eD:\\TranslationKit\\example\\VerySimple\\OpenFileEvents.txt\u003c/SomeFile\u003e \u003c/SomeEvent\u003e \u003c/SomeData\u003e  ###Main XSLT (Note the use of the context lookup):\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003cxsl:stylesheet xmlns=\"event-logging:3\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"2.0\"\u003e \u003cxsl:template match=\"SomeData\"\u003e \u003cEvents xsi:schemaLocation=\"event-logging:3 file://event-logging-v3.0.0.xsd\" Version=\"3.0.0\"\u003e \u003cxsl:apply-templates/\u003e \u003c/Events\u003e \u003c/xsl:template\u003e \u003cxsl:template match=\"SomeEvent\"\u003e \u003cxsl:if test=\"SomeAction = 'OPEN'\"\u003e \u003cEvent\u003e \u003cEventTime\u003e \u003cTimeCreated\u003e \u003cxsl:value-of select=\"s:format-date(SomeTime, 'dd/MM/yyyy:hh:mm:ss')\"/\u003e \u003c/TimeCreated\u003e \u003c/EventTime\u003e \u003cEventSource\u003e \u003cSystem\u003eExample\u003c/System\u003e \u003cEnvironment\u003eExample\u003c/Environment\u003e \u003cGenerator\u003eVery Simple Provider\u003c/Generator\u003e \u003cDevice\u003e \u003cIPAddress\u003e182.80.32.132\u003c/IPAddress\u003e \u003cLocation\u003e \u003cCountry\u003eUK\u003c/Country\u003e \u003cSite\u003e\u003cxsl:value-of select=\"s:lookup('CONTEXT', 'Machine')\"/\u003e\u003c/Site\u003e \u003cBuilding\u003eMain\u003c/Building\u003e \u003cFloor\u003e1\u003c/Floor\u003e \u003cRoom\u003e1aaa\u003c/Room\u003e \u003c/Location\u003e \u003c/Device\u003e \u003cUser\u003e\u003cId\u003e\u003cxsl:value-of select=\"SomeUser\"/\u003e\u003c/Id\u003e\u003c/User\u003e \u003c/EventSource\u003e \u003cEventDetail\u003e \u003cView\u003e \u003cDocument\u003e \u003cTitle\u003eUNKNOWN\u003c/Title\u003e \u003cFile\u003e \u003cPath\u003e\u003cxsl:value-of select=\"SomeFile\"/\u003e\u003c/Path\u003e \u003c/File\u003e \u003c/Document\u003e \u003c/View\u003e \u003c/EventDetail\u003e \u003c/Event\u003e \u003c/xsl:if\u003e \u003c/xsl:template\u003e \u003c/xsl:stylesheet\u003e  ###Main Output XML:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cEvents xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"event-logging:3\" xsi:schemaLocation=\"event-logging:3 file://event-logging-v3.0.0.xsd\" Version=\"3.0.0\"\u003e \u003cEvent Id=\"6:1\"\u003e \u003cEventTime\u003e \u003cTimeCreated\u003e2009-01-01T00:00:01.000Z\u003c/TimeCreated\u003e \u003c/EventTime\u003e \u003cEventSource\u003e \u003cSystem\u003eExample\u003c/System\u003e \u003cEnvironment\u003eExample\u003c/Environment\u003e \u003cGenerator\u003eVery Simple Provider\u003c/Generator\u003e \u003cDevice\u003e \u003cIPAddress\u003e182.80.32.132\u003c/IPAddress\u003e \u003cLocation\u003e \u003cCountry\u003eUK\u003c/Country\u003e \u003cSite\u003eMyMachine\u003c/Site\u003e \u003cBuilding\u003eMain\u003c/Building\u003e \u003cFloor\u003e1\u003c/Floor\u003e \u003cRoom\u003e1aaa\u003c/Room\u003e \u003c/Location\u003e \u003c/Device\u003e \u003cUser\u003e \u003cId\u003euserone\u003c/Id\u003e \u003c/User\u003e \u003c/EventSource\u003e \u003cEventDetail\u003e \u003cView\u003e \u003cDocument\u003e \u003cTitle\u003eUNKNOWN\u003c/Title\u003e \u003cFile\u003e \u003cPath\u003eD:\\TranslationKit\\example\\VerySimple\\OpenFileEvents.txt\u003c/Path\u003e \u003c/File\u003e \u003c/Document\u003e \u003c/View\u003e \u003c/EventDetail\u003e \u003c/Event\u003e \u003c/Events\u003e  ","categories":"","description":"","excerpt":"##Context File\n###Input File:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/pipelines/parser/context-data/","tags":["context"],"title":"Context Data"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/dashboard/","tags":"","title":"dashboard"},{"body":"Expressions can be used to manipulate data on Stroom Dashboards.\nEach function has a name, and some have additional aliases.\nIn some cases, functions can be nested. The return value for some functions being used as the arguments for other functions.\nThe arguments to functions can either be other functions, literal values, or they can refer to fields on the input data using the field reference ${val} syntax.\nAggregate Functions   Average Count Count Groups Count Unique Joining Max Min Standard Deviation Sum Variance    String Functions   Concat Current User Decode DecodeUrl EncodeUrl Exclude Hash Include Index Of Last Index Of Lower Case Match Query Param Query Params Replace String Length Substring Substring After Substring Before Upper Case    Mathematics Functions   Add Average Divide Max Min Modulo Multiply Negate Power Random Subtract Sum    Type Checking Functions   Is Boolean Is Double Is Error Is Integer Is Long Is Null Is Number Is String Is Value Type Of     Link Functions   Annotation Dashboard Data Link Stepping    Cast Functions   To Boolean To Double To Integer To Long To String    Date Functions   Format Date Parse Date Ceiling Functions Floor Functions Round Functions    Logic Functions   Equals Greater Than Greater Than or Equal To If Less Than Less Than or Equal To Not     Rounding Functions   Ceiling Floor Round    Selection Functions   Any Bottom First Last Nth Top    URI Functions   extractAuthorityFromUri extractFragmentFromUri extractHostFromUri extractPathFromUri extractPortFromUri extractQueryFromUri extractSchemeFromUri extractSchemeSpecificPartFromUri extractUserInfoFromUri    Value Functions   Err False Null True     ","categories":"","description":"Expression language used to manipulate data on Stroom Dashboards.\n","excerpt":"Expression language used to manipulate data on Stroom Dashboards.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/expressions/","tags":["dashboard","expression"],"title":"Dashboard Expressions"},{"body":"","categories":"","description":"The means of displaying and visualising the results of queries.\n","excerpt":"The means of displaying and visualising the results of queries.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/","tags":["dashboard"],"title":"Dashboards"},{"body":"By default Stroom will retain all the data it ingests and creates forever. It is likely that storage constraints/costs will mean that data needs to be deleted after a certain time. It is also likely that certain types of data may need to be kept for longer than other types.\nRules Stroom allows for a set of data retention policy rules to be created to control at a fine grained level what data is deleted and what is retained.\nThe data retention rules are accessible by selecting Data Retention from the Tools menu. On first use the Rules tab of the Data Retention screen will show a single rule named Default Retain All Forever Rule. This is the implicit rule in stroom that retains all data and is always in play unless another rule overrides it. This rule cannot be edited, moved or removed.\nRule Precedence Rules have a precedence, with a lower rule number being a higher priority. When running the data retention job, Stroom will look at each stream held on the system and the retention policy of the first rule (starting from the lowest numbered rule) that matches that stream will apply. One a matching rule is found all other rules with higher rule numbers (lower priority) are ignored. For example if rule 1 says to retain streams from feed X-EVENTS for 10 years and rule 2 says to retain streams from feeds *-EVENTS for 1 year then rule 1 would apply to streams from feed X-EVENTS and they would be kept for 10 years, but rule 2 would apply to feed Y-EVENTS and they would only be kept for 1 year. Rules are re-numbered as new rules are added/deleted/moved.\nCreating a Rule To create a rule do the following:\n Click the icon to add a new rule. Edit the expression to define the data that the rule will match on. Provide a name for the rule to help describe what its purpose is. Set the retention period for data matching this rule, i.e Forever or a set time period.  The new rule will be added at the top of the list of rules, i.e. with the highest priority. The and icons can be used to change the priority of the rule.\nRules can be enabled/disabled by clicking the checkbox next to the rule.\nChanges to rules will not take effect until the icon is clicked.\nRules can also be deleted ( ) and copied ( ).\nImpact Summary When you have a number of complex rules it can be difficult to determine what data will actually be deleted next time the Policy Based Data Retention job runs. To help with this, Stroom has the Impact Summary tab that acts as a dry run for the active rules. The impact summary provides a count of the number of streams that will be deleted broken down by rule, stream type and feed name. On large systems with lots of data or complex rules, this query may take a long time to run.\nThe impact summary operates on the current state of the rules on the Rules tab whether saved or un-saved. This allows you to make a change to the rules and test its impact before saving it.\n","categories":"","description":"Controlling the purging/retention of old data.\n","excerpt":"Controlling the purging/retention of old data.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-retention/","tags":["retention"],"title":"Data Retention"},{"body":"Data Splitter was created to transform text into XML. The XML produced is basic but can be processed further with XSLT to form any desired XML output.\nData Splitter works by using regular expressions to match a region of content or tokenizers to split content. The whole match or match group can then be output or passed to other expressions to further divide the matched data.\nThe root \u003cdataSplitter\u003e element controls the way content is read and buffered from the source. It then passes this content on to one or more child expressions that attempt to match the content. The child expressions attempt to match content one at a time in the order they are specified until one matches. The matching expression then passes the content that it has matched to other elements that either emit XML or apply other expressions to the content matched by the parent.\nThis process of content supply, match, (supply, match)*, emit is best illustrated in a simple CSV example. Note that the elements and attributes used in all examples are explained in detail in the element reference.\n","categories":"","description":"","excerpt":"Data Splitter was created to transform text into XML. The XML produced …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-splitter/","tags":"","title":"Data Splitter"},{"body":"Parse Date Parse a date and return a long number of milliseconds since the epoch.\nparseDate(aString) parseDate(aString, pattern) parseDate(aString, pattern, timeZone)  Example\nparseDate('2014 02 22', 'yyyy MM dd', '+0400') \u003e 1393012800000  Format Date Format a date supplied as milliseconds since the epoch.\nformatDate(aLong) formatDate(aLong, pattern) formatDate(aLong, pattern, timeZone)  Example\nformatDate(1393071132888, 'yyyy MM dd', '+1200') \u003e '2014 02 23'  Ceiling Year/Month/Day/Hour/Minute/Second ceilingYear(args...) ceilingMonth(args...) ceilingDay(args...) ceilingHour(args...) ceilingMinute(args...) ceilingSecond(args...)  Examples\nceilingSecond(\"2014-02-22T12:12:12.888Z\" \u003e \"2014-02-22T12:12:13.000Z\" ceilingMinute(\"2014-02-22T12:12:12.888Z\" \u003e \"2014-02-22T12:13:00.000Z\" ceilingHour(\"2014-02-22T12:12:12.888Z\" \u003e \"2014-02-22T13:00:00.000Z\" ceilingDay(\"2014-02-22T12:12:12.888Z\" \u003e \"2014-02-23T00:00:00.000Z\" ceilingMonth(\"2014-02-22T12:12:12.888Z\" \u003e \"2014-03-01T00:00:00.000Z\" ceilingYear(\"2014-02-22T12:12:12.888Z\" \u003e \"2015-01-01T00:00:00.000Z\"  Floor Year/Month/Day/Hour/Minute/Second floorYear(args...) floorMonth(args...) floorDay(args...) floorHour(args...) floorMinute(args...) floorSecond(args...)  Examples\nfloorSecond(\"2014-02-22T12:12:12.888Z\" \u003e \"2014-02-22T12:12:12.000Z\" floorMinute(\"2014-02-22T12:12:12.888Z\" \u003e \"2014-02-22T12:12:00.000Z\" floorHour(\"2014-02-22T12:12:12.888Z\" \u003e 2014-02-22T12:00:00.000Z\" floorDay(\"2014-02-22T12:12:12.888Z\" \u003e \"2014-02-22T00:00:00.000Z\" floorMonth(\"2014-02-22T12:12:12.888Z\" \u003e \"2014-02-01T00:00:00.000Z\" floorYear(\"2014-02-22T12:12:12.888Z\" \u003e \"2014-01-01T00:00:00.000Z\"  Round Year/Month/Day/Hour/Minute/Second roundYear(args...) roundMonth(args...) roundDay(args...) roundHour(args...) roundMinute(args...) roundSecond(args...)  Examples\nroundSecond(\"2014-02-22T12:12:12.888Z\") \u003e \"2014-02-22T12:12:13.000Z\" roundMinute(\"2014-02-22T12:12:12.888Z\") \u003e \"2014-02-22T12:12:00.000Z\" roundHour(\"2014-02-22T12:12:12.888Z\" \u003e \"2014-02-22T12:00:00.000Z\" roundDay(\"2014-02-22T12:12:12.888Z\" \u003e \"2014-02-23T00:00:00.000Z\" roundMonth(\"2014-02-22T12:12:12.888Z\" \u003e \"2014-03-01T00:00:00.000Z\" roundYear(\"2014-02-22T12:12:12.888Z\" \u003e \"2014-01-01T00:00:00.000Z\"  ","categories":"","description":"Functions for manipulating dates and times.\n","excerpt":"Functions for manipulating dates and times.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/expressions/date/","tags":"","title":"Date Functions"},{"body":"Creating Right click on a folder in the explorer tree that you want to create a dictionary in. Choose ‘New/Dictionary’ from the popup menu:\n TODO: Fix image\n Call the dictionary something like ‘My Dictionary’ and click OK.\n TODO: Fix image\n Now just add any search terms you want to the newly created dictionary and click save.\n TODO: Fix image\n You can add multiple terms.\n Terms on separate lines act as if they are part of an ‘OR’ expression when used in a search. Terms on a single line separated by spaces act as if they are part of an ‘AND’ expression when used in a search.  Using To perform a search using your dictionary, just choose the newly created dictionary as part of your search expression:\n TODO: Fix image\n ","categories":"","description":"","excerpt":"Creating Right click on a folder in the explorer tree that you want to …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/dictionaries/","tags":["TODO","dictionary"],"title":"Dictionaries"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/dictionary/","tags":"","title":"dictionary"},{"body":"It is possible to navigate directly to a specific Stroom dashboard using a direct URL. This can be useful when you have a dashboard that needs to be viewed by users that would otherwise not be using the Stroom user interface.\nURL format The format for the URL is as follows:\nhttps://\u003cHOST\u003e/stroom/dashboard?type=Dashboard\u0026uuid=\u003cDASHBOARD UUID\u003e[\u0026title=\u003cDASHBOARD TITLE\u003e][\u0026params=\u003cDASHBOARD PARAMETERS\u003e]\nExample:\nhttps://localhost/stroom/dashboard?type=Dashboard\u0026uuid=c7c6b03c-5d47-4b8b-b84e-e4dfc6c84a09\u0026title=My%20Dash\u0026params=userId%3DFred%20Bloggs\nHost and path The host and path are typically https://\u003cHOST\u003e/stroom/dashboard where \u003cHOST\u003e is the hostname/IP for Stroom.\ntype type is a required parameter and must always be Dashboard since we are opening a dashboard.\nuuid uuid is a required parameter where \u003cDASHBOARD UUID\u003e is the UUID for the dashboard you want a direct URL to, e.g. uuid=c7c6b03c-5d47-4b8b-b84e-e4dfc6c84a09\nThe UUID for the dashboard that you want to link to can be found by right clicking on the dashboard icon in the explorer tree and selecting Info.\nThe Info dialog will display something like this and the UUID can be copied from it:\nDB ID: 4 UUID: c7c6b03c-5d47-4b8b-b84e-e4dfc6c84a09 Type: Dashboard Name: Stroom Family App Events Dashboard Created By: INTERNAL Created On: 2018-12-10T06:33:03.275Z Updated By: admin Updated On: 2018-12-10T07:47:06.841Z  title (Optional) title is an optional URL parameter where \u003cDASHBOARD TITLE\u003e allows the specification of a specific title for the opened dashboard instead of the default dashboard name.\nThe inclusion of ${name} in the title allows the default dashboard name to be used and appended with other values, e.g. 'title=${name}%20-%20' + param.name\nparams (Optional) params is an optional URL parameter where \u003cDASHBOARD PARAMETERS\u003e includes any parameters that have been defined for the dashboard in any of the expressions, e.g. params=userId%3DFred%20Bloggs\nPermissions In order for as user to view a dashboard they will need the necessary permission on the various entities that make up the dashboard.\nFor a Lucene index query and associated table the following permissions will be required:\n Read permission on the Dashboard entity. Use permission on any Indexe entities being queried in the dashboard. Use permission on any Pipeline entities set as search extraction Pipelines in any of the dashboard’s tables. Use permission on any XSLT entities used by the above search extraction Pipeline entites. Use permission on any ancestor pipelines of any of the above search extraction Pipeline entites (if applicable). Use permission on any Feed entities that you want the user to be able to see data for.  For a SQL Statistics query and associated table the following permissions will be required:\n Read permission on the Dashboard entity. Use permission on the StatisticStore entity being queried.  For a visualisation the following permissions will be required:\n Read permission on any Visualiation entities used in the dashboard. Read permission on any Script entities used by the above Visualiation entities. Read permission on any Script entities used by the above Script entities.  ","categories":"","description":"Navigating directly to a specific _Stroom_ dashboard using a direct URL.\n","excerpt":"Navigating directly to a specific _Stroom_ dashboard using a direct …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/direct-urls/","tags":["dashboard"],"title":"Direct URLs"},{"body":"Viewing Data The data viewer is shown on the Data tab when you open (by double clicking) one of these items in the explorer tree:\n Feed - to show all data for that feed. Folder - to show all data for all feeds that are descendants of the folder. System Root Folder - to show all data for all feeds that are ancestors of the folder.  In all cases the data shown is dependant on the permissions of the user performing the action and any permissions set on the feeds/folders being viewed.\nThe Data Viewer screen is made up of the following three parts which are shown as three panes split horizontally.\nStream List This shows all streams within the opened entity (feed or folder). The streams are shown in reverse chronological order. By default Deleted and Locked streams are filtered out. The filtering can be changed by clicking on the icon. This will show all stream types by default so may be a mixture of Raw events, Events, Errors, etc. depending on the feed/folder in question.\nRelated Stream List This list only shows data when a stream is selected in the streams list above it. It shows all streams related to the currently selected stream. It may show streams that are ‘ancestors’ of the selected stream, e.g. showing the Raw Events stream for an Events stream, or show descendants, e.g. showing the Errors stream which resulted from processing the selected Raw Events stream.\nContent Viewer Pane This pane shows the contents of the stream selected in the Related Streams List. The content of a stream will differ depending on the type of stream selected and the child stream types in that stream. For more information on the anatomy of streams, see Streams. This pane is split into multiple sub tabs depending on the different types of content available.\nInfo Tab This sub-tab shows the information for the stream, such as creation times, size, physical file location, state, etc.\nError Tab This sub-tab is only visible for an Error stream. It shows a table of errors and warnings with associated messages and locations in the stream that it relates to.\nData Preview Tab This sub-tab shows the content of the data child stream, formatted if it is XML. It will only show a limited amount of data so if the data child stream is large then it will only show the first n characters.\nIf the stream is multi-part then you will see Part navigation controls to switch between parts. For each part you will be shown the first n character of that part (formatted if applicable).\nIf the stream is a Segmented stream stream then you will see the Record navigation controls to switch between records. Only one record is shown at once. If a record is very large then only the first n characters of the record will be shown.\nThis sub-tab is intended for seeing a quick preview of the data in a form that is easy to read, i.e. formatted. If you want to see the full data in its original form then click on the View Source link at the top right of the sub-tab.\nThe Data Preview tab shows a ‘progress’ bar to indicate what portion of the content is visible in the editor.\nContext Tab This sub-tab is only shown for non-segmented streams, e.g. Raw Events and Raw_Reference that have an associated context data child stream. For more details of context streams, see Context This sub-tab works in exactly the same way as the Data Preview sub-tab except that it shows a different child stream.\nMeta Tab This sub-tab is only shown for non-segmented streams, e.g. Raw Events and Raw_Reference that have an associated meta data child stream. For more details of meta streams, see Meta This sub-tab works in exactly the same way as the Data Preview sub-tab except that it shows a different child stream.\nSource View The source view is accessed by clicking the View Source link on the Data Preview sub-tab or from the data() dashboard column function. Its purpose is to display the selected child stream (data, context, meta, etc) or record in the form in which it was received, i.e un-formatted.\nThe Data Preview tab shows a ‘progress’ bar to indicate what portion of the content is visible in the editor.\nIn order to navigate through the data you have three options\n Click on the ‘progress bar’ to show a porting of the data starting from the position clicked on. Page through the data using the navigation controls. Select a source range to display using the Set Source Range dialog which is accessed by clicking on the Lines or Chars links. This allows you to precisely select the range to display. You can either specify a range with a just start point or a start point and some form of size/position limit. If no limit is specified then Stroom will limit the data shown to the configured maximum (stroom.ui.source.maxCharactersPerFetch). If a range is entered that is too big to display Stroom will limit the data to its maximum.  A Note About Characters Stroom does not know the size of a stream in terms of character lines/cols, it only knows the size in bytes. Due to the way character data is encoded into bytes it is not possible to say how many characters are in a stream based on its size in bytes. Stroom can only provide an estimate based on the ratio of characters to bytes seen so far in the stream.\nData Progress Bar Stroom often handles very large streams of data and it is not feasible to show all of this data in the editor at once. Therefore Stroom will show a limited amount of the data in the editor at a time. The ‘progress’ bar at the top of the Data Preview and Source View screens shows what percentage of the data is visible in the editor and where in the stream the visible portion is located. If all of the data is visible in the editor (which includes scrolling down to see it) the bar will be green and will occupy the full width. If only some of the data is visible then the bar will be blue and the coloured part will only occupy part of the width.\nEditor Stroom uses the Ace editor for editing and viewing text, such as XSLTs, raw data, cooked events, stepping, etc.\nKeyboard shortcuts The following are some useful keyboard shortcuts in the editor:\n ctrl-z - Undo last action. ctrl-shift-z - Redo previously undone action. ctrl-/ - Toggle commenting of current line/selection. Applies when editing XML, XSLT or Javascript. alt-up/alt-down - Move line/selection up/down respectively ctrl-d - Delete current line. ctrl-f - Open find dialog. ctrl-h - Open find/replace dialog. ctrl-k - Find next match. ctrl-shift-k - Find previous match. tab - Indent selection. shift-tab - Outdent selection. ctrl-u - Make selection upper-case.  See here (external link) for more.\nVim key bindings If you are familiar with the Vi/Vim text editors then it is possible to enable Vim key bindings in Stroom. This is currently done by enabling Vim bindings in the editor context menu in each editor screen. In future versions of Stroom it will be possible to store these preferences on a per user basis.\nThe Ace editor does not support all features of Vim however the core navigation/editing key bindings are present. The key supported features of Vim are:\n Visual mode and visual block mode. Searching with / (javascript flavour regex) Search/replace with commands like :%s/foo/bar/g Incrementing/decrementing numbers with ctrl-a/ctrl-x Code (un-)folding with zo, zc, etc. Text objects, e.g. \u003e, ), ], ', \", p paragraph, w word. Repetition with the . command. Jumping to a line with :\u003cline no\u003e.  Notable features not supported by the Ace editor:\n The following text objects are not supported  b - Braces, i.e { or [. t - Tags, i.e. XML tags \u003cvalue\u003e. s - Sentence.   The g command mode command, i.e. :g/foo/d Splits  For a list of useful Vim key bindings see this cheat sheet (external link), though not all bindings will be available in Stroom’s Ace editor.\nAuto-Completion And Snippets The editor supports a number of different types of auto-completion of text. Completion suggestions are triggered by the following mechanisms:\n ctrl-space - when live auto-complete is disabled. Typing - when live auto-complete is enabled.  When completion suggestions are triggered the follow types of completion may be available depending on the text being edited.\n Local - any word/token found in the existing text. Useful if you have typed a long word and need to type it again. Keyword - A word/token that has been defined in the syntax highlighting rules for the text type, i.e. function is a keyword when editing Javascript. Snippet - A block of text that has been defined as a snippet for the editor mode (XML, Javascript, etc.).  Snippets Snippets allow you to quickly entry pre-defined blocks of common text. For example when editing an XSLT you may want to insert a call-template with parameters. To do this using snippets you can do the following:\n Type call then hit ctrl-space. In the list of options use the cursor keys to select call-template with-param then hit enter or tab to insert the snippet. The snippet will look like \u003cxsl:call-template name=\"template\"\u003e \u003cxsl:with-param name=\"param\"\u003e\u003c/xsl:with-param\u003e \u003c/xsl:call-template\u003e   The cursor will be positioned on the first tab stop (the template name) with the tab stop text selected. At this point you can type in your template name, e.g. MyTemplate, then hit tab to advance to the next tab stop (the param name) Now type the name of the param, e.g. MyParam, then hit tab to advance to the last tab stop positioned within the \u003cwith-param\u003e ready to enter the param value.  Snippets can be disabled from the list of suggestions by selecting the option in the editor context menu.\n","categories":"","description":"How to view data and edit content.\n","excerpt":"How to view data and edit content.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/editing-and-viewing/","tags":"","title":"Editing and Viewing Data"},{"body":"##Event Feeds\nIn order for Stroom to be able to handle the various data types as described in the previous section, Stroom must be told what the data is when it is received. This is achieved using Event Feeds. Each feed has a unique name within the system.\nEvents Feeds can be related to one or more Reference Feed. Reference Feeds are used to provide look up data for a translation. E.g. look up a computer name by it’s IP address.\nFeeds can also have associated context data. Context data used to provide look up information that is only applicable for the events file it relates to. E.g. if the events file is missing information relating to the computer it was generated on, and you don’t want to create separate feeds for each computer, an associated context file could be used to provide this information.\n##Feed Identifiers\nFeed identifiers must be unique within the system. Identifiers should be in the following format\n\u003cSYSTEM\u003e-\u003cENVIRONMENT\u003e-\u003cTYPE\u003e-\u003cEVENTS/REFERENCE\u003e-\u003cVERSION\u003e  If feeds in a certain site need different reference data then the site can be broken down further.\n_ may be used as a space.\n","categories":"","description":"","excerpt":"##Event Feeds\nIn order for Stroom to be able to handle the various …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/feeds/","tags":["feed"],"title":"Event Feeds"},{"body":"The following article provides examples to help data providers send data to Stroom via the HTTPS interface. The code for the clients is in the stroom-clients repository stroom-clients (external link).\nUsing Docker stroom-log-sender (external link) is a small Docker image for sending data to stroom. This is the simplest way to get data into stroom if the data provider is itself running in docker. It can also be used for sending data to stroom from data providers that are not running in Docker. stroom-log-sender makes use of the send_to_stroom.sh bash script that is described below. For details on how to use stroom-log-sender, see the Dockerhub link above.\nUNIX (using send_to_stroom.sh) send_to_stroom.sh (external link) is a small bash script to make it easier to send data to stroom. To use it download the following files using wget or similar, replacing SEND_TO_STROOM_VER with the latest released version from here (external link):\nSEND_TO_STROOM_VER=\"send-to-stroom-v2.0\" \u0026\u0026 \\ wget \"https://raw.githubusercontent.com/gchq/stroom-clients/${SEND_TO_STROOM_VER}/bash/send_to_stroom.sh\" \u0026\u0026 \\ wget \"https://raw.githubusercontent.com/gchq/stroom-clients/${SEND_TO_STROOM_VER}/bash/send_to_stroom_args.sh\" \u0026\u0026 \\ chmod u+x send_to_stroom*.sh  To see the help for send_to_stroom.sh, enter ./send_to_stroom.sh --help\nThe following is an example of using send_to_stroom.sh to send all logs in a directory:\n./send_to_stroom.sh \\ --delete-after-sending \\ --file-regex \".*/access-[0-9]+.*\\.log(\\.gz)?$\" \\ --key ./client..key \\ --cert ./client.pem.crt \\ --cacert ./ca.pem.crt \\ /some_directory/logs \\ MY_FEED \\ MY_SYSTEM \\ DEV \\ https://stroom-host/stroom/datafeed  UNIX (using curl) Curl is a standard unix tool to send data to or from a server. In the following examples -H is used to specify the header arguments required by Stroom, see Header Arguments.\nNotes:\n The @ character must be used in front of the file being posted. If it is not then curl will post the file name instead of it’s contents. The –data-binary argument must always be used even for text formats, in order to prevent data corruption by curl stripping out newlines.  Example HTTPS post without authentication: curl -k --data-binary @file.dat \"https://\u003cStroom_HOST\u003e/stroom/datafeed\" -H \"Feed:EXAMPLE_FEED\" -H \"System:EXAMPLE_SYSTEM\" -H \"Environment:EXAMPLE_ENVIRONMENT\"  In the above example -k is required to stop curl from authenticating the server. The next example must be used to supply the necessary CA to authenticate the server if this is required.\nExample HTTPS With 1 way SSL authentication: curl --cacert root_ca.crt --data-binary @file.dat \"https://\u003cStroom_HOST\u003e/stroom/datafeed\" -H \"Feed:EXAMPLE_FEED\" -H \"System:EXAMPLE_SYSTEM\" -H \"Environment:EXAMPLE_ENVIRONMENT\"  The above example verifies that the certificate presented by Stroom is signed by the CA. The CA is provided to curl using the ‘–cacert root_ca.crt’ parameter.\nFor step by step instructions for creating, configuring and testing the PKI authentication, see the SSL Guide\nExample HTTPS With 2 way SSL authentication: curl --cert example.pem --cacert root_ca.crt --data-binary @file.dat \"https://\u003cStroom_HOST\u003e/stroom/datafeed\" -H \"Feed:EXAMPLE_FEED\" -H \"System:EXAMPLE_SYSTEM\" -H \"Environment:EXAMPLE_ENVIRONMENT\"  The above example both verifies that the certificate presented by Stroom is signed by the CA and also provides a certificate to authenticate itself with Stroom. The data provider provides a certificate using the ‘–cert example.pem’ parameter.\nIf your input file is not compressed you should compress it as follows:\ngzip -c uncompressedfile.dat | curl --cert example.pem --cacert root_ca.crt --data-binary @- \"https://\u003cStroom_HOST\u003e/stroom/datafeed\" -H \"Feed:EXAMPLE_FEED\" -H \"System:EXAMPLE_SYSTEM\" -H \"Environment:EXAMPLE_ENVIRONMENT\" -H \"Compression:Gzip\"  When delivering data from a RHEL4 host, an additional header argument must be added to specify the FQDN of the host:\n-H \"Hostname:host.being.audited\"  The hostname being sent as a header argument may be resolved upon execution using the command hostname -f.\nSSL Notes To create a .pem format key simply append the private key and certifcate.\ncat \u003cNAME\u003e.key \u003e\u003e \u003cNAME\u003e.pem cat \u003cNAME\u003e.crt \u003e\u003e \u003cNAME\u003e.pem  To remove the pass phrase from a openssl private key use.\nopenssl rsa -in server.key -out server-clear.key  The send-logs.sh script assumes the period start and end times are embedded in the file name (e.g. log_2010-01-01T12:00:00.000Z_2010-01-02T12:00:00.000Z.log). The certificates will need to be added to the script as above.\nWindows Using wget There is a version of wget for windows\n Use –post-file argument to supply the data Use –certificate and –certificate-type arguments to specify your client certificate Use –header argument to inform Stroom which feed and environment your data relates to  Using curl There is a version of curl for Windows\nWindows 10 is the latest desktop OS offering from Microsoft. From Windows 10 build 17063 and later, curl is now natively included - you can execute it directly from Cmd.exe or PowerShell.exe. Curl.exe is located at c:\\windows\\system32 (which is included in the standard PATH environment variable) - all you need to do is run Command Prompt with administrative rights and you can use Curl. You can execute it directly from Cmd.exe or PowerShell.exe. For older versions of Windows, the cURL project has Windows binaries.\ncurl -s -k --data-binary @file.dat \"https://stroomp.strmdev00.org/stroom/datafeed\" -H\"Feed:TEST-FEED-V1_0\" -H\"System:EXAMPLE_SYSTEM\" -H\"Environment:EXAMPLE_ENVIRONMENT\"  Windows curl CLI    Using VBScript extract-data.vbs uses wevtutil.exe to extract Security event information from the windows event log. This script has been tested on Windows 2008.\nThis script is designed to run periodically (say every 10 minutes). The first time the script is run it stores the current time in UTC format in the registry. Subsequent calls then extract event information from the last run time to the new current time. The events are stored in a zip file with the period dates embedded.\nThe script requires a working directory used as a buffer for the zip files. This can be set at the start of the script otherwise it will default to the working directory.\nThe send-data.vbs script is designed to run periodically (say every 10 minutes). The script will scan for zip files and send them to Stroom.\nThe script details several parameters that require setting per environment. Among these are the working directory that the zip files are stored in, the feed name and the URL of Stroom.\nSSL To send data over SSL (https) you must import a client certificate in p12 format into windows.\nTo convert a certificate (.crt) and private key (.key) into a p12 format use the following command:\nopenssl pkcs12 -export -in \u003cNAME\u003e.crt -inkey \u003cNAME\u003e.key -out \u003cNAME\u003e.p12 -name \"\u003cNAME\u003e\"  Once in p12 format use the windows certificate wizard to import the public private key.\nThe send-data-tree.vbs script works through a directory for different feed types.\nUsing Java The stroom-java-client provides an example Java client that can:\n Read a zip, gzip or uncompressed an input file. Perform a HTTP post of data with zip, gzip or uncompressed compression. Pass down arguments on the command line as HTTP request arguments. Supports HTTP and HTTPS with 1 or 2 way authentication.  (N.B. arguments must be in lower case).\nTo use the example client first compile the Java code:\njavac DataFeedClient.java  Example HTTP Post: java -classpath . DataFeedClient inputfile=datafeed url=http://\u003cStroom_HOST\u003e/stroom/datafeed system=EXAMPLE-SYSTEM environment=DEV feed=EXAMPLE-FEED  Example HTTPS With 1 way SSL authentication: java -classpath . -Djavax.net.ssl.trustStore=ca.jks -Djavax.net.ssl.trustStorePassword=capass DataFeedClient inputfile=datafeed url=https://\u003cStroom_HOST\u003e/stroom/datafeed system=EXAMPLE-SYSTEM environment=DEV feed=EXAMPLE-FEED  Example HTTPS With 2 way SSL authentication: java -classpath . -Djavax.net.ssl.trustStore=ca.jks -Djavax.net.ssl.trustStorePassword=capass -Djavax.net.ssl.keyStore=example.jks -Djavax.net.ssl.keyStorePassword=\u003cPASSWORD\u003e DataFeedClient inputfile=datafeed url=https://\u003cStroom_HOST\u003e/stroom/datafeed system=EXAMPLE-SYSTEM environment=DEV feed=EXAMPLE-FEED  Using C*#* The StroomCSharpClient is a C# port of the Java client and behaves in the same way. Note that this is just an example, not a fully functional client.\n","categories":"","description":"","excerpt":"The following article provides examples to help data providers send …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/sending-data/example-clients/","tags":"","title":"Example Clients"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/expression/","tags":"","title":"expression"},{"body":"Referencing matches in expressions is done using $. In addition to this a match group number may be added to just retrieve part of the expression match. The applicability and effect that this has depends on the type of expression used.\nReferences to \u003csplit\u003e Match Groups In the following example a line matched by a parent \u003csplit\u003e expression is referenced by a child \u003cdata\u003e element.\n\u003csplit delimiter=\"\\n\" \u003e \u003cdata name=\"line\" value=\"$\"/\u003e \u003c/split\u003e  A \u003csplit\u003e element matches content up to and including the specified delimiter, so the above reference would output the entire line plus the delimiter. However there are various match groups that can be used by child \u003cgroup\u003e and \u003cdata\u003e elements to reference sections of the matched content.\nTo illustrate the content provided by each match group, take the following example:\n\"This is some text\\, that we wish to match\", \"This is the next text\"  And the following \u003csplit\u003e element:\n\u003csplit delimiter=\",\" escape=\"\\\"\u003e  The match groups are as follows:\n $ or $0: The entire content that is matched including the specified delimiter at the end  \"This is some text\\, that we wish to match\",\n $1: The content up to the specified delimiter at the end  \"This is some text\\, that we wish to match\"\n $2: The content up to the specified delimiter at the end and filtered to remove escape characters (more expensive than $1)  \"This is some text, that we wish to match\"\nIn addition to this behaviour match groups 1 and 2 will omit outermost whitespace and container characters if specified, e.g. with the following content:\n\" This is some text\\, that we wish to match \" , \"This is the next text\"  And the following \u003csplit\u003e element:\n\u003csplit delimiter=\",\" escape=\"\\\" containerStart=\"\u0026quot\" containerEnd=\"\u0026quot\"\u003e  The match groups are as follows:\n $ or $0: The entire content that is matched including the specified delimiter at the end  \" This is some text\\, that we wish to match \" ,\n $1: The content up to the specified delimiter at the end and strips outer containers.  This is some text\\, that we wish to match\n $2: The content up to the specified delimiter at the end and strips outer containers and filtered to remove escape characters (more computationally expensive than $1)  This is some text, that we wish to match\nReferences to \u003cregex\u003e Match Groups Like the \u003csplit\u003e element various match groups can be referenced in a \u003cregex\u003e expression to retrieve portions of matched content. This content can be used as values for \u003cgroup\u003e and \u003cdata\u003e elements.\nGiven the following input:\nip=1.1.1.1 user=user1  And the following \u003cregex\u003e element:\n\u003cregex pattern=\"ip=([^ ]+) user=([^ ]+)\"\u003e  The match groups are as follows:\n $ or $0: The entire content that is matched by the expression  ip=1.1.1.1 user=user1\n $1: The content of the first match group  1.1.1.1\n $2: The content of the second match group  user1\nMatch group numbers in regular expressions are determined by the order that their open bracket appears in the expression.\nReferences to \u003cany\u003e Match Groups The \u003cany\u003e element does not have any match groups and always returns the entire content that was passed to it when referenced with $.\n","categories":"","description":"","excerpt":"Referencing matches in expressions is done using $. In addition to …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-splitter/match-reference/3-1-expression-match-references/","tags":"","title":"Expression match references"},{"body":"Expressions match some data supplied by a parent content provider. The content matched by an expression depends on the type of expression and how it is configured.\nThe \u003csplit\u003e, \u003cregex\u003e and \u003call\u003e elements are all expressions and match content as described below.\nThe \u003csplit\u003e element The \u003csplit\u003e element directs Data Splitter to break up content using a specified character sequence as a delimiter. In addition to this it is possible to specify characters that are used to escape the delimiter as well as characters that contain or “quote” a value that may include the delimiter sequence but allow it to be ignored.\nAttributes The \u003csplit\u003e element has the following attributes:\n id delimiter escape containerStart containerEnd maxMatch minMatch onlyMatch  id Optional attribute used to debug the location of expressions causing errors, see id.\ndelimiter A required attribute used to specify the character string that will be used as a delimiter to split the supplied content unless it is preceded by an escape character or within a container if specified. Several of the previous examples use this attribute.\nescape An optional attribute used to specify a character sequence that is used to escape the delimiter. Many delimited text formats have an escape character that is used to tell any parser that the following delimiter should be ignored, e.g. often a character such as ‘' is used to escape the character that follows it so that it is not treated as a delimiter. When specified this escape sequence also applies to any container characters that may be specified.\ncontainerStart An optional attribute used to specify a character sequence that will make this expression ignore the presence of delimiters until an end container is found. If the character is preceded by the specified escape sequence then this container sequence will be ignored and the expression will continue matching characters up to a delimiter.\nIf used containerEnd must also be specified. If the container characters are to be ignored from the match then match group 1 must be used instead of 0.\ncontainerEnd An optional attribute used to specify a character sequence that will make this expression stop ignoring the presence of delimiters if it believes it is currently in a container. If the character is preceded by the specified escape sequence then this container sequence will be ignored and the expression will continue matching characters while ignoring the presence of any delimiter.\nIf used containerStart must also be specified. If the container characters are to be ignored from the match then match group 1 must be used instead of 0.\nmaxMatch An optional attribute used to specify the maximum number of times this expression is allowed to match the supplied content. If you do not supply this attribute then the Data Splitter will keep matching the supplied content until it reaches the end. If specified Data Splitter will stop matching the supplied content when it has matched it the specified number of times.\nThis attribute is used in the ‘CSV with header line’ example to ensure that only the first line is treated as a header line.\nminMatch An optional attribute used to specify the minimum number of times this expression should match the supplied content. If you do not supply this attribute then Data Splitter will not enforce that the expression matches the supplied content. If specified Data Splitter will generate an error if the expression does not match the supplied content at least as many times as specified.\nUnlike maxMatch, minMatch does not control the matching process but instead controls the production of error messages generated if the parser is not seeing the expected input.\nonlyMatch Optional attribute to use this expression only for specific instances of a match of the parent expression, e.g. on the 4th, 5th and 8th matches of the parent expression specified by ‘4,5,8’. This is used when this expression should only be used to subdivide content from certain parent matches.\nThe \u003cregex\u003e element The \u003cregex\u003e element directs Data Splitter to match content using the specified regular expression pattern. In addition to this the same match control attributes that are available on the \u003csplit\u003e element are also present as well as attributes to alter the way the pattern works.\nAttributes The \u003cregex\u003e element has the following attributes:\n id pattern dotAll caseInsensitive maxMatch minMatch onlyMatch advance  id Optional attribute used to debug the location of expressions causing errors, see id.\npattern This is a required attribute used to specify a regular expression to use to match on the supplied content. The pattern is used to match the content multiple times until the end of the content is reached while the maxMatch and onlyMatch conditions are satisfied.\ndotAll An optional attribute used to specify if the use of ‘.’ in the supplied pattern matches all characters including new lines. If ‘true’ ‘.’ will match all characters including new lines, if ‘false’ it will only match up to a new line. If this attribute is not specified it defaults to ‘false’ and will only match up to a new line.\nThis attribute is used in many of the multiline examples above.\ncaseInsensitive An optional attribute used to specify if the supplied pattern should match content in a case insensitive way. If ‘true’ the expression will match content in a case insensitive manner, if ‘false’ it will match the content in a case sensitive manner. If this attribute is not specified it defaults to ‘false’ and will match the content in a case sensitive manner.\nmaxMatch This is used in the same way it is on the \u003csplit\u003e element, see maxMatch.\nminMatch This is used in the same way it is on the \u003csplit\u003e element, see minMatch.\nonlyMatch This is used in the same way it is on the \u003csplit\u003e element, see onlyMatch.\nadvance After an expression has matched content in the buffer, the buffer start position is advanced so that it moves to the end of the entire match. This means that subsequent expressions operating on the content buffer will not see the previously matched content again. This is normally required behaviour, but in some cases some of the content from a match is still required for subsequent matches. Take the following example of name value pairs:\nname1=some value 1 name2=some value 2 name3=some value 3  The first name value pair could be matched with the following expression:\n\u003cregex pattern=\"([^=]+)=(.+?) [^= ]+=\"\u003e  The above expression would match as follows:\nname1=some value 1 name2=some value 2 name3=some value 3  In this example we have had to do a reluctant match to extract the value in group 2 and not include the subsequent name. Because the reluctant match requires us to specify what we are reluctantly matching up to, we have had to include an expression after it that matches the next name.\nBy default the parser will move the character buffer to the end of the entire match so the next expression will be presented with the following:\nsome value 2 name3=some value 3  Therefore name2 will have been lost from the content buffer and will not be available for matching.\nThis behaviour can be altered by telling the expression how far to advance the character buffer after matching. This is done with the advance attribute and is used to specify the match group whose end position should be treated as the point the content buffer should advance to, e.g.\n\u003cregex pattern=\"([^=]+)=(.+?) [^= ]+=\" advance=\"2\"\u003e  In this example the content buffer will only advance to the end of match group 2 and subsequent expressions will be presented with the following content:\nname2=some value 2 name3=some value 3  Therefore name2 will still be available in the content buffer.\nIt is likely that the advance feature will only be useful in cases where a reluctant match is performed. Reluctant matches are discouraged for performance reasons so this feature should rarely be used. A better way to tackle the above example would be to present the content in reverse, however this is only possible if the expression is within a group, i.e. is not a root expression. There may also be more complex cases where reversal is not an option and the use of a reluctant match is the only option.\nThe \u003call\u003e element The \u003call\u003e element matches the entire content of the parent group and makes it available to child groups or \u003cdata\u003e elements. The purpose of \u003call\u003e is to act as a catch all expression to deal with content that is not handled by a more specific expression, e.g. to output some other unknown, unrecognised or unexpected data.\n\u003cgroup\u003e \u003cregex pattern=\"^\\s*([^=]+)=([^=]+)\\s*\"\u003e \u003cdata name=\"$1\" value=\"$2\" /\u003e \u003c/regex\u003e \u003c!-- Output unexpected data --\u003e \u003call\u003e \u003cdata name=\"unknown\" value=\"$\" /\u003e \u003c/all\u003e \u003c/group\u003e  The \u003call\u003e element provides the same functionality as using .* as a pattern in a \u003cregex\u003e element and where dotAll is set to true, e.g. \u003cregex pattern=\".*\" dotAll=\"true\"\u003e. However it performs much faster as it doesn’t require pattern matching logic and is therefore always preferred.\nAttributes The \u003call\u003e element has the following attributes:\n id  id Optional attribute used to debug the location of expressions causing errors, see id.\n","categories":"","description":"","excerpt":"Expressions match some data supplied by a parent content provider. The …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-splitter/element-reference/2-2-expressions/","tags":"","title":"Expressions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/feed/","tags":"","title":"feed"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/file/","tags":"","title":"file"},{"body":"When outputting files with Stroom, the output file names and paths can include various substitution variables to form the file and path names.\nContext Variables The following replacement variables are specific to the current processing context.\n ${feed} - The name of the feed that the stream being processed belongs to ${pipeline} - The name of the pipeline that is producing output ${sourceId} - The id of the input data being processed ${partNo} - The part number of the input data being processed where data is in aggregated batches ${searchId} - The id of the batch search being performed. This is only available during a batch search ${node} - The name of the node producing the output  Time Variables The following replacement variables can be used to include aspects of the current time in UTC.\n ${year} - Year in 4 digit form, e.g. 2000 ${month} - Month of the year padded to 2 digits ${day} - Day of the month padded to 2 digits ${hour} - Hour padded to 2 digits using 24 hour clock, e.g. 22 ${minute} - Minute padded to 2 digits ${second} - Second padded to 2 digits ${millis} - Milliseconds padded to 3 digits ${ms} - Milliseconds since the epoch  System (Environment) Variables System variables (environment variables) can also be used, e.g. ${TMP}.\nFile Name References rolledFileName in RollingFileAppender can use references to the fileName to incorporate parts of the non rolled file name.\n ${fileName} - The complete file name ${fileStem} - Part of the file name before the file extension, i.e. everything before the last ‘.’ ${fileExtension} - The extension part of the file name, i.e. everything after the last ‘.’  Other Variables  ${uuid} - A randomly generated UUID to guarantee unique file names  ","categories":"","description":"","excerpt":"When outputting files with Stroom, the output file names and paths can …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/pipelines/file-output/","tags":["file"],"title":"File Output"},{"body":" Version Information: Created with Stroom v7.0\nLast Updated: 01 Jun 2020\n Explorer Tree Explorer Tree    The Explorer Tree in stroom is the primary means of finding user created content, for example Feeds, XSLTs, Pipelines, etc.\nBranches of the Explorer Tree can be expanded and collapsed to reveal/hide the content at different levels.\nFiltering by Type The Explorer Tree can be filtered by the type of content, e.g. to display only Feeds, or only Feeds and XSLTs. This is done by clicking the filter icon . The following is an example of filtering by Feeds and XSLTs.\nExplorer Tree Type Filtering    Clicking All/None toggles between all types selected and no types selected.\nFiltering by type can also be achieved using the Quick Filter by entering the type name (or a partial form of the type name), prefixed with type:. I.e:\ntype:feed  For example:\nExplorer Tree Type Filtering     NOTE: If both the type picker and the Quick Filter are used to filter on type then the two filters will be combined as an AND.\n Filtering by Name The Explorer Tree can be filtered by the name of the entity. This is done by entering some text in the Quick Filter field. The tree will then be updated to only show entities matching the Quick Filter. The way the matching works for entity names is described in Common Fuzzy Matching\nFiltering by UUID What is a UUID? The Explorer Tree can be filtered by the UUID of the entity. The UUID Universally unique identifier (external link) is an identifier that can be relied on to be unique both within the system and universally across all other systems. Stroom uses UUIDs as the primary identifier for all content (Feeds, XSLTs, Pipelines, etc.) created in Stroom. An entity’s UUID is generated randomly by Stroom upon creation and is fixed for the life of that entity.\nWhen an entity is exported it is exported with its UUID and if it is then imported into another instance of Stroom the same UUID will be used. The name of an entity can be changed within Stroom but its UUID remains un-changed.\nWith the exception of Feeds, Stroom allows multiple entities to have the same name. This is because entities may exist that a user does not have access to see so restricting their choice of names based on existing invisible entities would be confusing. Where there are multiple entities with the same name the UUID can be used to distinguish between them.\nThe UUID of an entity can be viewed using the context menu for the entity. The context menu is accessed by right-clicking on the entity.\nEntity Context Menu    Clicking Info displays the entities UUID.\nEntity Info    The UUID can be copied by selecting it and then pressing ctrl-c.\nUUID Quick Filter Matching In the Explorer Tree Quick Filter you can filter by UUIDs in the following ways:\nTo show the entity matching a UUID, enter the full UUID value (with dashes) prefixed with the field qualifier uuid, e.g. uuid:a95e5c59-2a3a-4f14-9b26-2911c6043028.\nTo filter on part of a UUID you can do uuid:/2a3a to find an entity whose UUID contains 2a3a or uuid:^2a3a to find an entity whose UUID starts with 2a3a.\nQuick Filters Quick Filter controls are used in a number of screens in Stroom. The most prominent use of a Quick Filter is in the Explorer Tree as described above. Quick filters allow for quick searching of a data set or a list of items using a text based query language. The basis of the query language is described in Common Fuzzy Matching.\nA number of the Quick Filters are used for filter tables of data that have a number of fields. The quick filter query language supports matching in specified fields. Each Quick Filter will have a number of named fields that it can filter on. The field to match on is specified by prefixing the match term with the name of the field followed by a :, i.e. type:. Multiple field matches can be used, each separate by a space. E.g:\nname:^xml name:events$ type:feed  In the above example the filter will match on items with a name beginning xml, a name ending events and a type partially matching feed.\nAll the match terms are combined together with an AND operator. The same field can be used multiple times in the match. The list of filterable fields and their qualifier names (sometimes a shortened form) are listed by clicking on the help icon .\nOne or more of the fields will be defined as default fields. This means the if no qualifier is entered the match will be applied to all default fields using an OR operator. Sometimes all fields may be considered default which means a match term will be tested against all fields and an item will be included in the results if one or more of those fields match.\nFor example if the Quick Filter has fields Name, Type and Status, of which Name and Type are default:\nfeed status:ok  The above would match items where the Name OR Type fields match feed AND the Status field matches ok.\nMatch Negation Each match item can be negated using the ! prefix. This is also described in Common Fuzzy Matching. The prefix is applied after the field qualifier. E.g:\nname:xml source:!/default  In the above example it would match on items where the Name field matched xml and the Source field does NOT match the regex pattern default.\nSpaces and Quotes If your match term contains a space then you can surround the match term with double quotes. Also if your match term contains a double quote you can escape it with a \\ character. The following would be valid for example.\n\"name:csv splitter\" \"default field match\" \"symbol:\\\"\"  Multiple Terms If multiple terms are provided for the same field then an AND is used to combine them. This can be useful where you are not sure of the order of words within the items being filtered.\nFor example:\nUser input: spain plain rain\nWill match:\nThe rain in spain stays mainly in the plain ^^^^ ^^^^^ ^^^^^ rainspainplain ^^^^^^^^^^^^^^ spain plain rain ^^^^^ ^^^^^ ^^^^ raining spain plain ^^^^^^^ ^^^^^ ^^^^^  Won’t match: sprain, rain, spain\nOR Logic There is no support for combining terms with an OR. However you can acheive this using a sinlge regular expression term. For example\nUser input: status:/(disabled|locked)\nWill match:\nLocked ^^^^^^ Disabled ^^^^^^^^  Won’t match: A MAN, HUMAN\nSuggestion Input Fields Stroom uses a number of suggestion input fields, such as when selecting Feeds, Pipelines, types, status values, etc. in the pipeline processor filter screen.\nFeed Input Suggestions    These fields will typically display the full list of values or a truncated list where the total number of value is too large. Entering text in one of these fields will use the fuzzy matching algorithm to partially/fully match on values. See CommonFuzzy Matching below for details of how the matching works.\nCommon Fuzzy Matching A common fuzzy matching mechanism is used in a number of places in Stroom. It is used for partially matching the user input to a list of a list of possible values.\nIn some instances, the list of matched items will be truncated to a more manageable size with the expectation that the filter will be refined.\nThe fuzzy matching employs a number of approaches that are attempted in the following order:\nNOTE: In the following examples the ^ character is used to indicate which characters have been matched.\nNo Input If no input is provided all items will match.\nContains (Default) If no prefixes or suffixes are used then all characters in the user input will need to be contained as a whole somewhere within the string being tested. The matching is case insensitive.\nUser input: bad\nWill match:\nbad angry dog ^^^ BAD ^^^ very badly ^^^ Very bad ^^^  Won’t match: dab, ba d, ba\nCharacters Anywhere Matching If the user input is prefixed with a ~ (tilde) character then characters anywher matching will be employed. The matching is case insensitive.\nUser input: bad\nWill match:\nBig Angry Dog ^ ^ ^ bad angry dog ^^^ BAD ^^^ badly ^^^ Very bad ^^^ b a d ^ ^ ^ bbaadd ^ ^ ^  Won’t match: dab, ba\nWord Boundary Matching If the user input is prefixed with a ? character then word boundary matching will be employed. This approache uses upper case letters to denote the start of a word. If you know the some or all of words in the item you are looking for, and their order, then condensing those words down to their first letters (capitalised) makes this a more targeted way to find what you want than the characters anywhere matching above. Words can either be separated by characters like _- ()[]., or be distinguished with lowerCamelCase or upperCamelCase format. An upper case letter in the input denotes the beginning of a word and any subsequent lower case characters are treated as contiguously following the character at the start of the word.\nUser input: ?OTheiMa\nWill match:\nthe cat sat on their mat ^ ^^^^ ^^ ON THEIR MAT ^ ^^^^ ^^ Of their magic ^ ^^^^ ^^ o thei ma ^ ^^^^ ^^ onTheirMat ^ ^^^^ ^^ OnTheirMat ^ ^^^^ ^^  Won’t match: On the mat, the cat sat on there mat, On their moat\nUser input: ?MFN\nWill match:\nMY_FEED_NAME ^ ^ ^ MY FEED NAME ^ ^ ^ MY_FEED_OTHER_NAME ^ ^ ^ THIS_IS_MY_FEED_NAME_TOO ^ ^ ^ myFeedName ^ ^ ^ MyFeedName ^ ^ ^ also-my-feed-name ^ ^ ^ MFN ^^^ stroom.something.somethingElse.maxFileNumber ^ ^ ^  Won’t match: myfeedname, MY FEEDNAME\nRegular Expression Matching If the user input is prefixed with a / character then the remaining user input is treated as a Java syntax regular expression. An string will be considered a match if any part of it matches the regular expression pattern. The regular expression operates in case insensitive mode. For more details on the syntax of java regular expressions see this internet link https://docs.oracle.com/en/java/javase/15/docs/api/java.base/java/util/regex/Pattern.html.\nUser input: /(^|wo)man\nWill match:\nMAN ^^^ A WOMAN ^^^^^ Manly ^^^ Womanly ^^^^^  Won’t match: A MAN, HUMAN\nExact Match If the user input is prefixed with a ^ character and suffixed with a $ character then a case-insensitive exact match will be used. E.g:\nUser input: ^xml-events$\nWill match:\nxml-events ^^^^^^^^^^ XML-EVENTS ^^^^^^^^^^  Won’t match: xslt-events, XML EVENTS, SOME-XML-EVENTS, AN-XML-EVENTS-PIPELINE\nNote: Despite the similarity in syntax, this is NOT regular expression matching.\nStarts With If the user input is prefixed with a ^ character then a case-insensitive starts with match will be used. E.g:\nUser input: ^events\nWill match:\nevents ^^^^^^ EVENTS_FEED ^^^^^^ events-xslt ^^^^^^  Won’t match: xslt-events, JSON_EVENTS\nNote: Despite the similarity in syntax, this is NOT regular expression matching.\nEnds With If the user input is suffixed with a $ character then a case-insensitive ends with match will be used. E.g:\nUser input: events$\nWill match:\nevents ^^^^^^ xslt-events ^^^^^^ JSON_EVENTS ^^^^^^  Won’t match: EVENTS_FEED, events-xslt\nNote: Despite the similarity in syntax, this is NOT regular expression matching.\nWild-Carded Case Sensitive Exact Matching If one or more * characters are found in the user input then this form of matching will be used.\nThis form of matching is to support those fields that accept wild-carded values, e.g. a whild-carded feed name expression term. In this instance you are NOT picking a value from the suggestion list but entering a wild-carded value that will be evaluated when the expression/filter is actually used. The user may want an expression term that matches on all feeds starting with XML_, in which case they would enter XML_*. To give an indication of what it would match on if the list of feeds remains the same, the list of suggested items will reflect the wild-carded input.\nUser input: XML_*\nWill match:\nXML_ ^^^^ XML_EVENTS ^^^^  Won’t match: BAD_XML_EVENTS, XML-EVENTS, xml_events\nUser input: XML_*EVENTS*\nWill match:\nXML_EVENTS ^^^^^^^^^^ XML_SEC_EVENTS ^^^^ ^^^^^^ XML_SEC_EVENTS_FEED ^^^^ ^^^^^^  Won’t match: BAD_XML_EVENTS, xml_events\nMatch Negation A match can be negated, ie. the NOT operator using the prefix !. This prefix can be applied before all the match prefixes listed above. E.g:\n!/(error|warn)  In the above example it will match everything except those matched by the regex pattern (error|warn).\n","categories":"","description":"How to find things in Stroom, for example content, simple string values, etc.\n","excerpt":"How to find things in Stroom, for example content, simple string …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/finding-things/","tags":"","title":"Finding Things"},{"body":"The following data must be passed in as HTTP header arguments when sending files to Stroom via HTTP POST. These arguments are case insensitive.\n  System - The name by which the system is known within the organisation, e.g. PAYROLL_SYSTEM. This could be the name of a project/service or capability.\n  Environment - A means to identify the deployed instance of a system. This may indicate the deployment status, e.g. DEV, REF, LIVE, OPS, etc., and/or the location where the instance is deployed. An environment may be a combination of these attributes separated with an underscore.\n  Feed - The name of the feed this data relates to. This is mandatory and must match a feed defined within Stroom in order for Stroom to accept the data and know what to do with it.\n  Compression - This token is optionally used when the POST payload is compressed with either gzip of zip compression. Value of ZIP and GZIP are valid. Note: The Compression token MUST not be used in conjunction with the standard HTTP header token Content-Encoding otherwise stroom will be unable to un-compress the data. Use either Compression:GZIP or Content-Encoding:gzip, not both. Using Compression is preferred.\n  EffectiveTime - This is only applicable to reference data. It is used to indicate the point in time that the reference data is applicable to, i.e. all event data that uses the reference data that is created after the effective time will use the reference data until a new reference data item arrives with a later effective time. Note: This argument must be in ISO 8601 date time format, i.e: yyyy-MM-ddTHH:mm:ss.sssZ.\n  Example header arguments for a feed called MY_SYSTEM-EVENTS from system MY_SYSTEM and environment OPS\nSystem:MY_SYSTEM Environment:OPS Feed:MY_SYSTEM-EVENTS  The post payload must contain the events file. If the compression format is ZIP the payload must contain ZIP entries with the events files and optional context files ending in .ctx. Further details of supported payload formats can be found here.\n","categories":"","description":"The various HTTP headers that can be sent with data.\n","excerpt":"The various HTTP headers that can be sent with data.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/sending-data/header-arguments/","tags":["headers","http"],"title":"Header Arguments"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/headers/","tags":"","title":"headers"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/http/","tags":"","title":"http"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/java/","tags":"","title":"java"},{"body":"There are many times when you may wish to create a Java keystore from certificates and keys and vice versa. This guide aims to explain how this can be done.\nImport If you need to create a Java keystore from a .crt and .key then this is how to do it.\nConvert your keys to der format openssl x509 -in \u003cYOUR KEY\u003e.crt -inform PEM -out \u003cYOUR KEY\u003e.crt.der -outform DER openssl pkcs8 -topk8 -nocrypt -in \u003cYOUR KEY\u003e.key -inform PEM -out \u003cYOUR KEY\u003e.key.der -outform DER  ImportKey Use the ImportKey class in the stroom-java-client library to import keys.\nFor example:\njava ImportKey keystore=\u003cYOUR KEY\u003e.jks keypass=\u003cYOUR PASSWORD\u003e alias=\u003cYOUR KEY\u003e keyfile=\u003cYOUR KEY\u003e.key.der certfile=\u003cYOUR KEY\u003e.crt.der keytool -import -noprompt -alias CA -file \u003cCA CERT\u003e.crt -keystore ca.jks -storepass ca  Export ExportKey Use the ExportKey class in the stroom-java-client library to export keys. If you would like to use curl or similar application but only have keys contained within a Java keystore then they can be exported.\nFor example:\njava ExportKey keystore=\u003cYOUR KEY\u003e.jks keypass=\u003cYOUR PASSWORD\u003e alias=\u003cYOUR KEY\u003e  This will print both the key and certificate to standard out. This can then be copied into a PEM file for use with cURL or other similar application.\n","categories":"","description":"How to create java key/trust stores for use with Java client applications.\n","excerpt":"How to create java key/trust stores for use with Java client …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/sending-data/java-keystores/","tags":["keystore","truststore","java"],"title":"Java Keystores"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/job/","tags":"","title":"job"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/keystore/","tags":"","title":"keystore"},{"body":"Annotation A helper function to make forming links to annotations easier than using Link. The Annotation function allows you to create a link to open the Annotation editor, either to view an existing annotation or to begin creating one with pre-populated values.\nannotation(text, annotationId) annotation(text, annotationId, [streamId, eventId, title, subject, status, assignedTo, comment])  If you provide just the text and an annotationId then it will produce a link that opens an existing annotation with the supplied ID in the Annotation Edit dialog.\nExample\nannotation('Open annotation', ${annotation:Id}) \u003e [Open annotation](?annotationId=1234){annotation} annotation('Create annotation', '', ${StreamId}, ${EventId}) \u003e [Create annotation](?annotationId=\u0026streamId=1234\u0026eventId=45){annotation} annotation('Escalate', '', ${StreamId}, ${EventId}, 'Escalation', 'Triage required') \u003e [Escalate](?annotationId=\u0026streamId=1234\u0026eventId=45\u0026title=Escalation\u0026subject=Triage%20required){annotation}  If you don’t supply an annotationId then the link will open the Annotation Edit dialog pre-populated with the optional arguments so that an annotation can be created. If the annotationId is not provided then you must provide a streamId and an eventId. If you don’t need to pre-populate a value then you can use '' or null() instead.\nExample\nannotation('Create suspect event annotation', null(), 123, 456, 'Suspect Event', null(), 'assigned', 'jbloggs') \u003e [Create suspect event annotation](?streamId=123\u0026eventId=456\u0026title=Suspect%20Event\u0026assignedTo=jbloggs){annotation}  Dashboard A helper function to make forming links to dashboards easier than using Link.\ndashboard(text, uuid) dashboard(text, uuid, params)  Example\ndashboard('Click Here','e177cf16-da6c-4c7d-a19c-09a201f5a2da') \u003e [Click Here](?uuid=e177cf16-da6c-4c7d-a19c-09a201f5a2da){dashboard} dashboard('Click Here','e177cf16-da6c-4c7d-a19c-09a201f5a2da', 'userId=user1') \u003e [Click Here](?uuid=e177cf16-da6c-4c7d-a19c-09a201f5a2da\u0026params=userId%3Duser1){dashboard}  Data Creates a link to open a source for data for viewing.\ndata(text, id, partNo, [recordNo, lineFrom, colFrom, lineTo, colTo, viewType, displayType])  viewType can be one of:\n preview : Display the data as a formatted preview of a limited portion of the data. source : Display the un-formatted data in its original form with the ability to navigate around all of the data source.  displayType can be one of:\n dialog : Open as a modal popup dialog. tab : Open as a top level tab within the Stroom browser tab.  Example\ndata('Quick View', ${StreamId}, 1) \u003e [Quick View]?id=1234\u0026\u0026partNo=1)  Link Create a string that represents a hyperlink for display in a dashboard table.\nlink(url) link(text, url) link(text, url, type)  Example\nlink('http://www.somehost.com/somepath') \u003e [http://www.somehost.com/somepath](http://www.somehost.com/somepath) link('Click Here','http://www.somehost.com/somepath') \u003e [Click Here](http://www.somehost.com/somepath) link('Click Here','http://www.somehost.com/somepath', 'dialog') \u003e [Click Here](http://www.somehost.com/somepath){dialog} link('Click Here','http://www.somehost.com/somepath', 'dialog|Dialog Title') \u003e [Click Here](http://www.somehost.com/somepath){dialog|Dialog Title}  Type can be one of:\n dialog : Display the content of the link URL within a stroom popup dialog. tab : Display the content of the link URL within a stroom tab. browser : Display the content of the link URL within a new browser tab. dashboard : Used to launch a stroom dashboard internally with parameters in the URL.  If you wish to override the default title or URL of the target link in either a tab or dialog you can. Both dialog and tab types allow titles to be specified after a |, e.g. dialog|My Title.\nStepping Open the Stepping tab for the requested data source.\nstepping(text, id) stepping(text, id, partNo) stepping(text, id, partNo, recordNo)  Example\nstepping('Click here to step',${StreamId}) \u003e [Click here to step](?id=1)  ","categories":"","description":"Functions for linking to other screens in Stroom and/or to particular sets of data.\n","excerpt":"Functions for linking to other screens in Stroom and/or to particular …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/expressions/link/","tags":"","title":"Link Functions"},{"body":"Equals Evaluates if arg1 is equal to arg2\narg1 = arg2 equals(arg1, arg2)  Examples\n'foo' = 'bar' \u003e false 'foo' = 'foo' \u003e true 51 = 50 \u003e false 50 = 50 \u003e true equals('foo', 'bar') \u003e false equals('foo', 'foo') \u003e true equals(51, 50) \u003e false equals(50, 50) \u003e true  Note that equals cannot be applied to null and error values, e.g. x=null() or x=err(). The isNull() and isError() functions must be used instead.\nGreater Than Evaluates if arg1 is greater than to arg2\narg1 \u003e arg2 greaterThan(arg1, arg2)  Examples\n51 \u003e 50 \u003e true 50 \u003e 50 \u003e false 49 \u003e 50 \u003e false greaterThan(51, 50) \u003e true greaterThan(50, 50) \u003e false greaterThan(49, 50) \u003e false  Greater Than or Equal To Evaluates if arg1 is greater than or equal to arg2\narg1 \u003e= arg2 greaterThanOrEqualTo(arg1, arg2)  Examples\n51 \u003e= 50 \u003e true 50 \u003e= 50 \u003e true 49 \u003e= 50 \u003e false greaterThanOrEqualTo(51, 50) \u003e true greaterThanOrEqualTo(50, 50) \u003e true greaterThanOrEqualTo(49, 50) \u003e false  If Evaluates the supplied boolean condition and returns one value if true or another if false\nif(expression, trueReturnValue, falseReturnValue)  Examples\nif(5 \u003c 10, 'foo', 'bar') \u003e 'foo' if(5 \u003e 10, 'foo', 'bar') \u003e 'bar' if(isNull(null()), 'foo', 'bar') \u003e 'foo'  Less Than Evaluates if arg1 is less than to arg2\narg1 \u003c arg2 lessThan(arg1, arg2)  Examples\n51 \u003c 50 \u003e false 50 \u003c 50 \u003e false 49 \u003c 50 \u003e true lessThan(51, 50) \u003e false lessThan(50, 50) \u003e false lessThan(49, 50) \u003e true  Less Than or Equal To Evaluates if arg1 is less than or equal to arg2\narg1 \u003c= arg2 lessThanOrEqualTo(arg1, arg2)  Examples\n51 \u003c= 50 \u003e false 50 \u003c= 50 \u003e true 49 \u003c= 50 \u003e true lessThanOrEqualTo(51, 50) \u003e false lessThanOrEqualTo(50, 50) \u003e true lessThanOrEqualTo(49, 50) \u003e true  Not Inverts boolean values making true, false etc.\nnot(booleanValue)  Examples\nnot(5 \u003e 10) \u003e true not(5 = 5) \u003e false not(false()) \u003e true  ","categories":"","description":"","excerpt":"Equals Evaluates if arg1 is equal to arg2\narg1 = arg2 equals(arg1, …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/expressions/logic/","tags":"","title":"Logic Funtions"},{"body":"Add arg1 + arg2  Or reduce the args by successive addition\nadd(args...)  Examples\n34 + 9 \u003e 43 add(45, 6, 72) \u003e 123  Average Takes an average value of the arguments\naverage(args...) mean(args...)  Examples\naverage(10, 20, 30, 40) \u003e 25 mean(8.9, 24, 1.2, 1008) \u003e 260.525  Divide Divides arg1 by arg2\narg1 / arg2  Or reduce the args by successive division\ndivide(args...)  Examples\n42 / 7 \u003e 6 divide(1000, 10, 5, 2) \u003e 10 divide(100, 4, 3) \u003e 8.33  Max Determines the maximum value given in the args\nmax(args...)  Examples\nmax(100, 30, 45, 109) \u003e 109 # They can be nested max(max(${val}), 40, 67, 89) ${val} = [20, 1002] \u003e 1002  Min Determines the minimum value given in the args\nmin(args...)  Examples\nmin(100, 30, 45, 109) \u003e 30  They can be nested\nmin(max(${val}), 40, 67, 89) ${val} = [20, 1002] \u003e 20  Modulo Determines the modulus of the dividend divided by the divisor.\nmodulo(dividend, divisor)  Examples\nmodulo(100, 30) \u003e 10  Multiply Multiplies arg1 by arg2\narg1 * arg2  Or reduce the args by successive multiplication\nmultiply(args...)  Examples\n4 * 5 \u003e 20 multiply(4, 5, 2, 6) \u003e 240  Negate Multiplies arg1 by -1\nnegate(arg1)  Examples\nnegate(80) \u003e -80 negate(23.33) \u003e -23.33 negate(-9.5) \u003e 9.5  Power Raises arg1 to the power arg2\narg1 ^ arg2  Or reduce the args by successive raising to the power\npower(args...)  Examples\n4 ^ 3 \u003e 64 power(2, 4, 3) \u003e 4096  Random Generates a random number between 0.0 and 1.0\nrandom()  Examples\nrandom() \u003e 0.78 random() \u003e 0.89 ...you get the idea  Subtract arg1 - arg2  Or reduce the args by successive subtraction\nsubtract(args...)  Examples\n29 - 8 \u003e 21 subtract(100, 20, 34, 2) \u003e 44  Sum Sums all the arguments together\nsum(args...)  Examples\nsum(89, 12, 3, 45) \u003e 149  ","categories":"","description":"Standard mathematical functions, such as add subtract, multiple, etc.\n","excerpt":"Standard mathematical functions, such as add subtract, multiple, etc.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/expressions/mathematics/","tags":"","title":"Mathematics Functions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/node/","tags":"","title":"node"},{"body":"All nodes in an Stroom cluster must be configured correctly for them to communicate with each other.\nConfiguring nodes Open Monitoring/Nodes from the top menu. The nodes screen looks like this:\nTODO Screenshot  You need to edit each line by selecting it and then clicking the edit icon at the bottom. The URL for each node needs to be set as above but obviously substituting in the host name of the individual node, e.g. http://\u003cHOST_NAME\u003e:8080/stroom/clustercall.rpc\nNodes are expected communicate with each other on port 8080 over http. Ensure you have configured your firewall to allow nodes to talk to each other over this port. You can configure the URL to use a different port and possibly HTTPS but performance will be better with HTTP as no SSL termination is required.\nOnce you have set the URLs of each node you should also set the master assignment priority for each node to be different to all of the others. In the image above the priorities have been set in a random fashion to ensure that node3 assumes the role of master node for as long as it is enabled. You also need to check all of the nodes are enabled that you want to take part in processing or any other jobs.\nKeep refreshing the table until all nodes show healthy pings as above. If you do not get ping results for each node then they are not configured correctly.\nOnce a cluster is configured correctly you will get proper distribution of processing tasks and search will be able to access all nodes to take part in a distributed query.\n","categories":"","description":"Configuring the nodes in a Stroom cluster.\n","excerpt":"Configuring the nodes in a Stroom cluster.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/nodes/","tags":["TODO","cluster","node"],"title":"Nodes"},{"body":"As with all other aspects of Data Splitter, output XML is determined by adding certain elements to the Data Splitter configuration.\nThe \u003cdata\u003e element Output is created by Data Splitter using one or more \u003cdata\u003e elements in the configuration. The first \u003cdata\u003e element that is encountered within a matched expression will result in parent \u003crecord\u003e elements being produced in the output.\nAttributes The \u003cdata\u003e element has the following attributes:\n id name value  id Optional attribute used to debug the location of expressions causing errors, see id.\nname Both the name and value attributes of the \u003cdata\u003e element can be specified using match references.\nvalue Both the name and value attributes of the \u003cdata\u003e element can be specified using match references.\nSingle \u003cdata\u003e element example The simplest example that can be provided uses a single \u003cdata\u003e element within a \u003csplit\u003e expression.\nGiven the following input:\nThis is line 1 This is line 2 This is line 3  … and the following configuration:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\"\u003e \u003csplit delimiter=\"\\n\" \u003e \u003cdata value=\"$1\"/\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  … you would get the following output:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003crecords xmlns=\"records:2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"records:2 file://records-v2.0.xsd\" version=\"3.0\"\u003e \u003crecord\u003e \u003cdata value=\"This is line 1\" /\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata value=\"This is line 2\" /\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata value=\"This is line 3\" /\u003e \u003c/record\u003e \u003c/records\u003e  Multiple \u003cdata\u003e element example You could also output multiple \u003cdata\u003e elements for the same \u003crecord\u003e by adding multiple elements within the same expression:\nGiven the following input:\nip=1.1.1.1 user=user1 ip=2.2.2.2 user=user2 ip=3.3.3.3 user=user3  … and the following configuration:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\"\u003e \u003cregex pattern=\"ip=([^ ]+) user=([^ ]+)\\s*\"\u003e \u003cdata name=\"ip\" value=\"$1\"/\u003e \u003cdata name=\"user\" value=\"$2\"/\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  … you would get the following output:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003crecords xmlns=\"records:2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"records:2 file://records-v2.0.xsd\" version=\"3.0\"\u003e \u003crecord\u003e \u003cdata name=\"ip\" value=\"1.1.1.1\" /\u003e \u003cdata name=\"user\" value=\"user1\" /\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata name=\"ip\" value=\"2.2.2.2\" /\u003e \u003cdata name=\"user\" value=\"user2\" /\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata name=\"ip\" value=\"3.3.3.3\" /\u003e \u003cdata name=\"user\" value=\"user3\" /\u003e \u003c/record\u003e \u003c/records\u003e  Multi level \u003cdata\u003e elements As long as all data elements occur within the same parent/ancestor expression, all data elements will be output within the same record.\nGiven the following input:\nip=1.1.1.1 user=user1 ip=2.2.2.2 user=user2 ip=3.3.3.3 user=user3  … and the following configuration:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\"\u003e \u003csplit delimiter=\"\\n\" \u003e \u003cdata name=\"line\" value=\"$1\"/\u003e \u003cgroup value=\"$1\"\u003e \u003cregex pattern=\"ip=([^ ]+) user=([^ ]+)\"\u003e \u003cdata name=\"ip\" value=\"$1\"/\u003e \u003cdata name=\"user\" value=\"$2\"/\u003e \u003c/regex\u003e \u003c/group\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  … you would get the following output:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003crecords xmlns=\"records:2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"records:2 file://records-v2.0.xsd\" version=\"3.0\"\u003e \u003crecord\u003e \u003cdata name=\"line\" value=\"ip=1.1.1.1 user=user1\" /\u003e \u003cdata name=\"ip\" value=\"1.1.1.1\" /\u003e \u003cdata name=\"user\" value=\"user1\" /\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata name=\"line\" value=\"ip=2.2.2.2 user=user2\" /\u003e \u003cdata name=\"ip\" value=\"2.2.2.2\" /\u003e \u003cdata name=\"user\" value=\"user2\" /\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata name=\"line\" value=\"ip=3.3.3.3 user=user3\" /\u003e \u003cdata name=\"ip\" value=\"3.3.3.3\" /\u003e \u003cdata name=\"user\" value=\"user3\" /\u003e \u003c/record\u003e \u003c/records\u003e  Nesting \u003cdata\u003e elements Rather than having \u003cdata\u003e elements all appear as children of \u003crecord\u003e it is possible to nest them either as direct children or within child groups.\nDirect children Given the following input:\nip=1.1.1.1 user=user1 ip=2.2.2.2 user=user2 ip=3.3.3.3 user=user3  … and the following configuration:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\"\u003e \u003cregex pattern=\"ip=([^ ]+) user=([^ ]+)\\s*\"\u003e \u003cdata name=\"line\" value=\"$\"\u003e \u003cdata name=\"ip\" value=\"$1\"/\u003e \u003cdata name=\"user\" value=\"$2\"/\u003e \u003c/data\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  … you would get the following output:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003crecords xmlns=\"records:2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"records:2 file://records-v2.0.xsd\" version=\"3.0\"\u003e \u003crecord\u003e \u003cdata name=\"line\" value=\"ip=1.1.1.1 user=user1\"\u003e \u003cdata name=\"ip\" value=\"1.1.1.1\" /\u003e \u003cdata name=\"user\" value=\"user1\" /\u003e \u003c/data\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata name=\"line\" value=\"ip=2.2.2.2 user=user2\"\u003e \u003cdata name=\"ip\" value=\"2.2.2.2\" /\u003e \u003cdata name=\"user\" value=\"user2\" /\u003e \u003c/data\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata name=\"line\" value=\"ip=3.3.3.3 user=user3\"\u003e \u003cdata name=\"ip\" value=\"3.3.3.3\" /\u003e \u003cdata name=\"user\" value=\"user3\" /\u003e \u003c/data\u003e \u003c/record\u003e \u003c/records\u003e  Within child groups Given the following input:\nip=1.1.1.1 user=user1 ip=2.2.2.2 user=user2 ip=3.3.3.3 user=user3  … and the following configuration:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\"\u003e \u003csplit delimiter=\"\\n\" \u003e \u003cdata name=\"line\" value=\"$1\"\u003e \u003cgroup value=\"$1\"\u003e \u003cregex pattern=\"ip=([^ ]+) user=([^ ]+)\"\u003e \u003cdata name=\"ip\" value=\"$1\"/\u003e \u003cdata name=\"user\" value=\"$2\"/\u003e \u003c/regex\u003e \u003c/group\u003e \u003c/data\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  … you would get the following output:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003crecords xmlns=\"records:2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"records:2 file://records-v2.0.xsd\" version=\"3.0\"\u003e \u003crecord\u003e \u003cdata name=\"line\" value=\"ip=1.1.1.1 user=user1\"\u003e \u003cdata name=\"ip\" value=\"1.1.1.1\" /\u003e \u003cdata name=\"user\" value=\"user1\" /\u003e \u003c/data\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata name=\"line\" value=\"ip=2.2.2.2 user=user2\"\u003e \u003cdata name=\"ip\" value=\"2.2.2.2\" /\u003e \u003cdata name=\"user\" value=\"user2\" /\u003e \u003c/data\u003e \u003c/record\u003e \u003crecord\u003e \u003cdata name=\"line\" value=\"ip=3.3.3.3 user=user3\"\u003e \u003cdata name=\"ip\" value=\"3.3.3.3\" /\u003e \u003cdata name=\"user\" value=\"user3\" /\u003e \u003c/data\u003e \u003c/record\u003e \u003c/records\u003e  The above example produces the same output as the previous but could be used to apply much more complex expression logic to produce the child \u003cdata\u003e elements, e.g. the inclusion of multiple child expressions to deal with different types of lines.\n","categories":"","description":"","excerpt":"As with all other aspects of Data Splitter, output XML is determined …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-splitter/element-reference/2-4-output/","tags":"","title":"Output"},{"body":"The following capabilities are available to parse input data:\n XML - XML input can be parsed with the XML parser. XML Fragment - Treat input data as an XML fragment, i.e. XML that does not have an XML declaration or root elements. Data Splitter - Delimiter and regular expression based language for turning non XML data into XML (e.g. CSV)  ","categories":"","description":"Parsing input data.\n","excerpt":"Parsing input data.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/pipelines/parser/","tags":"","title":"Parser"},{"body":"Stroom can support multiple payload formats with different compression applied. However all data once uncompressed must be text and not binary.\nStroom can receive data in the following formats:\n Uncompressed - Text data is sent to Stroom and no compression flag is set in the header arguments. GZIP - Text data is GZIP compressed and the compression flag is set to GZIP. ZIP - A text file is compressed into a ZIP archive and sent to Stroom with the compression flag set to ZIP. The ZIP file must contain one data file and an optional context file, see below.  Context Files ZIP files sent to Stroom are expected to contain the data file and an optional context file *.ctx. If provided a context file can be used to provide reference data that is specific to the data file that has been sent. Context data is supplimentary information that is not contained within logged events, e.g. the machine name, ip address etc may be delivered in a context file if it is not written by an application in each logged event.\nCharacter Encodings Although Stroom only supports data in text format, text can be encoded using multiple character encodings. Supported encodings include:\n ISO-8859-1 (understood by default) Windows-1252 - ANSI (understood by default) ASCII (understood by default) UTF-8 (with or without BOM) UTF-16LE (little endian with or without BOM) UTF-16BE (big endian with or without BOM) UTF-32LE (little endian with or without BOM) UTF-32BE (big endian with or without BOM)  In order to tell Stroom what character encoding to use the feed that the data belongs to can be configured within the Stroom application to use a specific character encoding. Separate character encodings can be specified for logged event and context data.\n","categories":"","description":"Description of the data formats for sending data into a Stroom instance.\n","excerpt":"Description of the data formats for sending data into a Stroom …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/sending-data/payloads/","tags":"","title":"Payloads"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/pipeline/","tags":"","title":"pipeline"},{"body":"Pipelines Every feed has an associated translation. The translation is used to convert the input text or XML into event logging XML format.\nXSLT is used to translate from XML to event logging XML.\n","categories":"","description":"Pipelines are the mechanism for processin and transforming ingested data.\n","excerpt":"Pipelines are the mechanism for processin and transforming ingested …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/pipelines/","tags":["pipeline","processing"],"title":"Pipelines"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/processing/","tags":"","title":"processing"},{"body":" This documentation applies to Stroom 7.0 or higher\n Properties are the means of configuring the Stroom application and are typically maintained by the Stroom system administrator. The value of some properties are required in order for Stroom to function, e.g. database connection details, and thus need to be set prior to running Stroom. Some properties can be changed at runtime to alter the behaviour of Stroom.\nSources Property values can be defined in the following locations.\nSystem Default The system defaults are hard-coded into the Stroom application code by the developers and can’t be changed. These represent reasonable defaults, where applicable, but may need to be changed, e.g. to reflect the scale of the Stroom system or the specific environment.\nThe default property values can either be viewed in the Stroom user interface (Tools -\u003e Properties) or in the file config/config-defaults.yml in the Stroom distribution.\nGlobal Database Value Global database values are property values stored in the database that are global across the whole cluster.\nThe global database value is defined as a record in the config table in the database. The database record will only exist if a database value has explicitly been set. The database value will apply to all nodes in the cluster, overriding the default value, unless a node also has a value set in its YAML configuration.\nDatabase values can be set from the Stroom user interface using Tools -\u003e Properties. Some properties are marked Read Only which means they cannot have a database value set for them. These properties can only be altered via the YAML configuration file on each node. Such properties are typically used to configure values required for Stroom to be able to boot, so it does not make sense for them to be configurable from the User Interface.\nYAML Configuration file Stroom is built on top of a framework called Drop Wizard. Drop Wizard uses a YAML configuration file on each node to configure the application. This is typically config.yml and is located in the config directory.\nThis file contains both the Drop Wizard configuration settings (settings for ports, paths and application logging) and the Stroom specific properties configuration. The file is in YAML format and the Stroom properties are located under the appConfig key. For details of the Drop Wizard configuration structure, see here (external link).\nThe file is split into three sections using these keys:\n server - Configuration of the web server, e.g. ports, paths, request logging. logging - Configuration of application logging appConfig - The stroom configuration properties  The following is an example of the YAML configuration file:\n# Drop Wizard configuration section server: # e.g. ports and paths logging: # e.g. logging levels/appenders # Stroom properties configuration section appConfig: commonDbDetails: connection: jdbcDriverClassName: ${STROOM_JDBC_DRIVER_CLASS_NAME:-com.mysql.cj.jdbc.Driver} jdbcDriverUrl: ${STROOM_JDBC_DRIVER_URL:-jdbc:mysql://localhost:3307/stroom?useUnicode=yes\u0026characterEncoding=UTF-8} jdbcDriverUsername: ${STROOM_JDBC_DRIVER_USERNAME:-stroomuser} jdbcDriverPassword: ${STROOM_JDBC_DRIVER_PASSWORD:-stroompassword1} contentPackImport: enabled: true ...  In the Stroom user interface properties are named with a dot notation key, e.g. stroom.contentPackImport.enabled. Each part of the dot notation property name represents a key in the YAML file, e.g. for this example, the location in the YAML would be:\nappConfig: contentPackImport: enabled: true # stroom.contentPackImport.enabled  The stroom part of the dot notation name is replaced with appConfig.\nVariable Substitution The YAML configuration file supports Bash style variable substitution in the form of:\n${ENV_VAR_NAME:-value_if_not_set}  This allows values to be set either directly in the file or via an environment variable, e.g.\njdbcDriverClassName: ${STROOM_JDBC_DRIVER_CLASS_NAME:-com.mysql.cj.jdbc.Driver}  In the above example, if the STROOM_JDBC_DRIVER_CLASS_NAME environment variable is not set then the value com.mysql.cj.jdbc.Driver will be used instead.\nTyped Values YAML supports typed values rather than just strings, see https://yaml.org/refcard.html. YAML understands booleans, strings, integers, floating point numbers, as well as sequences/lists and maps. Some properties will be represented differently in the user interface to the YAML file. This is due to how values are stored in the database and how the current user interface works. This will likely be improved in future versions. For details of how different types are represented in the YAML and the UI, see Data Types.\nSource Precedence The three sources (Default, Database \u0026 YAML) are listed in increasing priority, i.e YAML trumps Database, which trumps Default.\nFor example, in a two node cluster, this table shows the effective value of a property on each node. A - indicates the value has not been set in that source. NULL indicates that the value has been explicitly set to NULL.\n   Source Node1 Node2     Default red red   Database - -   YAML - blue   Effective red blue    Or where a Database value is set.\n   Source Node1 Node2     Default red red   Database green green   YAML - blue   Effective green blue    Or where a YAML value is explicitly set to NULL.\n   Source Node1 Node2     Default red red   Database green green   YAML - NULL   Effective green NULL    Data Types Stroom property values can be set using a number of different data types. Database property values are currently set in the user interface using the string form of the value. For each of the data types defined below, there will be an example of how the data type is recorded in its string form.\n   Data Type Example UI String Forms Example YAML form     Boolean true false true false   String This is a string \"This is a string\"   Integer/Long 123 123   Float 1.23 1.23   Stroom Duration P30D P1DT12H PT30S 30d 30s 30000 \"P30D\" \"P1DT12H\" \"PT30S\" \"30d\" \"30s\" \"30000\" See Stroom Duration Data Type.   List #red#Green#Blue ,1,2,3 See List Data Type   Map ,=red=FF0000,Green=00FF00,Blue=0000FF See Map Data Type   DocRef ,docRef(MyType,a56ff805-b214-4674-a7a7-a8fac288be60,My DocRef name) See DocRef Data Type   Enum HIGH LOW \"HIGH\" \"LOW\"   Path /some/path/to/a/file \"/some/path/to/a/file\"   ByteSize 32, 512Kib 32, 512Kib See Byte Size Data Type    Stroom Duration Data Type The Stroom Duration data type is used to specify time durations, for example the time to live of a cache or the time to keep data before it is purged. Stroom Duration uses a number of string forms to support legacy property values.\nISO 8601 Durations Stroom Duration can be expressed using ISO 8601 (external link) duration strings. It does NOT support the full ISO 8601 format, only D, H, M and S. For details of how the string is parsed to a Stroom Duration, see Duration (external link)\nThe following are examples of ISO 8601 duration strings:\n P30D - 30 days P1DT12H - 1 day 12 hours (36 hours) PT30S - 30 seconds PT0.5S - 500 milliseconds  Legacy Stroom Durations This format was used in versions of Stroom older than v7 and is included to support legacy property values.\nThe following are examples of legacy duration strings:\n 30d - 30 days 12h - 12 hours 10m - 10 minutes 30s - 30 seconds 500 - 500 milliseconds  Combinations such as 1m30s are not supported.\nList Data Type This type supports ordered lists of items, where an item can be of any supported data type, e.g. a list of strings or list of integers.\nThe following is an example of how a property (statusValues) that is is List of strings is represented in the YAML:\nannotation: statusValues: - \"New\" - \"Assigned\" - \"Closed\"  This would be represented as a string in the User Interface as:\n|New|Assigned|Closed.\nSee Delimiters in String Conversion for details of how the items are delimited in the string.\nThe following is an example of how a property (cpu) that is is List of DocRefs is represented in the YAML:\nstatistics: internal: cpu: - type: \"StatisticStore\" uuid: \"af08c4a7-ee7c-44e4-8f5e-e9c6be280434\" name: \"CPU\" - type: \"StroomStatsStore\" uuid: \"1edfd582-5e60-413a-b91c-151bd544da47\" name: \"CPU\"  This would be represented as a string in the User Interface as:\n|,docRef(StatisticStore,af08c4a7-ee7c-44e4-8f5e-e9c6be280434,CPU)|,docRef(StroomStatsStore,1edfd582-5e60-413a-b91c-151bd544da47,CPU)\nSee Delimiters in String Conversion for details of how the items are delimited in the string.\nMap Data Type This type supports a collection of key/value pairs where the key is unique within the collection. The type of the key must be string, but the type of the value can be any supported type.\nThe following is an example of how a property (mapProperty) that is a map of string =\u003e string would be represented in the YAML:\nmapProperty: red: \"FF0000\" green: \"00FF00\" blue: \"0000FF\"  This would be represented as a string in the User Interface as:\n,=red=FF0000,Green=00FF00,Blue=0000FF\nThe delimiter between pairs is defined first, then the delimiter for the key and value.\nSee Delimiters in String Conversion for details of how the items are delimited in the string.\nDocRef Data Type A DocRef (or Document Reference) is a type specific to Stroom that defines a reference to an instance of a Document within Stroom, e.g. an XLST, Pipeline, Dictionary, etc. A DocRef consists of three parts, the type, the UUID (external link) and the name of the Document.\nThe following is an example of how a property (aDocRefProperty) that is a DocRef would be represented in the YAML:\naDocRefProperty: type: \"MyType\" uuid: \"a56ff805-b214-4674-a7a7-a8fac288be60\" name: \"My DocRef name\"  This would be represented as a string in the User Interface as:\n,docRef(MyType,a56ff805-b214-4674-a7a7-a8fac288be60,My DocRef name)\nSee Delimiters in String Conversion for details of how the items are delimited in the string.\nByte Size Data Type The Byte Size data type is used to represent a quantity of bytes using the IEC (external link) standard. Quantities are represented as powers of 1024, i.e. a Kib (Kibibyte) means 1024 bytes.\nExamples of Byte Size values in string form are (a YAML value would optionally be surrounded with double quotes):\n 32, 32b, 32B, 32bytes - 32 bytes 32K, 32KB, 32KiB - 32 kibibytes 32M, 32MB, 32MiB - 32 mebibytes 32G, 32GB, 32GiB - 32 gibibytes 32T, 32TB, 32TiB - 32 tebibytes 32P, 32PB, 32PiB - 32 pebibytes  The *iB form is preferred as it is more explicit and avoids confusion with SI units.\nDelimiters in String Conversion The string conversion used for collection types like List, Map etc. relies on the string form defining the delimiter(s) to use for the collection. The delimiter(s) are added as the first n characters of the string form, e.g. |red|green|blue or |=red=FF0000|Green=00FF00|Blue=0000FF. It is possible to use a number of different delimiters to allow for delimiter characters appearing in the actual value, e.g. #some text#some text with a | in it The following are the delimiter characters that can be used.\n|, :, ;, ,, !, /, \\, #, @, ~, -, _, =, +, ?\nWhen Stroom records a property value to the database it may use a delimiter of its own choosing, ensuring that it picks a delimiter that is not used in the property value.\nRestart Required Some properties are marked as requiring a restart. There are two scopes for this:\nRequires UI Refresh If a property is marked in UI as requiring a UI refresh then this means that a change to the property requires that the Stroom nodes serving the UI are restarted for the new value to take effect.\nRequires Restart If a property is marked in UI as requiring a restart then this means that a change to the property requires that all Stroom nodes are restarted for the new value to take effect.\n","categories":"","description":"Configuration of Stroom's application properties.\n","excerpt":"Configuration of Stroom's application properties.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/properties/","tags":["properties","configuration"],"title":"Properties"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/properties/","tags":"","title":"properties"},{"body":"Dashboard queries are created with the query expression builder. The expression builder allows for complex boolean logic to be created across multiple index fields. The way in which different index fields may be queried depends on the type of data that the index field contains.\nDate Time Fields Time fields can be queried for times equal, greater than, greater than or equal, less than, less than or equal or between two times.\nTimes can be specified in two ways:\n  Absolute times\n  Relative times\n  Absolute Times An absolute time is specified in ISO 8601 date time format, e.g. 2016-01-23T12:34:11.844Z\nRelative Times In addition to absolute times it is possible to specify times using expressions. Relative time expressions create a date time that is relative to the execution time of the query. Supported expressons are as follows:\n  now() - The current execution time of the query.\n  second() - The current execution time of the query rounded down to the nearest second.\n  minute() - The current execution time of the query rounded down to the nearest minute.\n  hour() - The current execution time of the query rounded down to the nearest hour.\n  day() - The current execution time of the query rounded down to the nearest day.\n  week() - The current execution time of the query rounded down to the first day of the week (Monday).\n  month() - The current execution time of the query rounded down to the start of the current month.\n  year() - The current execution time of the query rounded down to the start of the current year.\n  Adding/Subtracting Durations With relative times it is possible to add or subtract durations so that queries can be constructed to provide for example, the last week of data, the last hour of data etc.\nTo add/subtract a duration from a query term the duration is simply appended after the relative time, e.g.\nnow() + 2d\nMultiple durations can be combined in the expression, e.g.\nnow() + 2d - 10h\nnow() + 2w - 1d10h\nDurations consist of a number and duration unit. Supported duration units are:\n  s - Seconds\n  m - Minutes\n  h - Hours\n  d - Days\n  w - Weeks\n  M - Months\n  y - Years\n  Using these durations a query to get the last weeks data could be as follows:\nbetween now() - 1w and now()\nOr midnight a week ago to midnight today:\nbetween day() - 1w and day()\nOr if you just wanted data for the week so far:\ngreater than week()\nOr all data for the previous year:\nbetween year() - 1y and year()\nOr this year so far:\ngreater than year()\n","categories":"","description":"How to query the data in Stroom.\n","excerpt":"How to query the data in Stroom.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/queries/","tags":["query"],"title":"Queries"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/query/","tags":"","title":"query"},{"body":"The Query API uses common request/response models and end points for querying each type of data source held in Stroom. The request/response models are defined in stroom-query (external link).\nCurrently Stroom exposes a set of query endpoints for the following data source types. Each data source type will have its own endpoint due to differences in the way the data is queried and the restrictions imposed on the query terms. However they all share the same API definition.\n stroom-index (external link) (the Lucene based event index) sqlstatistics (external link) (Stroom’s own statistics store)  The detailed documentation for the request/responses is contained in the Swagger definition linked to above.\nCommon endpoints The standard query endpoints are\n  /datasource\n  /search\n  /destroy\n  datasource The data source endpoint is used to query Stroom for the details of a data source with a given docRef. The details will include such things as the fields available and any restrictions on querying the data.\nsearch The search endpoint is used to initiate a search against a data source or to request more data for an active search. A search request can be made using iterative mode, where it will perform the search and then only return the data it has immediately available. Subsequent requests for the same queryKey will also return the data immediately available, expecting that more results will have been found by the query. Requesting a search in non-iterative mode will result in the response being returned when the query has completed and all known results have been found.\nThe SearchRequest model is fairly complicated and contains not only the query terms but also a definition of how the data should be returned. A single SearchRequest can include multiple ResultRequest sections to return the queried data in multiple ways, e.g. as flat data and in an alternative aggregated form.\nStroom as a query builder Stroom is able to export the json form of a SearchRequest model from its dashboards. This makes the dashboard a useful tool for building a query and the table settings to go with it. You can use the dashboard to defined the data source, define the query terms tree and build a table definition (or definitions) to describe how the data should be returned. The, clicking the download icon on the query pane of the dashboard will generate the SearchRequest json which can be immediately used with the /search API or modified to suit.\ndestroy This endpoint is used to kill an active query by supplying the queryKey for query in question.\n","categories":"","description":"An API to allow other systems to query the data held in Stroom.\n","excerpt":"An API to allow other systems to query the data held in Stroom.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/api/query-api/","tags":["api"],"title":"Query API"},{"body":" Version Information: Created with Stroom v7.0\nLast Updated: 15 September 2020\nSee Also:\n HOWTO - Creating a Simple Reference Feed XSLT Functions   In Stroom reference data is primarily used to decorate events using stroom:lookup() calls in XSLTs. For example you may have reference data feed that associates the FQDN of a device to the physical location. You can then perform a stroom:lookup() in the XSLT to decorate an event with the physical location of a device by looking up the FQDN found in the event.\nReference data is time sensitive and each stream of reference data has an Effective Date set against it. This allows reference data lookups to be performed using the date of the event to ensure the reference data that was actually effective at the time of the event is used.\nUsing reference data involves the following steps/processes:\n Ingesting the raw reference data into Stroom. Creating (and processing) a pipeline to transform the raw reference into reference-data:2 format XML. Creating a reference loader pipeline with a Reference Data Filter element to load cooked reference data into the reference data store. Adding reference pipeline/feeds to an XSLT Filter in your event pipeline. Adding the lookup call to the XSLT. Processing the raw events through the event pipeline.  The process of creating a reference data pipeline is described in the HOWTO linked at the top of this document.\nReference Data Structure A reference data entry essentially consists of the following:\n Effective time - The data/time that the entry was effective from, i.e the time the raw reference data was received. Map name - A unique name for the key/value map that the entry will be stored in. The name only needs to be unique within all map names that may be loaded within an XSLT Filter. In practice it makes sense to keep map names globally unique. Key - The text that will be used to lookup the value in the reference data map. Mutually exclusive with Range. Range - The inclusive range of integer keys that the entry applies to. Mutually exclusive with Key. Value - The value can either be simple text, e.g. an IP address, or an XML fragment that can be inserted into another XML document. XML values must be correctly namespaced.  The following is an example of some reference data that has been converted from its raw form into reference-data:2 XML. In this example the reference data contains three entries that each belong to a different map. Two of the entries are simple text values and the last has an XML value.\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\"?\u003e \u003creferenceData xmlns=\"reference-data:2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:stroom=\"stroom\" xmlns:evt=\"event-logging:3\" xsi:schemaLocation=\"reference-data:2 file://reference-data-v2.0.xsd\" version=\"2.0.1\"\u003e \u003c!-- A simple string value --\u003e \u003creference\u003e \u003cmap\u003eFQDN_TO_IP\u003c/map\u003e \u003ckey\u003estroomnode00.strmdev00.org\u003c/key\u003e \u003cvalue\u003e \u003cIPAddress\u003e192.168.2.245\u003c/IPAddress\u003e \u003c/value\u003e \u003c/reference\u003e \u003c!-- A simple string value --\u003e \u003creference\u003e \u003cmap\u003eIP_TO_FQDN\u003c/map\u003e \u003ckey\u003e192.168.2.245\u003c/key\u003e \u003cvalue\u003e \u003cHostName\u003estroomnode00.strmdev00.org\u003c/HostName\u003e \u003c/value\u003e \u003c/reference\u003e \u003c!-- A key range --\u003e \u003creference\u003e \u003cmap\u003eUSER_ID_TO_COUNTRY_CODE\u003c/map\u003e \u003crange\u003e \u003cfrom\u003e1\u003c/from\u003e \u003cto\u003e1000\u003c/to\u003e \u003c/range\u003e \u003cvalue\u003eGBR\u003c/value\u003e \u003c/reference\u003e \u003c!-- An XML fragment value --\u003e \u003creference\u003e \u003cmap\u003eFQDN_TO_LOC\u003c/map\u003e \u003ckey\u003estroomnode00.strmdev00.org\u003c/key\u003e \u003cvalue\u003e \u003cevt:Location\u003e \u003cevt:Country\u003eGBR\u003c/evt:Country\u003e \u003cevt:Site\u003eBristol-S00\u003c/evt:Site\u003e \u003cevt:Building\u003eGZero\u003c/evt:Building\u003e \u003cevt:Room\u003eR00\u003c/evt:Room\u003e \u003cevt:TimeZone\u003e+00:00/+01:00\u003c/evt:TimeZone\u003e \u003c/evt:Location\u003e \u003c/value\u003e \u003c/reference\u003e \u003c/referenceData\u003e  Reference Data Namespaces When XML reference data values are created, as in the example XML above, the XML values must be qualified with a namespace to distinguish them from the reference-data:2 XML that surrounds them. In the above example the XML fragment will become as follows when injected into an event:\n\u003cevt:Location xmlns:evt=\"event-logging:3\" \u003e \u003cevt:Country\u003eGBR\u003c/evt:Country\u003e \u003cevt:Site\u003eBristol-S00\u003c/evt:Site\u003e \u003cevt:Building\u003eGZero\u003c/evt:Building\u003e \u003cevt:Room\u003eR00\u003c/evt:Room\u003e \u003cevt:TimeZone\u003e+00:00/+01:00\u003c/evt:TimeZone\u003e \u003c/evt:Location\u003e  Even if evt is already declared in the XML being injected into it, if it has been declared for the reference fragment then it will be explicitly declared in the destination. While duplicate namespacing may appear odd it is valid XML.\nThe namespacing can also be achieved like this:\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\"?\u003e \u003creferenceData xmlns=\"reference-data:2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:stroom=\"stroom\" xsi:schemaLocation=\"reference-data:2 file://reference-data-v2.0.xsd\" version=\"2.0.1\"\u003e \u003c!-- An XML value --\u003e \u003creference\u003e \u003cmap\u003eFQDN_TO_LOC\u003c/map\u003e \u003ckey\u003estroomnode00.strmdev00.org\u003c/key\u003e \u003cvalue\u003e \u003cLocation xmlns=\"event-logging:3\"\u003e \u003cCountry\u003eGBR\u003c/Country\u003e \u003cSite\u003eBristol-S00\u003c/Site\u003e \u003cBuilding\u003eGZero\u003c/Building\u003e \u003cRoom\u003eR00\u003c/Room\u003e \u003cTimeZone\u003e+00:00/+01:00\u003c/TimeZone\u003e \u003c/Location\u003e \u003c/value\u003e \u003c/reference\u003e \u003c/referenceData\u003e  This reference data will be injected into event XML exactly as it, i.e.:\n\u003cLocation xmlns=\"event-logging:3\"\u003e \u003cCountry\u003eGBR\u003c/Country\u003e \u003cSite\u003eBristol-S00\u003c/Site\u003e \u003cBuilding\u003eGZero\u003c/Building\u003e \u003cRoom\u003eR00\u003c/Room\u003e \u003cTimeZone\u003e+00:00/+01:00\u003c/TimeZone\u003e \u003c/Location\u003e  Reference Data Storage Reference data is stored in two different places on a Stroom node. All reference data is only visible to the node where it is located. Each node that is performing reference data lookups will need to load and store its own reference data. While this will result in duplicate data being held by nodes it makes the storage of reference data and its subsequent lookup very performant.\nOn-Heap Store The On-Heap store is the reference data store that is held in memory in the Java Heap. This store is volatile and will be lost on shut down of the node. The On-Heap store is only used for storage of context data.\nOff-Heap Store The Off-Heap store is the reference data store that is held in memory outside of the Java Heap and is persisted to to local disk. As the store is also persisted to local disk it means the reference data will survive the shutdown of the stroom instance. Storing the data off-heap means Stroom can run with a much smaller Java Heap size.\nThe Off-Heap store is based on the Lightning Memory-Mapped Database (LMDB). LMDB makes use of the Linux page cache to ensure that hot portions of the reference data are held in the page cache (making use of all available free memory). Infrequently used portions of the reference data will be evicted from the page cache by the Operating System. Given that LMDB utilises the page cache for holding reference data in memory the more free memory the host has the better as there will be less shifting of pages in/out of the OS page cache. When storing large amounts of data you may experience the OS reporting very little free memory as a large amount will be in use by the page cache. This is not an issue as the OS will evict pages when memory is needed for other applications, e.g. the Java Heap.\nLocal Disk The Off-Heap store is intended to be located on local disk on the Stroom node. The location of the store is set using the property stroom.pipeline.referenceData.localDir. Using LMDB on remote storage is NOT advised, see http://www.lmdb.tech/doc.\nIf you are running stroom on AWS EC2 instances then you will need to attach some local instance storage to the host, e.g. SSD, to use for the reference data store. In tests EBS storage was found to be VERY slow. It should be noted that AWS instance storage is not persistent between instance stops, terminations and hardware failure. However any loss of the reference data store will mean that the next time Stroom boots a new store will be created and reference data will be loaded on demand as normal.\nTransactions LMDB is a transactional database with ACID semantics. All interaction with LMDB is done within a read or write transaction. There can only be one write transaction at a time so if there are a number of concurrent reference data loads then they will have to wait in line. Read transactions, i.e. lookups, are not blocked by each other or by write transactions so once the data is loaded and is in memory lookups can be performed very quickly.\nRead-Ahead Mode When data is read from the store, if the data is not already in the page cache then it will be read from disk and added to the page cache by the OS. Read-ahead is the process of speculatively reading ahead to load more pages into the page cache then were requested. This is on the basis that future requests for data may need the pages speculatively read into memory. If the reference data store is very large or is larger than the available memory then it is recommended to turn read-ahead off as the result will be to evict hot reference data from the page cache to make room for speculative pages that may not be needed. It can be tuned off with the system property stroom.pipeline.referenceData.readAheadEnabled.\nKey Size When reference data is created care must be taken to ensure that the Key used for each entry is less than 507 bytes. For simple ASCII characters then this means less than 507 characters. If non-ASCII characters are in the key then these will take up more than one byte per character so the length of the key in characters will be less. This is a limitation inherent to LMDB.\nCommit intervals The property stroom.pipeline.referenceData.maxPutsBeforeCommit controls the number of entries that are put into the store between each commit. As there can be only one transaction writing to the store at a time, committing periodically allows other process to jump in and make writes. There is a trade off though as reducing the number of entries put between each commit can seriously affect performance. For the fastest single process performance a value of zero should be used which means it will not commit mid-load. This however means all other processes wanting to write to the store will need to wait.\nCloning The Off Heap Store If you are provisioning a new stroom node it is possible to copy the off heap store from another node. Stroom should not be running on the node being copied from. Simply copy the contents of stroom.pipeline.referenceData.localDir into the same configured location on the other node. The new node will use the copied store and have access to its reference data.\nStore Size \u0026 Compaction Due to the way LMDB works the store can only grow in size, it will never shrink, even if data is deleted. Deleted data frees up space for new writes to the store so will be reused but never freed. If there is a regular process of purging old data and adding new reference data then this should not be an issue as the new reference data will use the space made available by the purged data.\nIf store size becomes an issue then it is possible to compact the store. lmdb-utils is available on some package managers and this has an mdb_copy command that can be used with the -c switch to copy the LMDB environment to a new one, compacting it in the process. This should be done when Stroom is down to avoid writes happening to the store while the copy is happening.\nGiven that the store is essentially a cache and all data can be re-loaded another option is to delete the contents of stroom.pipeline.referenceData.localDir when Stroom is not running. On boot Stroom will create a brand new store and reference data will be re-loaded as required.\nThe Loading Process Reference data is loaded into the store on demand during the processing of a stroom:lookup() method call. Reference data will only be loaded if it does not already exist in the store, however it is always loaded as a complete stream, rather than entry by entry.\nThe test for existence in the store is based on the following criteria:\n The UUID of the reference loader pipeline. The version of the reference loader pipeline. The Stream ID for the stream of reference data that has been deemed effective for the lookup. The Stream Number (in the case of multi part streams).  If a reference stream has already been loaded matching the above criteria then no additional load is required.\nIMPORTANT: It should be noted that as the version of the reference data pipeline forms part of the criteria, if the reference loader pipeline is changed, for whatever reason, then this will invalidate ALL existing reference data associated with that reference loader pipeline.\nTypically the reference loader pipeline is very static so this should not be an issue.\nStandard practice is to convert raw reference data into reference:2 XML on receipt using a pipeline separate to the reference loader. The reference loader is then only concerned with reading cooked reference:2 into the Reference Data Filter.\nIn instances where reference data streams are infrequently used it may be preferable to not convert the raw reference on receipt but instead to do it in the reference loader pipeline.\nDuplicate Keys The Reference Data Filter pipeline element has a property overrideExistingValues which if set to true means if an entry is found in an effective stream with the same key as an entry already loaded then it will overwrite the existing one. Entries are loaded in the order they are found in the reference:2 XML document. If set to false then the existing entry will be kept. If warnOnDuplicateKeys is set to true then a warning will be logged for any duplicate keys, whether an overwrite happens or not.\nValue De-Duplication Only unique values are held in the store to reduce the storage footprint. This is useful given that typically, reference data updates may be received daily and each one is a full snapshot of the whole reference data. As a result this can mean many copies of the same value being loaded into the store. The store will only hold the first instance of duplicate values.\nQuerying the Reference Data Store The reference data store can be queried within a Dashboard in Stroom by selecting Reference Data Store in the data source selection pop-up. Querying the store is currently an experimental feature and is mostly intended for use in fault finding. Given the localised nature of the reference data store the dashboard can currently only query the store on the node that the user interface is being served from. In a multi-node environment where some nodes are UI only and most are processing only, the UI nodes will have no reference data in their store.\nPurging Old Reference Data Reference data loading and purging is done at the level of a reference stream. Whenever a reference lookup is performed the last accessed time of the reference stream in the store is checked. If it is older than one hour then it will be updated to the current time. This last access time is used to determine reference streams that are no longer in active use and thus can be purged.\nThe Stroom job Ref Data Off-heap Store Purge is used to perform the purge operation on the Off-Heap reference data store. No purge is required for the On-Heap store as that only holds transient data. When the purge job is run it checks the time since each reference stream was accessed against the purge cut-off age. The purge age is configured via the property stroom.pipeline.referenceData.purgeAge. It is advised to schedule this job for quiet times when it is unlikely to conflict with reference loading operations as they will fight for access to the single write transaction.\nLookups Lookups are performed in XSLT Filters using the XSLT functions. In order to perform a lookup one or more reference feeds must be specified on the XSLT Filter pipeline element. Each reference feed is specified along with a reference loader pipeline that will ingest the specified feed (optional convert it into reference:2 XML if it is not already) and pass it into a Reference Data Filter pipeline element.\nReference Feeds \u0026 Loaders In the XSLT Filter pipeline element multiple combinations of feed and reference loader pipeline can be specified. There must be at least one in order to perform lookups. If there are multiple then when a lookup is called for a given time, the effective stream for each feed/loader combination is determined. The effective stream for each feed/loader combination will be loaded into the store, unless it is already present.\nWhen the actual lookup is performed Stroom will try to find the key in each of the effective streams that have been loaded and that contain the map in the lookup call. If the lookup is unsuccessful in the effective stream for the first feed/loader combination then it will try the next, and so on until it has tried all of them. For this reason if you have multiple feed/loader combinations then order is important. It is possible for multiple effective streams to contain the same map/key so a feed/loader combination higher up the list will trump one lower down with the same map/key. Also if you have some lookups that may not return a value and others that should always return a value then the feed/loader for the latter should be higher up the list so it is searched first.\nStandard Key/Value Lookups Standard key/value lookups consist of a simple string key and a value that is either a simple string or an XML fragment. Standard lookups are performed using the various forms of the stroom:lookup() XSLT function.\nRange Lookups Range lookups consist of a key that is an integer and a value that is either a simple string or an XML fragment. For more detail on range lookups see the XSLT function stroom:lookup().\nNested Map Lookups Nested map lookups involve chaining a number of lookups with the value of each map being used as the key for the next. For more detail on nested lookups see the XSLT function stroom:lookup().\nBitmap Lookups A bitmap lookup is a special kind of lookup that actually performs a lookup for each enabled bit position of the passed bitmap value. For more detail on bitmap lookups see the XSLT function stroom:bitmap-lookup().\nValues can either be a simple string or an XML fragment.\nContext data lookups Some event streams have a Context stream associated with them. Context streams allow the system sending the events to Stroom to supply an additional stream of data that provides context to the raw event stream. This can be useful when the system sending the events has no control over the event content but needs to supply additional information. The context stream can be used in lookups as a reference source to decorate events on receipt. Context reference data is specific to a single event stream so is transient in nature, therefore the On Heap Store is used to hold it for the duration of the event stream processing only.\nTypically the reference loader for a context stream will include a translation step to convert the raw context data into reference:2 XML.\nReference Data API The reference data store has an API to allow other systems to access the reference data store. The lookup endpoint requires the caller to provide details of the reference feed and loader pipeline so if the effective stream is not in the store it can be loaded prior to performing the lookup.\nBelow is an example of a lookup request.\n{ \"mapName\": \"USER_ID_TO_LOCATION\", \"effectiveTime\": \"2020-12-02T08:37:02.772Z\", \"key\": \"jbloggs\", \"referenceLoaders\": [ { \"loaderPipeline\" : { \"name\" : \"Reference Loader\", \"uuid\" : \"da1c7351-086f-493b-866a-b42dbe990700\", \"type\" : \"Pipeline\" }, \"referenceFeed\" : { \"name\": \"USER_ID_TOLOCATION-REFERENCE\", \"uuid\": \"60f9f51d-e5d6-41f5-86b9-ae866b8c9fa3\", \"type\" : \"Feed\" } } ] }  ","categories":"","description":"Performing temporal reference data lookups to decorate event data.\n","excerpt":"Performing temporal reference data lookups to decorate event data.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/pipelines/reference-data/","tags":"","title":"Reference Data"},{"body":"Stroom will return a HTTP response code to indicate success or failure. An additional response Header “Stroom-Status” will indicate a more precise error message code. A user readable message will appear in the response body.\n   HTTP Status Stroom-Status Message Reason     200 0 OK Post of data successful   406 100 Feed must be specified You must provide Feed as a header argument in the request   406 110 Feed is not set to receive data The feed you have provided is not setup to receive data (maybe does not exist or is set to reject)   406 200 Unknown compression Compression argument must be one of ZIP, GZIP and NONE   401 300 Client Certificate Required The feed you have provided requires a client HTTPS certificate to send data   403 310 Client Certificate not authorised The feed you have provided does not allow your client certificate to send data   500 400 Compressed stream invalid The stream of data sent does not form a valid compressed file. Maybe it terminated unexpectedly or is corrupt.   500 999 Unknown error An unknown unexpected error occurred    In the event that data is not successfully received by Stroom, i.e. the response code is not 200, the client system should buffer data and keep trying to re-send it. Data should only be removed from the client system when it has been sent successfully.\n","categories":"","description":"The HTTP response codes returned by stroom.\n","excerpt":"The HTTP response codes returned by stroom.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/sending-data/response-codes/","tags":["http"],"title":"Response Codes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/retention/","tags":"","title":"retention"},{"body":" TODO Describe application level permissions and how users and groups behave  ","categories":"","description":"","excerpt":" TODO Describe application level permissions and how users and groups …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/roles/","tags":["roles","authorisation","TODO"],"title":"Roles"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/roles/","tags":"","title":"roles"},{"body":"These functions require a value, and an optional decimal places. If the decimal places are not given it will give you nearest whole number.\nCeiling ceiling(value, decimalPlaces\u003coptional\u003e)  Examples\nceiling(8.4234) \u003e 9 ceiling(4.56, 1) \u003e 4.6 ceiling(1.22345, 3) \u003e 1.223  Floor floor(value, decimalPlaces\u003coptional\u003e)  Examples\nfloor(8.4234) \u003e 8 floor(4.56, 1) \u003e 4.5 floor(1.2237, 3) \u003e 1.223  Round round(value, decimalPlaces\u003coptional\u003e)  Examples\nround(8.4234) \u003e 8 round(4.56, 1) \u003e 4.6 round(1.2237, 3) \u003e 1.224  ","categories":"","description":"Functions for rounding data to a set precision.\n","excerpt":"Functions for rounding data to a set precision.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/expressions/rounding/","tags":"","title":"Rounding Functions"},{"body":"Shared Storage For most large installations Stroom uses shared storage for its data store. This storage could be a CIFS, NFS or similar shared file system. It is recommended that access to this shared storage is protected so that only the application can access it. This could be achieved by placing the storage and application behind a firewall and by requiring appropriate authentication to the shared storage. It should be noted that NFS is unauthenticated so should be used with appropriate safeguards.\nMySQL Accounts It is beyond the scope of this article to discuss this in detail but all MySQL accounts should be secured on initial install. Official guidance for doing this can be found here (external link).\nCommunication Communication between MySQL and the application should be secured. This can be achieved in one of the following ways:\n Placing MySQL and the application behind a firewall Securing communication through the use of iptables Making MySQL and the application communicate over SSL (see here (external link) for instructions)  The above options are not mutually exclusive and may be combined to better secure communication.\nApplication Node to node communication In a multi node Stroom deployment each node communicates with the master node. This can be configured securely in one of several ways:\n Direct communication to Tomcat on port 8080 - Secured by being behind a firewall or using iptables Direct communication to Tomcat on port 8443 - Secured using SSL and certificates Removal of Tomcat connectors other than AJP and configuration of Apache to communicate on port 443 using SSL and certificates  Application to Stroom Proxy Communication The application can be configured to share some information with Stroom Proxy so that Stroom Proxy can decide whether or not to accept data for certain feeds based on the existence of the feed or it’s reject/accept status. The amount of information shared between the application and the proxy is minimal but could be used to discover what feeds are present within the system. Securing this communication is harder as both the application and the proxy will not typically reside behind the same firewall. Despite this communication can still be performed over SSL thus protecting this potential attack vector.\nAdmin port Stroom (v6 and above) and its associated family of stroom-* DropWizard based services all expose an admin port (8081 in the case of stroom). This port serves up various health check and monitoring pages as well as a number of restful services for initiating admin tasks. There is currently no authentication on this admin port so it is assumed that access to this port will be tightly controlled using a firewall, iptables or similar.\nServlets There are several servlets in Stroom that are accessible by certain URLs. Considerations should be made about what URLs are made available via Apache and who can access them. The servlets, path and function are described below:\n   Servlet Path Function Risk     DataFeed /datafeed or /datafeed/* Used to receive data Possible denial of service attack by posting too much data/noise   RemoteFeedService /remoting/remotefeedservice.rpc Used by proxy to ask application about feed status (described in previous section) Possible to systematically discover which feeds are available. Communication with this service should be secured over SSL discussed above   DynamicCSSServlet /stroom/dynamic.css Serves dynamic CSS based on theme configuration Low risk as no important data is made available by this servlet   DispatchService /stroom/dispatch.rpc Service for UI and server communication All back-end services accessed by this umbrella service are secured appropriately by the application   ImportFileServlet /stroom/importfile.rpc Used during configuration upload Users must be authenticated and have appropriate permissions to import configuration   ScriptServlet /stroom/script Serves user defined visualisation scripts to the UI The visualisation script is considered to be part of the application just as the CSS so is not secured   ClusterCallService /clustercall.rpc Used for node to node communication as discussed above Communication must be secured as discussed above   ExportConfig /export/* Servlet used to export configuration data Servlet access must be restricted with Apache to prevent configuration data being made available to unauthenticated users   Status /status Shows the application status including volume usage Needs to be secured so that only appropriate users can see the application status   Echo /echo Block GZIP data posted to the echo servlet is sent back uncompressed. This is a utility servlet for decompression of external data URL should be secured or not made available   Debug /debug Servlet for echoing HTTP header arguments including certificate details Should be secured in production environments   SessionList /sessionList Lists the logged in users Needs to be secured so that only appropriate users can see who is logged in   SessionResourceStore /resourcestore/* Used to create, download and delete temporary files liked to a users session such as data for export This is secured by using the users session and requiring authentication    HDFS, Kafka, HBase, Zookeeper Stroom and stroom-stats can integrate with HDFS, Kafka, HBase and Zookeeper. It should be noted that communication with these external services is currently not secure. Until additional security measures (e.g. authentication) are put in place it is assumed that access to these services will be careful controlled (using a firewall, iptables or similar) so that only stroom nodes can access the open ports.\nContent It may be possible for a user to write XSLT, Data Splitter or other content that may expose data that we do not wish to or to cause the application some harm. At present processing operations are not isolated processes and so it is easy to cripple processing performance with a badly written translation whether written accidentally or on purpose. To mitigate this risk it is recommended that users that are given permission to create XSLT, Data Splitter and Pipeline configurations are trusted to do so.\nVisualisations can be completely customised with javascript. The javascript that is added is executed in a clients browser potentially opening up the possibility of XSS attacks, an attack on the application to access data that a user shouldn’t be able to access, an attack to destroy data or simply failure/incorrect operation of the user interface. To mitigate this risk all user defined javascript is executed within a separate browser IFrame. In addition all javascript should be examined before being added to a production system unless the author is trusted. This may necessitate the creation of a separate development and testing environment for user content.\n","categories":"","description":"There are many aspects of security that should be considered when installing and running Stroom.\n","excerpt":"There are many aspects of security that should be considered when …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/security/","tags":["security"],"title":"Security"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/security/","tags":"","title":"security"},{"body":"Selection functions are a form of aggregate function operating on grouped data.\nAny Selects the first value found in the group that is not null() or err(). If no explicit ordering is set then the value selected is indeterminate.\nany(${val})  Examples\nany(${val}) ${val} = [10, 20, 30, 40] \u003e 10  Bottom Selects the bottom N values and returns them as a delimited string in the order they are read.\nbottom(${val}, delimiter, limit)  Examples\nbottom(${val}, ', ', 2) ${val} = [10, 20, 30, 40] \u003e '30, 40'  First Selects the first value found in the group even if it is null() or err(). If no explicit ordering is set then the value selected is indeterminate.\nfirst(${val})  Examples\nfirst(${val}) ${val} = [10, 20, 30, 40] \u003e 10  Last Selects the last value found in the group even if it is null() or err(). If no explicit ordering is set then the value selected is indeterminate.\nlast(${val})  Examples\nlast(${val}) ${val} = [10, 20, 30, 40] \u003e 40  Nth Selects the Nth value in a set of grouped values. If there is no explicit ordering on the field selected then the value returned is indeterminate.\nnth(${val}, position)  Examples\nnth(${val}, 2) ${val} = [20, 40, 30, 10] \u003e 40  Top Selects the top N values and returns them as a delimited string in the order they are read.\ntop(${val}, delimiter, limit)  Examples\ntop(${val}, ', ', 2) ${val} = [10, 20, 30, 40] \u003e '10, 20'  ","categories":"","description":"Functions for selecting a sub-set of a set of data.\n","excerpt":"Functions for selecting a sub-set of a set of data.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/expressions/selection/","tags":"","title":"Selection Functions"},{"body":"Stroom has a simple HTTP POST interface that requires HTTP header arguments to be supplied as described here.\nFiles are posted to Stroom as described here.\nStroom will return a response code indicating the success or failure status of the post as described here\nData can be sent from may operating systems or applications. Some examples to aid in sending data can be found here\nSSL General notes on Java Keystores\nConfiguring and testing SSL\n","categories":"","description":"How to send data into Stroom.\n","excerpt":"How to send data into Stroom.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/sending-data/","tags":"","title":"Sending Data"},{"body":"This page provides a step by step guide to getting PKI authentication working correctly for Unix hosts so as to be able to sign deliveries from cURL.\nFirst make sure you have a copy of your organisations CA certificate.\nCheck that the CA certificate works by running the following command:\necho \"Test\" | curl --cacert CA.crt --data-binary @- \"https://\u003cStroom_HOST\u003e/stroom/datafeed\"  If the response starts with the line:\ncurl: (60) SSL certificate problem, verify that the CA cert is OK.  then you do not have the correct CA certificate.\nIf the response contains the line\nHTTP Status 406 - Stroom Status 100 - Feed must be specified  then one-way SSL authentication using the CA certificate is successful.\nThe VBScript file to check windows certificates is check-certs.vbs (TODO link).\n#Final Testing\nOnce one-way authentication has been tested, two-way authentication should be configured:\nThe server certificate and private key should be concatenated to create a PEM file:\ncat hostname.cert hostname.key \u003e hostname.pem  Finally, test for 2-way authentication:\necho \"Test\" | curl --cacert CA.crt --cert hostname.pem --data-binary @- \"https://\u003cStroom_HOST\u003e/stroom/datafeed\"  If the response contains the line\nHTTP Status 406 - Stroom Status 100 - Feed must be specified  then two-way SSL authentication is successful.\n#Final Tidy Up\nThe files ca.crt and hostname.pem are the only files required for two-way authentication and should be stored permanently on the server; all other remaining files may be deleted or backed up if required.\n#Certificate Expiry\nPKI certificates expire after 2 years. To check the expiry date of a certificate, run the following command:\nopenssl x509 -in /path/to/certificate.pem -noout -enddate  This will give a response looking similar to:\nnotAfter=Aug 15 10:01:42 2013 GMT  ","categories":"","description":"Configuring SSL with cURL.\n","excerpt":"Configuring SSL with cURL.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/sending-data/ssl/","tags":"","title":"SSL Configuration"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/stream/","tags":"","title":"stream"},{"body":"Data within Stroom can be exported to a directory using the StreamDumpTool. The tool is contained within the core Stroom Java library and can be accessed via the command line, e.g.\njava -cp \"apache-tomcat-7.0.53/lib/*:lib/*:instance/webapps/stroom/WEB-INF/lib/*\" stroom.util.StreamDumpTool outputDir=output\nNote the classpath may need to be altered depending on your installation.\nThe above command will export all content from Stroom and output it to a directory called output. Data is exported to zip files in the same format as zip files in proxy repositories. The structure of the exported data is ${feed}/${pathId}/${id} by default with a .zip extension.\nTo provide greater control over what is exported and how the following additional parameters can be used:\nfeed - Specify the name of the feed to export data for (all feeds by default).\nstreamType - The single stream type to export (all stream types by default).\ncreatePeriodFrom - Exports data created after this time specified in ISO8601 UTC format, e.g. 2001-01-01T00:00:00.000Z (exports from earliest data by default).\ncreatePeriodTo - Exports data created before this time specified in ISO8601 UTC format, e.g. 2001-01-01T00:00:00.000Z (exports up to latest data by default).\noutputDir - The output directory to write data to (required).\nformat - The format of the output data directory and file structure (${feed}/${pathId}/${id} by default).\nFormat The format parameter can include several replacement variables:\nfeed - The name of the feed for the exported data.\nstreamType - The data type of the exported data, e.g. RAW_EVENTS.\nstreamId - The id of the data being exported.\npathId - A incrementing numeric id that creates sub directories when required to ensure no directory ends up containing too many files.\nid - A incrementing numeric id similar to pathId but without sub directories.\n","categories":"","description":"A tool for exporting stream data from Stroom.\n","excerpt":"A tool for exporting stream data from Stroom.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/tools/stream-dump-tool/","tags":"","title":"Stream Dump Tool"},{"body":"Streams can either be created when data is directly POSTed in to Stroom or during the proxy aggregation process. When data is directly POSTed to Stroom the content of the POST will be stored as one Stream. With proxy aggregation multiple files in the proxy repository will/can be aggregated together into a single Stream.\nAnatomy of a Stream A Stream is made up of a number of parts of which the raw or cooked data is just one. In addition to the data the Stream can contain a number of other child stream types, e.g. Context and Meta Data.\nThe hierarchy of a stream is as follows:\n Stream nnn  Part [1 to *]  Data [1-1] Context [0-1] Meta Data [0-1]      Although all streams conform to the above hierarchy there are three main types of Stream that are used in Stroom:\n Non-segmented Stream - Raw events, Raw Reference Segmented Stream - Events, Reference Segmented Error Stream - Error  Segmented means that the data has been demarcated into segments or records.\nChild Stream Types Data This is the actual data of the stream, e.g. the XML events, raw CSV, JSON, etc.\nContext This is additional contextual data that can be sent with the data. Context data can be used for reference data lookups.\nMeta Data This is the data about the Stream (e.g. the feed name, receipt time, user agent, etc.). This meta data either comes from the HTTP headers when the data was POSTed to Stroom or is added by Stroom or Stroom-Proxy on receipt/processing.\nNon-Segmented Stream The following is a representation of a non-segmented stream with three parts, each with Meta Data and Context child streams.\nNon-Segmented Stream    Raw Events and Raw Reference streams contain non-segmented data, e.g. a large batch of CSV, JSON, XML, etc. data. There is no notion of a record/event/segment in the data, it is simply data in any form (including malformed data) that is yet to be processed and demarcated into records/events, for example using a Data Splitter or an XML parser.\nThe Stream may be single-part or multi-part depending on how it is received. If it is the product of proxy aggregation then it is likely to be multi-part. Each part will have its own context and meta data child streams, if applicable.\nSegmented Stream The following is a representation of a segmented stream that contains three records (i.e events) and has Meta Data and Context child streams.\nSegmented Stream    Cooked Events and Reference data are forms of segmented data. The raw data has been parsed and split into records/events and the resulting data is stored in a way that allows Stroom to know where each record/event starts/ends. These streams only have a single part.\nError Stream Error Stream    Error streams are similar to segmented Event/Reference streams in that they are single-part and have demarcated records (where each error/warning/info message is a record). Error streams do not have any Meta Data or Context child streams.\n","categories":"","description":"The unit of data that Stroom operates on, essentially a bounded stream of data.\n","excerpt":"The unit of data that Stroom operates on, essentially a bounded stream …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/concepts/streams/","tags":["stream"],"title":"Streams"},{"body":"Concat Appends all the arguments end to end in a single string\nconcat(args...)  Example\nconcat('this ', 'is ', 'how ', 'it ', 'works') \u003e 'this is how it works'  Current User Returns the username of the user running the query.\ncurrentUser()  Example\ncurrentUser() \u003e 'jbloggs'  Decode The arguments are split into 3 parts\n The input value to test Pairs of regex matchers with their respective output value A default result, if the input doesn’t match any of the regexes  decode(input, test1, result1, test2, result2, ... testN, resultN, otherwise)  It works much like a Java Switch/Case statement\nExample\ndecode(${val}, 'red', 'rgb(255, 0, 0)', 'green', 'rgb(0, 255, 0)', 'blue', 'rgb(0, 0, 255)', 'rgb(255, 255, 255)') ${val}='blue' \u003e rgb(0, 0, 255) ${val}='green' \u003e rgb(0, 255, 0) ${val}='brown' \u003e rgb(255, 255, 255) // falls back to the 'otherwise' value  in Java, this would be equivalent to\nString decode(value) { switch(value) { case \"red\": return \"rgb(255, 0, 0)\" case \"green\": return \"rgb(0, 255, 0)\" case \"blue\": return \"rgb(0, 0, 255)\" default: return \"rgb(255, 255, 255)\" } }  decode('red') \u003e 'rgb(255, 0, 0)'  DecodeUrl Decodes a URL\ndecodeUrl('userId%3Duser1') \u003e userId=user1  EncodeUrl Encodes a URL\nencodeUrl('userId=user1') \u003e userId%3Duser1  Exclude If the supplied string matches one of the supplied match strings then return null, otherwise return the supplied string\nexclude(aString, match...)  Example\nexclude('hello', 'hello', 'hi') \u003e null exclude('hi', 'hello', 'hi') \u003e null exclude('bye', 'hello', 'hi') \u003e 'bye'  Hash Cryptographically hashes a string\nhash(value) hash(value, algorithm) hash(value, algorithm, salt)  Example\nhash(${val}, 'SHA-512', 'mysalt') \u003e A hashed result...  If not specified the hash() function will use the SHA-256 algorithm. Supported algorithms are determined by Java runtime environment.\nInclude If the supplied string matches one of the supplied match strings then return it, otherwise return null\ninclude(aString, match...)  Example\ninclude('hello', 'hello', 'hi') \u003e 'hello' include('hi', 'hello', 'hi') \u003e 'hi' include('bye', 'hello', 'hi') \u003e null  Index Of Finds the first position of the second string within the first\nindexOf(firstString, secondString)  Example\nindexOf('aa-bb-cc', '-') \u003e 2  Last Index Of Finds the last position of the second string within the first\nlastIndexOf(firstString, secondString)  Example\nlastIndexOf('aa-bb-cc', '-') \u003e 5  Lower Case Converts the string to lower case\nlowerCase(aString)  Example\nlowerCase('Hello DeVeLoPER') \u003e 'hello developer'  Match Test an input string using a regular expression to see if it matches\nmatch(input, regex)  Example\nmatch('this', 'this') \u003e true match('this', 'that') \u003e false  Query Param Returns the value of the requested query parameter.\nqueryParam(paramKey)  Examples\nqueryParam('user') \u003e 'jbloggs'  Query Params Returns all query parameters as a space delimited string.\nqueryParams()  Examples\nqueryParams() \u003e 'user=jbloggs site=HQ'  Replace Perform text replacement on an input string using a regular expression to match part (or all) of the input string and a replacement string to insert in place of the matched part\nreplace(input, regex, replacement)  Example\nreplace('this', 'is', 'at') \u003e 'that'  String Length Takes the length of a string\nstringLength(aString)  Example\nstringLength('hello') \u003e 5  Substring Take a substring based on start/end index of letters\nsubstring(aString, startIndex, endIndex)  Example\nsubstring('this', 1, 2) \u003e 'h'  Substring After Get the substring from the first string that occurs after the presence of the second string\nsubstringAfter(firstString, secondString)  Example\nsubstringAfter('aa-bb', '-') \u003e 'bb'  Substring Before Get the substring from the first string that occurs before the presence of the second string\nsubstringBefore(firstString, secondString)  Example\nsubstringBefore('aa-bb', '-') \u003e 'aa'  Upper Case Converts the string to upper case\nupperCase(aString)  Example\nupperCase('Hello DeVeLoPER') \u003e 'HELLO DEVELOPER'  ","categories":"","description":"Functions for manipulating strings (text data).\n","excerpt":"Functions for manipulating strings (text data).\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/expressions/string/","tags":"","title":"String Functions"},{"body":"There are various jobs that run in the background within Stroom. Among these are jobs that control pipeline processing, removing old files from the file system, checking the status of nodes and volumes etc. Each task executes at a different time depending on the purpose of the task. There are three ways that a task can be executed:\n Scheduled jobs execute periodically according to a cron schedule. These include jobs such as cleaning the file system where Stroom only needs to perform this action once a day and can do so overnight. Frequency controlled jobs are executed every X seconds, minutes, hours etc. Most of the jobs that execute with a given frequency are status checking jobs that perform a short lived action fairly frequently. Distributed jobs are only applicable to stream processing with a pipeline. Distributed jobs are executed by a worker node as soon as a worker has available threads to execute a jobs and the task distributor has work available.  A list of task types and their execution method can be seen by opening Monitoring/Jobs from the main menu.\n TODO: image\n Expanding each task type allows you to configure how a task behaves on each node:\n TODO: image\n Account Maintenance This job checks user accounts on the system and de-activates them under the following conditions:\n An unused account that has been inactive for longer than the age configured by stroom.security.identity.passwordPolicy.neverUsedAccountDeactivationThreshold. An account that has been inactive for longer than the age configured by stroom.security.identity.passwordPolicy.unusedAccountDeactivationThreshold.  Attribute Value Data Retention Deletes Meta attribute values (additional and less valuable metadata) older than stroom.data.meta.metaValue.deleteAge.\nData Delete Before data is physically removed from the database and file system it is marked as logically deleted by adding a flag to the metadata record in the database. Data can be logically deleted by a user from the UI or via a process such as data retention. Data is deleted logically as it is faster to do than a physical delete (important in the UI), and it also allows for data to be restored (undeleted) from the UI. This job performs the actual physical deletion of data that has been marked logically deleted for longer than the duration configured with stroom.data.store.deletePurgeAge. All data files associated with a metadata record are deleted from the file system before the metadata is physically removed from the database.\nData Processor Processes data by finding data that matches processing filters on each pipeline. When enabled, each worker node asks the master node for data processing tasks. The master node creates tasks based on processing filters added to the Processors screen of each pipeline and supplies them to the requesting workers.\nFeed Based Data Retention This job uses the retention property of each feed to logically delete data from the associated feed that is older than the retention period. The recommended way of specifying data retention rules is via the data retention policy feature, but feed based retention still exists for backwards compatibility. Feed based data retention will be removed in a future release and should be considered deprecated.\nFile System Clean (deprecated) This is the previous incarnation of the Data Delete job. This job scans the file system looking for files that are no longer associated with metadata in the database or where the metadata is marked as deleted and deletes the files if this is the case. The process is slow to run as it has to traverse all stored data files and examine each. However, this version of the data deletion process was created when metadata was deleted immediately, i.e. not marked for future physical deletion, so was the only way to perform this clean up activity at the time. This job will be removed in a future release. The Data Delete job should be used instead from now on.\nFile System Volume Status Scans your data volumes to ensure they are available and determines how much free space they have. Records this status in the Volume Status table.\nIndex Searcher Cache Refresh Refresh references to Lucene index searchers that have been cached for a while.\nIndex Shard Delete How frequently index shards that have been logically deleted are physically deleted from the file system.\nIndex Shard Retention How frequently index shards that are older then their retention period are logically deleted.\nIndex Volume Status Scans your index volumes to ensure they are available and determines how much free space they have. Records this status in the Index Volume Status table.\nIndex Writer Cache Sweep How frequently entries in the Index Shard Writer cache are evicted based on the time-to-live, time-to-idle and cache size settings.\nIndex Writer Flush How frequently in-memory changes to the index shards are flushed to the file system and committed to the index.\nJava Heap Histogram Statistics How frequently heap histogram statistics will be captured. This can be useful for diagnosing issues or seeing where memory is being used. Each run will result in a JVM pause so car should be taken when running this on a production system.\nNode Status How frequently we try write stats about node status including JVM and memory usage.\nPipeline Destination Roll How frequently rolling pipeline destinations, e.g. a Rolling File Appender are checked to see if they need to be rolled. This frequency should be at least as short as the most frequent rolling frequency.\nPolicy Based Data Retention Run the policy based data retention rules over the data and logically delete and data that should no longer be retained.\nProcessor Task Queue Statistics How frequently statistics about the state of the stream processing task queue are captured.\nProcessor Task Retention How frequently failed and completed tasks are checked to see if they are older than the delete age threshold set by stroom.processor.deleteAge. Any that are older than this threshold are deleted.\nProperty Cache Reload Stroom’s configuration properties can each be configured globally in the database. This job controls the frequency that each node refreshes the values of its properties cache from the global database values. See also Properties.\nProxy Aggregation If you front Stroom with an Stroom proxy which is configured to ‘store’ rather than ‘store/forward’, then this task when run will pick up all files in the proxy repository dir, aggregate them by feed and bring them into Stroom. It uses the system property stroom.proxyDir.\nQuery History Clean How frequently items in the query history are removed from the history if their age is older than stroom.history.daysRetention or if the number of items in the history exceeds stroom.history.itemsRetention.\nRef Data Off-heap Store Purge Purges all data older than the purge age defined by property stroom.pipeline.purgeAge. See also Reference Data.\nSolr Index Optimise How frequently Solr index segments are explicitly optimised by merging them into one.\nSolr Index Retention How frequently a process is run to delete items from the Solr indexes that don’t meet the retention rule of that index.\nSQL Stats Database Aggregation This job controls the frequency that the database statistics aggregation process is run. This process takes the entries in SQL_STAT_VAL_SRC and merges them into the main statistics tables SQL_STAT_KEY and SQL_STAT_KEY. As this process is reliant on data flushed by the SQL Stats In Memory Flush job it is advisable to schedule it to run after that, leaving some time for the in-memory flush to finish.\nSQL Stats In Memory Flush SQL Statistics are initially held and aggregated in memory. This job controls the frequency that the in memory statistics are flushed from the in memory buffer to the staging table SQL_STAT_VAL_SRC in the database.\n","categories":"","description":"Managing background jobs.\n","excerpt":"Managing background jobs.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/jobs/","tags":["job"],"title":"Stroom Jobs"},{"body":"","categories":"","description":"Various additional tools to assist in administering Stroom and accessing its data.\n","excerpt":"Various additional tools to assist in administering Stroom and …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/tools/","tags":"","title":"Tools"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/truststore/","tags":"","title":"truststore"},{"body":"Is Boolean Checks if the passed value is a boolean data type.\nisBoolean(arg1)  Examples:\nisBoolean(toBoolean('true')) \u003e true  Is Double Checks if the passed value is a double data type.\nisDouble(arg1)  Examples:\nisDouble(toDouble('1.2')) \u003e true  Is Error Checks if the passed value is an error caused by an invalid evaluation of an expression on passed values, e.g. some values passed to an expression could result in a divide by 0 error. Note that this method must be used to check for error as error equality using x=err() is not supported.\nisError(arg1)  Examples:\nisError(toLong('1')) \u003e false isError(err()) \u003e true  Is Integer Checks if the passed value is an integer data type.\nisInteger(arg1)  Examples:\nisInteger(toInteger('1')) \u003e true  Is Long Checks if the passed value is a long data type.\nisLong(arg1)  Examples:\nisLong(toLong('1')) \u003e true  Is Null Checks if the passed value is null. Note that this method must be used to check for null as null equality using x=null() is not supported.\nisNull(arg1)  Examples:\nisNull(toLong('1')) \u003e false isNull(null()) \u003e true  Is Number Checks if the passed value is a numeric data type.\nisNumber(arg1)  Examples:\nisNumber(toLong('1')) \u003e true  Is String Checks if the passed value is a string data type.\nisString(arg1)  Examples:\nisString(toString(1.2)) \u003e true  Is Value Checks if the passed value is a value data type, e.g. not null or error.\nisValue(arg1)  Examples:\nisValue(toLong('1')) \u003e true isValue(null()) \u003e false  Type Of Returns the data type of the passed value as a string.\ntypeOf(arg1)  Examples:\ntypeOf('abc') \u003e string typeOf(toInteger(123)) \u003e integer typeOf(err()) \u003e error typeOf(null()) \u003e null typeOf(toBoolean('false')) \u003e false  ","categories":"","description":"Functions for evaluating the type of a value.\n","excerpt":"Functions for evaluating the type of a value.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/expressions/type-checking/","tags":"","title":"Type Checking Functions"},{"body":"Fields containing a Uniform Resource Identifier (URI) in string form can queried to extract the URI’s individual components of authority, fragment, host, path, port, query, scheme, schemeSpecificPart and userInfo. See either RFC 2306: Uniform Resource Identifiers (URI): Generic Syntax or Java’s java.net.URI Class for details regarding the components. If any component is not present within the passed URI, then an empty string is returned.\nThe extraction functions are\n extractAuthorityFromUri() - extract the Authority component extractFragmentFromUri() - extract the Fragment component extractHostFromUri() - extract the Host component extractPathFromUri() - extract the Path component extractPortFromUri() - extract the Port component extractQueryFromUri() - extract the Query component extractSchemeFromUri() - extract the Scheme component extractSchemeSpecificPartFromUri() - extract the Scheme specific part component extractUserInfoFromUri() - extract the UserInfo component  If the URI is http://foo:bar@w1.superman.com:8080/very/long/path.html?p1=v1\u0026amp;p2=v2#more-details the table below displays the extracted components\n   Expression Extraction     extractAuthorityFromUri(${URI}) foo:bar@w1.superman.com:8080   extractFragmentFromUri(${URI}) more-details   extractHostFromUri(${URI}) w1.superman.com   extractPathFromUri(${URI}) /very/long/path.html   extractPortFromUri(${URI}) 8080   extractQueryFromUri(${URI}) p1=v1\u0026p2=v2   extractSchemeFromUri(${URI}) http   extractSchemeSpecificPartFromUri(${URI}) //foo:bar@w1.superman.com:8080/very/long/path.html?p1=v1\u0026p2=v2   extractUserInfoFromUri(${URI}) foo:bar    extractAuthorityFromUri Extracts the Authority component from a URI\nextractAuthorityFromUri(uri)\nExample\nextractAuthorityFromUri('http://foo:bar@w1.superman.com:8080/very/long/path.html?p1=v1\u0026p2=v2#more-details') \u003e 'foo:bar@w1.superman.com:8080'  extractFragmentFromUri Extracts the Fragment component from a URI\nextractFragmentFromUri(uri)\nExample\nextractFragmentFromUri('http://foo:bar@w1.superman.com:8080/very/long/path.html?p1=v1\u0026p2=v2#more-details') \u003e 'more-details'  extractHostFromUri Extracts the Host component from a URI\nextractHostFromUri(uri)\nExample\nextractHostFromUri('http://foo:bar@w1.superman.com:8080/very/long/path.html?p1=v1\u0026p2=v2#more-details') \u003e 'w1.superman.com'  extractPathFromUri Extracts the Path component from a URI\nextractPathFromUri(uri)\nExample\nextractPathFromUri('http://foo:bar@w1.superman.com:8080/very/long/path.html?p1=v1\u0026p2=v2#more-details') \u003e '/very/long/path.html'  extractPortFromUri Extracts the Port component from a URI\nextractPortFromUri(uri)\nExample\nextractPortFromUri('http://foo:bar@w1.superman.com:8080/very/long/path.html?p1=v1\u0026p2=v2#more-details') \u003e '8080'  extractQueryFromUri Extracts the Query component from a URI\nextractQueryFromUri(uri)\nExample\nextractQueryFromUri('http://foo:bar@w1.superman.com:8080/very/long/path.html?p1=v1\u0026p2=v2#more-details') \u003e 'p1=v1\u0026p2=v2'  extractSchemeFromUri Extracts the Scheme component from a URI\nextractSchemeFromUri(uri)\nExample\nextractSchemeFromUri('http://foo:bar@w1.superman.com:8080/very/long/path.html?p1=v1\u0026p2=v2#more-details') \u003e 'http'  extractSchemeSpecificPartFromUri Extracts the SchemeSpecificPart component from a URI\nextractSchemeSpecificPartFromUri(uri)\nExample\nextractSchemeSpecificPartFromUri('http://foo:bar@w1.superman.com:8080/very/long/path.html?p1=v1\u0026p2=v2#more-details') \u003e '//foo:bar@w1.superman.com:8080/very/long/path.html?p1=v1\u0026p2=v2'  extractUserInfoFromUri Extracts the UserInfo component from a URI\nextractUserInfoFromUri(uri)\nExample\nextractUserInfoFromUri('http://foo:bar@w1.superman.com:8080/very/long/path.html?p1=v1\u0026p2=v2#more-details') \u003e 'foo:bar'  ","categories":"","description":"Functions for extracting parts from a Uniform Resource Identifier (URI).\n","excerpt":"Functions for extracting parts from a Uniform Resource Identifier …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/expressions/uri/","tags":"","title":"URI Functions"},{"body":"Any \u003cgroup\u003e value or \u003cdata\u003e name and value can use references to matched content, but in addition to this it is possible just to output a known string, e.g.\n\u003cdata name=\"somename\" value=\"$\" /\u003e  The above example would output somename as the \u003cdata\u003e name attribute. This can often be useful where there are no headings specified in the input data but we want to associate certain names with certain values.\nGiven the following data:\n01/01/2010,00:00:00,192.168.1.100,SOMEHOST.SOMEWHERE.COM,user1,logon,  We could provide useful headings with the following configuration:\n\u003cregex pattern=\"([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),\"\u003e \u003cdata name=\"date\" value=\"$1\" /\u003e \u003cdata name=\"time\" value=\"$2\" /\u003e \u003cdata name=\"ipAddress\" value=\"$3\" /\u003e \u003cdata name=\"hostName\" value=\"$4\" /\u003e \u003cdata name=\"user\" value=\"$5\" /\u003e \u003cdata name=\"action\" value=\"$6\" /\u003e \u003c/regex\u003e  ","categories":"","description":"","excerpt":"Any \u003cgroup\u003e value or \u003cdata\u003e name and value can use references to …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-splitter/match-reference/3-3-use-of-fixed-strings/","tags":"","title":"Use of fixed strings"},{"body":"Err Returns err\nerr()  False Returns boolean false\nfalse()  Null Returns null\nnull()  True Returns boolean true\ntrue()  ","categories":"","description":"Functions that return a static value.\n","excerpt":"Functions that return a static value.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/dashboards/expressions/value/","tags":"","title":"Value Functions"},{"body":"Variables are added to Data Splitter configuration using the \u003cvar\u003e element, see variables. Each variable must have a unique id so that it can be referenced. References to variables have the form $VARIABLE_ID$, e.g.\n\u003cdata name=\"$heading$\" value=\"$\" /\u003e  Identification Data Splitter validates the configuration on load and ensures that all element ids are unique and that referenced ids belong to a variable.\nA variable will only store data if it is referenced so variables that are not referenced will do nothing. In addition to this a variable will only store data for match groups that are referenced, e.g. if $heading$1 is the only reference to a variable with an id of ‘heading’ then only data for match group 1 will be stored for reference lookup.\nScopes Variables have two scopes which affect how data is retrieved when referenced:\n Local scope Remote scope  Local Scope Variables are local to a reference if the reference exists as a descendant of the variables parent expression, e.g.\n\u003csplit delimiter=\"\\n\" \u003e \u003cvar id=\"line\" /\u003e \u003cgroup value=\"$1\"\u003e \u003cregex pattern=\"ip=([^ ]+) user=([^ ]+)\"\u003e \u003cdata name=\"line\" value=\"$line$\"/\u003e \u003cdata name=\"ip\" value=\"$1\"/\u003e \u003cdata name=\"user\" value=\"$2\"/\u003e \u003c/regex\u003e \u003c/group\u003e \u003c/split\u003e  In the above example, matches for the outermost \u003csplit\u003e expression are stored in the variable with the id of line. The only reference to this variable is in a data element that is a descendant of the variables parent expression \u003csplit\u003e, i.e. it is nested within split/group/regex.\nBecause the variable is referenced locally only the most recent parent match is relevant, i.e. no retrieval of values by iteration, iteration offset or fixed position is applicable. These features only apply to remote variables that store multiple values.\nRemote Scope The CSV example with a heading is an example of a variable being referenced from a remote scope.\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\"\u003e \u003c!-- Match heading line (note that maxMatch=\"1\" means that only the first line will be matched by this splitter) --\u003e \u003csplit delimiter=\"\\n\" maxMatch=\"1\"\u003e \u003c!-- Store each heading in a named list --\u003e \u003cgroup\u003e \u003csplit delimiter=\",\"\u003e \u003cvar id=\"heading\" /\u003e \u003c/split\u003e \u003c/group\u003e \u003c/split\u003e \u003c!-- Match each record --\u003e \u003csplit delimiter=\"\\n\"\u003e \u003c!-- Take the matched line --\u003e \u003cgroup value=\"$1\"\u003e \u003c!-- Split the line up --\u003e \u003csplit delimiter=\",\"\u003e \u003c!-- Output the stored heading for each iteration and the value from group 1 --\u003e \u003cdata name=\"$heading$1\" value=\"$1\" /\u003e \u003c/split\u003e \u003c/group\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  In the above example the parent expression of the variable is not the ancestor of the reference in the \u003cdata\u003e element. This makes the \u003cdata\u003e elements reference to the variable a remote one. In this situation the variable knows that it must store multiple values as the remote reference \u003cdata\u003e may retrieve one of many values from the variable based on:\n The match count of the parent expression. The match count of the parent expression, plus or minus an offset. A fixed position in the variable store.  Retrieval of value by iteration In the above example the first line is taken then repeatedly matched by delimiting with commas. This results in multiple values being stored in the ‘heading’ variable. Once this is done subsequent lines are matched and then also repeatedly matched by delimiting with commas in the same way the heading is.\nEach time a line is matched the internal match count of all sub expressions, (e.g. the \u003csplit\u003e expression that is delimited by comma) is reset to 0. Every time the sub \u003csplit\u003e expression matches up to a comma delimiter the match count is incremented. Any references to remote variables will, by default, use the current match count as an index to retrieve one of the many values stored in the variable. This means that the \u003cdata\u003e element in the above example will retrieve the corresponding heading for each value as the match count of the values will match the storage position of each heading.\nRetrieval of value by iteration offset In some cases there may be a mismatch between the position where a value is stored in a variable and the match count applicable when remotely referencing the variable.\nTake the following input:\nBAD,Date,Time,IPAddress,HostName,User,EventType,Detail 01/01/2010,00:00:00,192.168.1.100,SOMEHOST.SOMEWHERE.COM,user1,logon,  In the above example we can see that the first heading ‘BAD’ is not correct for the first value of every line. In this situation we could either adjust the way the heading line is parsed to ignore ‘BAD’ or just adjust the way the heading variable is referenced.\nTo make this adjustment the reference just needs to be told what offset to apply to the current match count to correctly retrieve the stored value. In the above example this would be done like this:\n\u003cdata name=\"$heading$1[+1]\" value=\"$1\" /\u003e  The above reference just uses the match count plus 1 to retrieve the stored value. Any integral offset plus or minus may be used, e.g. [+4] or [-10]. Offsets that result in a position that is outside of the storage range for the variable will not return a value.\nRetrieval of value by fixed position In addition to retrieval by offset from the current match count, a stored value can be returned by a fixed position that has no relevance to the current match count.\nIn the following example the value retrieved from the ‘heading’ variable will always be ‘IPAddress’ as this is the fourth value stored in the ‘heading’ variable and the position index starts at 0.\n\u003cdata name=\"$heading$1[3]\" value=\"$1\" /\u003e  ","categories":"","description":"","excerpt":"Variables are added to Data Splitter configuration using the \u003cvar\u003e …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-splitter/match-reference/3-2-variable-reference/","tags":"","title":"Variable reference"},{"body":"A variable is added to Data Splitter using the \u003cvar\u003e element. A variable is used to store matches from a parent expression for use in a reference elsewhere in the configuration, see variable reference.\nThe most recent matches are stored for use in local references, i.e. references that are in the same match scope as the variable. Multiple matches are stored for use in references that are in a separate match scope. The concept of different variable scopes is described in scopes.\nThe \u003cvar\u003e element The \u003cvar\u003e element is used to tell Data Splitter to store matches from a parent expression for use in a reference.\nAttributes The \u003cvar\u003e element has the following attributes:\n id  id Mandatory attribute used to uniquely identify it within the configuration (see id) and is the means by which a variable is referenced, e.g. $VAR_ID$.\n","categories":"","description":"","excerpt":"A variable is added to Data Splitter using the \u003cvar\u003e element. A …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/data-splitter/element-reference/2-3-variables/","tags":"","title":"Variables"},{"body":" TODO Describe volumes  ","categories":"","description":"Stroom's logical storage volumes for storing event and index data.\n","excerpt":"Stroom's logical storage volumes for storing event and index data.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/volumes/","tags":["volumes"],"title":"Volumes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/volumes/","tags":"","title":"volumes"},{"body":"Some input XML data may be missing an XML declaration and root level enclosing elements. This data is not a valid XML document and must be treated as an XML fragment. To use XML fragments the input type for a translation must be set to ‘XML Fragment’. A fragment wrapper must be defined in the XML conversion that tells Stroom what declaration and root elements to place around the XML fragment data.\nHere is an example:\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE records [ \u003c!ENTITY fragment SYSTEM \"fragment\"\u003e ]\u003e \u003crecords xmlns=\"records:2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"records:2 file://records-v2.0.xsd\" version=\"2.0\"\u003e \u0026fragment; \u003c/records\u003e  During conversion Stroom replaces the fragment text entity with the input XML fragment data. Note that XML fragments must still be well formed so that they can be parsed correctly.\n","categories":"","description":"Handling XML data without root level elements.\n","excerpt":"Handling XML data without root level elements.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/pipelines/parser/xml-fragments/","tags":"","title":"XML Fragments"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/xslt/","tags":"","title":"xslt"},{"body":"Once the text file has been converted into Intermediary XML (or the feed is already XML), XSLT is used to translate the XML into event logging XML format.\nEvent Feeds must be translated into the events schema and Reference into the reference schema. You can browse documentation relating to the schemas within the application.\nHere is an example XSLT:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003cxsl:stylesheet xmlns=\"event-logging:3\" xmlns:s=\"stroom\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"2.0\"\u003e \u003cxsl:template match=\"SomeData\"\u003e \u003cEvents xsi:schemaLocation=\"event-logging:3 file://event-logging-v3.0.0.xsd\" Version=\"3.0.0\"\u003e \u003cxsl:apply-templates/\u003e \u003c/Events\u003e \u003c/xsl:template\u003e \u003cxsl:template match=\"SomeEvent\"\u003e \u003cxsl:variable name=\"dateTime\" select=\"SomeTime\"/\u003e \u003cxsl:variable name=\"formattedDateTime\" select=\"s:format-date($dateTime, 'dd/MM/yyyyhh:mm:ss')\"/\u003e \u003cxsl:if test=\"SomeAction = 'OPEN'\"\u003e \u003cEvent\u003e \u003cEventTime\u003e \u003cTimeCreated\u003e \u003cxsl:value-of select=\"$formattedDateTime\"/\u003e \u003c/TimeCreated\u003e \u003c/EventTime\u003e \u003cEventSource\u003e \u003cSystem\u003eExample\u003c/System\u003e \u003cEnvironment\u003eExample\u003c/Environment\u003e \u003cGenerator\u003eVery Simple Provider\u003c/Generator\u003e \u003cDevice\u003e \u003cIPAddress\u003e3.3.3.3\u003c/IPAddress\u003e \u003c/Device\u003e \u003cUser\u003e \u003cId\u003e\u003cxsl:value-of select=\"SomeUser\"/\u003e\u003c/Id\u003e \u003c/User\u003e \u003c/EventSource\u003e \u003cEventDetail\u003e \u003cView\u003e \u003cDocument\u003e \u003cTitle\u003eUNKNOWN\u003c/Title\u003e \u003cFile\u003e \u003cPath\u003e\u003cxsl:value-of select=\"SomeFile\"/\u003e\u003c/Path\u003e \u003cFile\u003e \u003c/Document\u003e \u003c/View\u003e \u003c/EventDetail\u003e \u003c/Event\u003e \u003c/xsl:if\u003e \u003c/xsl:template\u003e \u003c/xsl:stylesheet\u003e  ","categories":"","description":"Using Extensible Stylesheet Language Transformations (XSLT) to transform data.\n","excerpt":"Using Extensible Stylesheet Language Transformations (XSLT) to …","ref":"/stroom-docs/hugo-docsy/docs/user-guide/pipelines/xslt/","tags":["xslt"],"title":"XSLT Conversion"},{"body":"By including the following namespace:\nxmlns:s=\"stroom\"  E.g.\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003cxsl:stylesheet xmlns=\"event-logging:3\" xmlns:s=\"stroom\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"2.0\"\u003e  The following functions are available to aid your translation:\n bitmap-lookup(String map, String key) - Bitmap based look up against reference data map using the period start time bitmap-lookup(String map, String key, String time) - Bitmap based look up against reference data map using a specified time, e.g. the event time bitmap-lookup(String map, String key, String time, Boolean ignoreWarnings) - Bitmap based look up against reference data map using a specified time, e.g. the event time, and ignore any warnings generated by a failed lookup bitmap-lookup(String map, String key, String time, Boolean ignoreWarnings, Boolean trace) - Bitmap based look up against reference data map using a specified time, e.g. the event time, and ignore any warnings generated by a failed lookup and get trace information for the path taken to resolve the lookup. classification() - The classification of the feed for the data being processed col-from() - The column in the input that the current record begins on (can be 0). col-to() - The column in the input that the current record ends at. current-time() - The current system time current-user() - The current user logged into Stroom (only relevant for interactive use, e.g. search) decode-url(String encodedUrl) - Decode the provided url. dictionary(String name) - Loads the contents of the named dictionary for use within the translation encode-url(String url) - Encode the provided url. feed-name() - Name of the feed for the data being processed format-date(String date, String pattern) - Format a date that uses the specified pattern using the default time zone format-date(String date, String pattern, String timeZone) - Format a date that uses the specified pattern with the specified time zone format-date(String date, String patternIn, String timeZoneIn, String patternOut, String timeZoneOut) - Parse a date with the specified input pattern and time zone and format the output with the specified output pattern and time zone format-date(String milliseconds) - Format a date that is specified as a number of milliseconds since a standard base time known as “the epoch”, namely January 1, 1970, 00:00:00 GMT get(String key) - Returns the value associated with a key that has been stored using put() hash(String value) - Hash a string value using the default SHA-256 algorithm and no salt hash(String value, String algorithm, String salt) - Hash a string value using the specified hashing algorithm and supplied salt value. Supported hashing algorithms include SHA-256, SHA-512, MD5. hex-to-dec(String hex) - Convert hex to dec representation hex-to-oct(String hex) - Convert hex to oct representation json-to-xml(String json) - Returns an XML representation of the supplied JSON value for use in XPath expressions line-from() - The line in the input that the current record begins on (1 based). line-to() - The line in the input that the current record ends at. link(String url) - Creates a stroom dashboard table link. link(String title, String url) - Creates a stroom dashboard table link. link(String title, String url, String type) - Creates a stroom dashboard table link. log(String severity, String message) - Logs a message to the processing log with the specified severity lookup(String map, String key) - Look up a reference data map using the period start time lookup(String map, String key, String time) - Look up a reference data map using a specified time, e.g. the event time lookup(String map, String key, String time, Boolean ignoreWarnings) - Look up a reference data map using a specified time, e.g. the event time, and ignore any warnings generated by a failed lookup lookup(String map, String key, String time, Boolean ignoreWarnings, Boolean trace) - Look up a reference data map using a specified time, e.g. the event time, ignore any warnings generated by a failed lookup and get trace information for the path taken to resolve the lookup. meta(String key) - Lookup a meta data value for the current stream using the specified key. The key can be Feed, StreamType, CreatedTime, EffectiveTime, Pipeline or any other attribute supplied when the stream was sent to Stroom, e.g. meta(‘System’). numeric-ip(String ipAddress) - Convert an IP address to a numeric representation for range comparison part-no() - The current part within a multi part aggregated input stream (AKA the substream number) (1 based) parse-uri(String URI) - Returns an XML structure of the URI providing authority, fragment, host, path, port, query, scheme, schemeSpecificPart, and userInfo components if present. random() - Get a system generated random number between 0 and 1. record-no() - The current record number within the current part (substream) (1 based). search-id() - Get the id of the batch search when a pipeline is processing as part of a batch search source() - Returns an XML structure with the stroom-meta namespace detailing the current source location. source-id() - Get the id of the current input stream that is being processed stream-id() - An alias for source-id included for backward compatibility. pipeline-name() - Name of the current processing pipeline using the XSLT put(String key, String value) - Store a value for use later on in the translation  bitmap-lookup() The bitmap-lookup() function looks up a bitmap key from reference or context data a value (which can be an XML node set) for each set bit position and adds it to the resultant XML.\nbitmap-lookup(String map, String key) bitmap-lookup(String map, String key, String time) bitmap-lookup(String map, String key, String time, Boolean ignoreWarnings) bitmap-lookup(String map, String key, String time, Boolean ignoreWarnings, Boolean trace)   map - The name of the reference data map to perform the lookup against. key - The bitmap value to lookup. This can either be represented as a decimal integer (e.g. 14) or as hexadecimal by prefixing with 0x (e.g 0xE). time - Determines which set of reference data was effective at the requested time. If no reference data exists with an effective time before the requested time then the lookup will fail. Time is in the format yyyy-MM-dd'T'HH:mm:ss.SSSXX, e.g. 2010-01-01T00:00:00.000Z. ignoreWarnings - If true, any lookup failures will be ignored, else they will be reported as warnings. trace - If true, additional trace information is output as INFO messages.  If the look up fails no result will be returned.\nThe key is a bitmap expressed as either a decimal integer or a hexidecimal value, e.g. 14/0xE is 1110 as a binary bitmap. For each bit position that is set, (i.e. has a binary value of 1) a lookup will be performed using that bit position as the key. In this example, positions 1, 2 \u0026 3 are set so a lookup would be performed for these bit positions. The result of each lookup for the bitmap are concatenated together in bit position order, separated by a space.\nIf ignoreWarnings is true then any lookup failures will be ignored and it will return the value(s) for the bit positions it was able to lookup.\nThis function can be useful when you have a set of values that can be represented as a bitmap and you need them to be converted back to individual values. For example if you have a set of additive account permissions (e.g Admin, ManageUsers, PerformExport, etc.), each of which is associated with a bit position, then a user’s permissions could be defined as a single decimal/hex bitmap value. Thus a bitmap lookup with this value would return all the permissions held by the user.\nFor example the reference data store may contain:\n   Key (Bit position) Value     0 Administrator   1 Manage_Users   2 Perform_Export   3 View_Data   4 Manage_Jobs   5 Delete_Data   6 Manage_Volumes    The following are example lookups using the above reference data:\n   Lookup Key (decimal) Lookup Key (Hex) Bitmap Result     0 0x0 0000000 -   1 0x1 0000001 Administrator   74 0x4A 1001010 Manage_Users View_Data Manage_Volumes   2 0x2 0000010 Manage_Users   96 0x60 1100000 Delete_Data ManageUsers    dictionary() The dictionary() function gets the contents of the specified dictionary for use during translation. The main use for this function is to allow users to abstract the management of a set of keywords from the XSLT so that it is easier for some users to make quick alterations to a dictionary that is used by some XSLT, without the need for the user to understand the complexities of XSLT.\nformat-date() The format-date() function takes a Pattern and optional TimeZone arguments and replaces the parsed contents with an XML standard Date Format. The pattern must be a Java based SimpleDateFormat. If the optional TimeZone argument is present the pattern must not include the time zone pattern tokens (z and Z). A special time zone value of “GMT/BST” can be used to guess the time based on the date (BST during British Summer Time).\nE.g. Convert a GMT date time “2009/12/01 12:34:11”\n\u003cxsl:value-of select=\"s:format-date('2009/08/01 12:34:11', 'yyyy/MM/dd HH:mm:ss')\"/\u003e  E.g. Convert a GMT or BST date time “2009/08/01 12:34:11”\n\u003cxsl:value-of select=\"s:format-date('2009/08/01 12:34:11', 'yyyy/MM/dd HH:mm:ss', 'GMT/BST')\"/\u003e  E.g. Convert a GMT+1:00 date time “2009/08/01 12:34:11”\n\u003cxsl:value-of select=\"s:format-date('2009/08/01 12:34:11', 'yyyy/MM/dd HH:mm:ss', 'GMT+1:00')\"/\u003e  E.g. Convert a date time specified as milliseconds since the epoch “1269270011640”\n\u003cxsl:value-of select=\"s:format-date('1269270011640')\"/\u003e  Time Zone Must be as per the rules defined in SimpleDateFormat under General Time Zone syntax.\nlink() Create a string that represents a hyperlink for display in a dashboard table.\nlink(url) link(title, url) link(title, url, type)  Example\nlink('http://www.somehost.com/somepath') \u003e [http://www.somehost.com/somepath](http://www.somehost.com/somepath) link('Click Here','http://www.somehost.com/somepath') \u003e [Click Here](http://www.somehost.com/somepath) link('Click Here','http://www.somehost.com/somepath', 'dialog') \u003e [Click Here](http://www.somehost.com/somepath){dialog} link('Click Here','http://www.somehost.com/somepath', 'dialog|Dialog Title') \u003e [Click Here](http://www.somehost.com/somepath){dialog|Dialog Title}  Type can be one of:\n dialog : Display the content of the link URL within a stroom popup dialog. tab : Display the content of the link URL within a stroom tab. browser : Display the content of the link URL within a new browser tab. dashboard : Used to launch a stroom dashboard internally with parameters in the URL.  If you wish to override the default title or URL of the target link in either a tab or dialog you can. Both dialog and tab types allow titles to be specified after a |, e.g. dialog|My Title.\nlog() The log() function writes a message to the processing log with the specified severity. Severities of INFO, WARN, ERROR and FATAL can be used. Severities of ERROR and FATAL will result in records being omitted from the output if a RecordOutputFilter is used in the pipeline. The counts for RecWarn, RecError will be affected by warnings or errors generated in this way therefore this function is useful for adding business rules to XML output.\nE.g. Warn if a SID is not the correct length.\n\u003cxsl:if test=\"string-length($sid) != 7\"\u003e \u003cxsl:value-of select=\"s:log('WARN', concat($sid, ' is not the correct length'))\"/\u003e \u003c/xsl:if\u003e  lookup() The lookup() function looks up from reference or context data a value (which can be an XML node set) and adds it to the resultant XML.\nlookup(String map, String key) lookup(String map, String key, String time) lookup(String map, String key, String time, Boolean ignoreWarnings) lookup(String map, String key, String time, Boolean ignoreWarnings, Boolean trace)   map - The name of the reference data map to perform the lookup against. key - The key to lookup. The key can be a simple string, an integer value in a numeric range or a nested lookup key. time - Determines which set of reference data was effective at the requested time. If no reference data exists with an effective time before the requested time then the lookup will fail. Time is in the format yyyy-MM-dd'T'HH:mm:ss.SSSXX, e.g. 2010-01-01T00:00:00.000Z. ignoreWarnings - If true, any lookup failures will be ignored, else they will be reported as warnings. trace - If true, additional trace information is output as INFO messages.  If the look up fails no result will be returned. By testing the result a default value may be output if no result is returned.\nE.g. Look up a SID given a PF\n\u003cxsl:variable name=\"pf\" select=\"PFNumber\"/\u003e \u003cxsl:if test=\"$pf\"\u003e \u003cxsl:variable name=\"sid\" select=\"s:lookup('PF_TO_SID', $pf, $formattedDateTime)\"/\u003e \u003cxsl:choose\u003e \u003cxsl:when test=\"$sid\"\u003e \u003cUser\u003e \u003cId\u003e\u003cxsl:value-of select=\"$sid\"/\u003e\u003c/Id\u003e \u003c/User\u003e \u003c/xsl:when\u003e \u003cxsl:otherwise\u003e \u003cdata name=\"PFNumber\"\u003e \u003cxsl:attribute name=\"Value\"\u003e\u003cxsl:value-of select=\"$pf\"/\u003e\u003c/xsl:attribute\u003e \u003c/data\u003e \u003c/xsl:otherwise\u003e \u003c/xsl:choose\u003e \u003c/xsl:if\u003e  Range lookups Reference data entries can either be stored with single string key or a key range that defines a numeric range, e.g 1-100. When a lookup is preformed the passed key is looked up as if it were a normal string key. If that lookup fails Stroom will try to convert the key to an integer (long) value. If it can be converted to an integer than a second lookup will be performed against entries with key ranges to see if there is a key range that includes the requested key.\nRange lookups can be used for looking up an IP address where the reference data values are associated with ranges of IP addresses. In this use case, the IP address must first be converted into a numeric value using numeric-ip(), e.g:\nstroom:lookup('IP_TO_LOCATION', numeric-ip($ipAddress))  Similarly the reference data must be stored with key ranges whose bounds were created using this function.\nNested Maps The lookup function allows you to perform chained lookups using nested maps. For example you may have a reference data map called USER_ID_TO_LOCATION that maps user IDs to some location information for that user and a map called USER_ID_TO_MANAGER that maps user IDs to the user ID of their manager. If you wanted to decorate a user’s event with the location of their manager you could use a nested map to achieve the lookup chain. To perform the lookup set the map argument to the list of maps in the lookup chain, separated by a /, e.g. USER_ID_TO_MANAGER/USER_ID_TO_LOCATION.\nThis will perform a lookup against the first map in the list using the requested key. If a value is found the value will be used as the key in a lookup against the next map. The value from each map lookup is used as the key in the next map all the way down the chain. The value from the last lookup is then returned as the result of the lookup() call. If no value is found at any point in the chain then that results in no value being returned from the function.\nIn order to use nested map lookups each intermediate map must contain simple string values. The last map in the chain can either contain string values or XML fragment values.\nput() and get() You can put values into a map using the put() function. These values can then be retrieved later using the get() function. Values are stored against a key name so that multiple values can be stored. These functions can be used for many purposes but are most commonly used to count a number of records that meet certain criteria.\nAn example of how to count records is shown below:\n\u003c!-- Get the current record count --\u003e \u003cxsl:variable name=\"currentCount\" select=\"number(s:get('count'))\" /\u003e \u003c!-- Increment the record count --\u003e \u003cxsl:variable name=\"count\"\u003e \u003cxsl:choose\u003e \u003cxsl:when test=\"$currentCount\"\u003e \u003cxsl:value-of select=\"$currentCount + 1\" /\u003e \u003c/xsl:when\u003e \u003cxsl:otherwise\u003e \u003cxsl:value-of select=\"1\" /\u003e \u003c/xsl:otherwise\u003e \u003c/xsl:choose\u003e \u003c/xsl:variable\u003e \u003c!-- Store the count for future retrieval --\u003e \u003cxsl:value-of select=\"s:put('count', $count)\" /\u003e \u003c!-- Output the new count --\u003e \u003cdata name=\"Count\"\u003e \u003cxsl:attribute name=\"Value\" select=\"$count\" /\u003e \u003c/data\u003e  parse-uri() The parse-uri() function takes a Uniform Resource Identifier (URI) in string form and returns an XML node with a namespace of uri containing the URI’s individual components of authority, fragment, host, path, port, query, scheme, schemeSpecificPart and userInfo. See either RFC 2306: Uniform Resource Identifiers (URI): Generic Syntax or Java’s java.net.URI Class for details regarding the components.\nThe following xml\n\u003c!-- Display and parse the URI contained within the text of the rURI element --\u003e \u003cxsl:variable name=\"u\" select=\"s:parseUri(rURI)\" /\u003e \u003cURI\u003e \u003cxsl:value-of select=\"rURI\" /\u003e \u003c/URI\u003e \u003cURIDetail\u003e \u003cxsl:copy-of select=\"$v\"/\u003e \u003c/URIDetail\u003e  given the rURI text contains\n http://foo:bar@w1.superman.com:8080/very/long/path.html?p1=v1\u0026amp;p2=v2#more-details  would provide\n\u003cURL\u003ehttp://foo:bar@w1.superman.com:8080/very/long/path.html?p1=v1\u0026amp;p2=v2#more-details\u003c/URL\u003e \u003cURIDetail\u003e \u003cauthority xmlns=\"uri\"\u003efoo:bar@w1.superman.com:8080\u003c/authority\u003e \u003cfragment xmlns=\"uri\"\u003emore-details\u003c/fragment\u003e \u003chost xmlns=\"uri\"\u003ew1.superman.com\u003c/host\u003e \u003cpath xmlns=\"uri\"\u003e/very/long/path.html\u003c/path\u003e \u003cport xmlns=\"uri\"\u003e8080\u003c/port\u003e \u003cquery xmlns=\"uri\"\u003ep1=v1\u0026amp;p2=v2\u003c/query\u003e \u003cscheme xmlns=\"uri\"\u003ehttp\u003c/scheme\u003e \u003cschemeSpecificPart xmlns=\"uri\"\u003e//foo:bar@w1.superman.com:8080/very/long/path.html?p1=v1\u0026amp;p2=v2\u003c/schemeSpecificPart\u003e \u003cuserInfo xmlns=\"uri\"\u003efoo:bar\u003c/userInfo\u003e \u003c/URIDetail\u003e  ","categories":"","description":"Custom XSLT functions available in Stroom.\n","excerpt":"Custom XSLT functions available in Stroom.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/pipelines/xslt/xslt-functions/","tags":["xslt"],"title":"XSLT Functions"},{"body":"You can use an XSLT import to include XSLT from another translation. E.g.\n\u003cxsl:import href=\"ApacheAccessCommon\" /\u003e  This would include the XSLT from the ApacheAccessCommon translation.\n","categories":"","description":"Using an XSLT import to include XSLT from another translation.\n","excerpt":"Using an XSLT import to include XSLT from another translation.\n","ref":"/stroom-docs/hugo-docsy/docs/user-guide/pipelines/xslt/xslt-includes/","tags":"","title":"XSLT Includes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/style/","tags":"","title":"style"},{"body":"","categories":"","description":"As yet unreleased key features and changes.\n","excerpt":"As yet unreleased key features and changes.\n","ref":"/stroom-docs/hugo-docsy/news/releases/unreleased/","tags":"","title":"Unreleased"},{"body":"Assumptions The following assumptions are used in this document.\n the user has reasonable RHEL/Centos System administration skills installations are on Centos 7.3 minimal systems (fully patched) the security of the HTTPD deployment should be reviewed for a production environment.  Installation of Apache httpd and Mod_JK Software To deploy Stroom using Apache’s httpd web service as a front end (https) and Apache’s mod_jk as the interface between Apache and the Stroom tomcat applications, we also need\n apr apr-util apr-devel gcc httpd httpd-devel mod_ssl epel-release tomcat-native apache’s mod_jk tomcat connector plugin  Most of the required software are packages available via standard repositories and hence we can simply execute\nsudo yum -y install apr apr-util apr-devel gcc httpd httpd-devel mod_ssl epel-release sudo yum -y install tomcat-native  The reason for the distinct tomcat-native installation is that this package is from the EPEL (external link) repository so it must be installed first.\nFor the Apache mod_jk Tomcat connector we need to acquire a recent release (external link) and install it. The following commands achieve this for the 1.2.42 release.\nsudo bash cd /tmp V=1.2.42 wget https://www.apache.org/dist/tomcat/tomcat-connectors/jk/tomcat-connectors-${V}-src.tar.gz tar xf tomcat-connectors-${V}-src.tar.gz cd tomcat-connectors-*-src/native ./configure --with-apxs=/bin/apxs make \u0026\u0026 make install cd /tmp rm -rf tomcat-connectors-*-src  Although you could remove the gcc compiler at this point, we leave it installed as one should continue to upgrade the Tomcat Connectors to later releases.\nConfigure Apache httpd We need to configure Apache as the root user.\nIf the Apache httpd service is ‘fronting’ a Stroom user interface, we should create an index file (/var/www/html/index.html) on all nodes so browsing to the root of the node will present the Stroom UI. This is not needed if you are deploying a Forwarding or Standalone Stroom proxy.\nForwarding file for Stroom User Interface deployments F=/var/www/html/index.html printf '\u003chtml\u003e\\n' \u003e ${F} printf '\u003chead\u003e\\n' \u003e\u003e ${F} printf ' \u003cmeta http-equiv=\"Refresh\" content=\"0; URL=stroom\"/\u003e\\n' \u003e\u003e ${F} printf '\u003c/head\u003e\\n' \u003e\u003e ${F} printf '\u003c/html\u003e\\n' \u003e\u003e ${F} chmod 644 ${F}  Remember, deploy this file on all nodes running the Stroom Application.\nHttpd.conf Configuration We modify /etc/httpd/conf/httpd.conf on all nodes, but backup the file first with\ncp /etc/httpd/conf/httpd.conf /etc/httpd/conf/httpd.conf.ORIG  Irrespective of the Stroom scenario being deployed - Multi Node Stroom (Application and Proxy), single Standalone Stroom Proxy or single Forwarding Stroom Proxy, the configuration of the /etc/httpd/conf/httpd.conf file is the same.\nWe start by modify the configuration file by, add just before the ServerRoot directive the following directives which are designed to make the httpd service more secure.\n# Stroom Change: Start - Apply generic security directives ServerTokens Prod ServerSignature Off FileETag None RewriteEngine On RewriteCond %{THE_REQUEST} !HTTP/1.1$ RewriteRule .* - [F] Header set X-XSS-Protection \"1; mode=block\" # Stroom Change: End  That is,\n... # Do not add a slash at the end of the directory path. If you point # ServerRoot at a non-local disk, be sure to specify a local disk on the # Mutex directive, if file-based mutexes are used. If you wish to share the # same ServerRoot for multiple httpd daemons, you will need to change at # least PidFile. # ServerRoot \"/etc/httpd\" # # Listen: Allows you to bind Apache to specific IP addresses and/or ...  becomes\n... # Do not add a slash at the end of the directory path. If you point # ServerRoot at a non-local disk, be sure to specify a local disk on the # Mutex directive, if file-based mutexes are used. If you wish to share the # same ServerRoot for multiple httpd daemons, you will need to change at # least PidFile. # # Stroom Change: Start - Apply generic security directives ServerTokens Prod ServerSignature Off FileETag None RewriteEngine On RewriteCond %{THE_REQUEST} !HTTP/1.1$ RewriteRule .* - [F] Header set X-XSS-Protection \"1; mode=block\" # Stroom Change: End ServerRoot \"/etc/httpd\" # # Listen: Allows you to bind Apache to specific IP addresses and/or ...  We now block access to the /var/www directory by commenting out\n\u003cDirectory \"/var/www\"\u003e AllowOverride None # Allow open access: Require all granted \u003c/Directory\u003e  that is\n... # # Relax access to content within /var/www. # \u003cDirectory \"/var/www\"\u003e AllowOverride None # Allow open access: Require all granted \u003c/Directory\u003e # Further relax access to the default document root: ...  becomes\n... # # Relax access to content within /var/www. # # Stroom Change: Start - Block access to /var/www # \u003cDirectory \"/var/www\"\u003e # AllowOverride None # # Allow open access: # Require all granted # \u003c/Directory\u003e # Stroom Change: End # Further relax access to the default document root: ...  then within the /var/www/html directory turn off Indexes FollowSymLinks by commenting out the line\nOptions Indexes FollowSymLinks  That is\n... # The Options directive is both complicated and important. Please see # http://httpd.apache.org/docs/2.4/mod/core.html#options # for more information. # Options Indexes FollowSymLinks # # AllowOverride controls what directives may be placed in .htaccess files. # It can be \"All\", \"None\", or any combination of the keywords: ...  becomes\n... # The Options directive is both complicated and important. Please see # http://httpd.apache.org/docs/2.4/mod/core.html#options # for more information. # # Stroom Change: Start - turn off indexes and FollowSymLinks # Options Indexes FollowSymLinks # Stroom Change: End # # AllowOverride controls what directives may be placed in .htaccess files. # It can be \"All\", \"None\", or any combination of the keywords: ...  Then finally we add two new log formats and configure the access log to use the new format. This is done within the \u003cIfModule logio_module\u003e by adding the two new LogFormat directives\nLogFormat \"%a/%{REMOTE_PORT}e %X %t %l \\\"%u\\\" \\\"%r\\\" %s/%\u003es %D %I/%O/%B \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %V/%p\" blackboxUser LogFormat \"%a/%{REMOTE_PORT}e %X %t %l \\\"%{SSL_CLIENT_S_DN}x\\\" \\\"%r\\\" %s/%\u003es %D %I/%O/%B \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %V/%p\" blackboxSSLUser  and replacing the CustomLog directive\nCustomLog \"logs/access_log\" combined  with\nCustomLog logs/access_log blackboxSSLUser  That is\n... LogFormat \"%h %l %u %t \\\"%r\\\" %\u003es %b\" common \u003cIfModule logio_module\u003e # You need to enable mod_logio.c to use %I and %O LogFormat \"%h %l %u %t \\\"%r\\\" %\u003es %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %I %O\" combinedio \u003c/IfModule\u003e # # The location and format of the access logfile (Common Logfile Format). # If you do not define any access logfiles within a \u003cVirtualHost\u003e # container, they will be logged here. Contrariwise, if you *do* # define per-\u003cVirtualHost\u003e access logfiles, transactions will be # logged therein and *not* in this file. # #CustomLog \"logs/access_log\" common # # If you prefer a logfile with access, agent, and referer information # (Combined Logfile Format) you can use the following directive. # CustomLog \"logs/access_log\" combined \u003c/IfModule\u003e ...  becomes\n... LogFormat \"%h %l %u %t \\\"%r\\\" %\u003es %b\" common \u003cIfModule logio_module\u003e # You need to enable mod_logio.c to use %I and %O LogFormat \"%h %l %u %t \\\"%r\\\" %\u003es %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %I %O\" combinedio # Stroom Change: Start - Add new logformats LogFormat \"%a/%{REMOTE_PORT}e %X %t %l \\\"%u\\\" \\\"%r\\\" %s/%\u003es %D %I/%O/%B \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %V/%p\" blackboxUser LogFormat \"%a/%{REMOTE_PORT}e %X %t %l \\\"%{SSL_CLIENT_S_DN}x\\\" \\\"%r\\\" %s/%\u003es %D %I/%O/%B \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %V/%p\" blackboxSSLUser # Stroom Change: End \u003c/IfModule\u003e # Stroom Change: Start - Add new logformats without the additional byte values \u003cIfModule !logio_module\u003e LogFormat \"%a/%{REMOTE_PORT}e %X %t %l \\\"%u\\\" \\\"%r\\\" %s/%\u003es %D 0/0/%B \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %V/%p\" blackboxUser LogFormat \"%a/%{REMOTE_PORT}e %X %t %l \\\"%{SSL_CLIENT_S_DN}x\\\" \\\"%r\\\" %s/%\u003es %D 0/0/%B \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %V/%p\" blackboxSSLUser \u003c/IfModule\u003e # Stroom Change: End # # The location and format of the access logfile (Common Logfile Format). # If you do not define any access logfiles within a \u003cVirtualHost\u003e # container, they will be logged here. Contrariwise, if you *do* # define per-\u003cVirtualHost\u003e access logfiles, transactions will be # logged therein and *not* in this file. # #CustomLog \"logs/access_log\" common # # If you prefer a logfile with access, agent, and referer information # (Combined Logfile Format) you can use the following directive. # # Stroom Change: Start - Make the access log use a new format # CustomLog \"logs/access_log\" combined CustomLog logs/access_log blackboxSSLUser # Stroom Change: End \u003c/IfModule\u003e ...  Remember, deploy this file on all nodes.\nConfiguration of ssl.conf We modify /etc/httpd/conf.d/ssl.conf on all nodes, backing up first,\ncp /etc/httpd/conf.d/ssl.conf /etc/httpd/conf.d/ssl.conf.ORIG  The configuration of the /etc/httpd/conf.d/ssl.conf does change depending on the Stroom scenario deployed. In the following we will indicate differences by tagged sub-headings. If the configuration applies irrespective of scenario, then All scenarios is the tag, else the tag indicated the type of Stroom deployment.\nssl.conf: HTTP to HTTPS Redirection - All scenarios Before the context we add http to https redirection by adding the directives (noting we specify the actual server name)\n\u003cVirtualHost *:80\u003e ServerName stroomp00.strmdev00.org Redirect permanent \"/\" \"https://stroomp00.strmdev00.org/\" \u003c/VirtualHost\u003e  That is, we change\n... ## SSL Virtual Host Context ## \u003cVirtualHost _default_:443\u003e ...  to\n... ## SSL Virtual Host Context ## # Stroom Change: Start - Add http redirection to https \u003cVirtualHost *:80\u003e ServerName stroomp00.strmdev00.org Redirect permanent \"/\" \"https://stroomp00.strmdev00.org/\" \u003c/VirtualHost\u003e # Stroom Change: End \u003cVirtualHost _default_:443\u003e  ssl.conf: VirtualHost directives - Multi Node ‘Application and Proxy’ deployment Within the context we set the directives, in this case, we use the CNAME stroomp.strmdev00.org\nServerName stroomp.strmdev00.org JkMount /stroom* loadbalancer JkMount /stroom/remoting/cluster* local JkMount /stroom/datafeed* loadbalancer_proxy JkMount /stroom/remoting* loadbalancer_proxy JkMount /stroom/datafeed/direct* loadbalancer JkOptions +ForwardKeySize +ForwardURICompat +ForwardSSLCertChain -ForwardDirectories  That is, we change\n... \u003cVirtualHost _default_:443\u003e # General setup for the virtual host, inherited from global configuration #DocumentRoot \"/var/www/html\" #ServerName www.example.com:443 # Use separate log files for the SSL virtual host; note that LogLevel # is not inherited from httpd.conf. ...  to\n... \u003cVirtualHost _default_:443\u003e # General setup for the virtual host, inherited from global configuration #DocumentRoot \"/var/www/html\" #ServerName www.example.com:443 # Stroom Change: Start - Set servername and mod_jk connectivity ServerName stroomp.strmdev00.org JkMount /stroom* loadbalancer JkMount /stroom/remoting/cluster* local JkMount /stroom/datafeed* loadbalancer_proxy JkMount /stroom/remoting* loadbalancer_proxy JkMount /stroom/datafeed/direct* loadbalancer JkOptions +ForwardKeySize +ForwardURICompat +ForwardSSLCertChain -ForwardDirectories # Stroom Change: End # Use separate log files for the SSL virtual host; note that LogLevel # is not inherited from httpd.conf. ...  ssl.conf: VirtualHost directives - Standalone or Forwarding Proxy deployment Within the context set the directives, in this case, for a node named say stroomfp0.strmdev00.org\nServerName stroomfp0.strmdev00.org JkMount /stroom/datafeed* local_proxy JkOptions +ForwardKeySize +ForwardURICompat +ForwardSSLCertChain -ForwardDirectories  That is, we change\n... \u003cVirtualHost _default_:443\u003e # General setup for the virtual host, inherited from global configuration #DocumentRoot \"/var/www/html\" #ServerName www.example.com:443 # Use separate log files for the SSL virtual host; note that LogLevel # is not inherited from httpd.conf. ...  to\n... \u003cVirtualHost _default_:443\u003e # General setup for the virtual host, inherited from global configuration #DocumentRoot \"/var/www/html\" #ServerName www.example.com:443 # Stroom Change: Start - Set servername and mod_jk connectivity ServerName stroomfp0.strmdev00.org JkMount /stroom/datafeed* local_proxy JkOptions +ForwardKeySize +ForwardURICompat +ForwardSSLCertChain -ForwardDirectories # Stroom Change: End # Use separate log files for the SSL virtual host; note that LogLevel # is not inherited from httpd.conf. ...  ssl.conf: VirtualHost directives - Single Node ‘Application and Proxy’ deployment Within the context set the directives, in this case, for a node name stroomp00.strmdev00.org\nServerName stroomp00.strmdev00.org JkMount /stroom* local JkMount /stroom/remoting/cluster* local JkMount /stroom/datafeed* local_proxy JkMount /stroom/remoting* local_proxy JkMount /stroom/datafeed/direct* local JkOptions +ForwardKeySize +ForwardURICompat +ForwardSSLCertChain -ForwardDirectories  That is, we change\n... \u003cVirtualHost _default_:443\u003e # General setup for the virtual host, inherited from global configuration #DocumentRoot \"/var/www/html\" #ServerName www.example.com:443 # Use separate log files for the SSL virtual host; note that LogLevel # is not inherited from httpd.conf. ...  to\n... \u003cVirtualHost _default_:443\u003e # General setup for the virtual host, inherited from global configuration #DocumentRoot \"/var/www/html\" #ServerName www.example.com:443 # Stroom Change: Start - Set servername and mod_jk connectivity ServerName stroomp00.strmdev00.org JkMount /stroom* local JkMount /stroom/remoting/cluster* local JkMount /stroom/datafeed* local_proxy JkMount /stroom/remoting* local_proxy JkMount /stroom/datafeed/direct* local JkOptions +ForwardKeySize +ForwardURICompat +ForwardSSLCertChain -ForwardDirectories # Stroom Change: End # Use separate log files for the SSL virtual host; note that LogLevel # is not inherited from httpd.conf. ...  ssl.conf: Certificate file changes - All scenarios We replace the standard certificate files with the generated certificates. In the example below, we are using the multi node scenario, in that the key file names are stroomp.crt and stroomp.key. For other scenarios, use the appropriate file names generated. We replace\nSSLCertificateFile /etc/pki/tls/certs/localhost.crt  with\nSSLCertificateFile /home/stroomuser/stroom-jks/public/stroomp.crt  and\nSSLCertificateKeyFile /etc/pki/tls/private/localhost.key  with\nSSLCertificateKeyFile /home/stroomuser/stroom-jks/private/stroomp.key  That is, change\n... # pass phrase. Note that a kill -HUP will prompt again. A new # certificate can be generated using the genkey(1) command. SSLCertificateFile /etc/pki/tls/certs/localhost.crt # Server Private Key: # If the key is not combined with the certificate, use this # directive to point at the key file. Keep in mind that if # you've both a RSA and a DSA private key you can configure # both in parallel (to also allow the use of DSA ciphers, etc.) SSLCertificateKeyFile /etc/pki/tls/private/localhost.key # Server Certificate Chain: # Point SSLCertificateChainFile at a file containing the ...  to\n... # pass phrase. Note that a kill -HUP will prompt again. A new # certificate can be generated using the genkey(1) command. # Stroom Change: Start - Replace with Stroom server certificate # SSLCertificateFile /etc/pki/tls/certs/localhost.crt SSLCertificateFile /home/stroomuser/stroom-jks/public/stroomp.crt # Stroom Change: End # Server Private Key: # If the key is not combined with the certificate, use this # directive to point at the key file. Keep in mind that if # you've both a RSA and a DSA private key you can configure # both in parallel (to also allow the use of DSA ciphers, etc.) # Stroom Change: Start - Replace with Stroom server private key file # SSLCertificateKeyFile /etc/pki/tls/private/localhost.key SSLCertificateKeyFile /home/stroomuser/stroom-jks/private/stroomp.key # Stroom Change: End # Server Certificate Chain: # Point SSLCertificateChainFile at a file containing the ...  ssl.conf: Certificate Bundle/NO-CA Verification - All scenarios If you have signed your Stroom server certificate with a Certificate Authority, then change\nSSLCACertificateFile /etc/pki/tls/certs/ca-bundle.crt  to be your own certificate bundle which you should probably store as ~stroomuser/stroom-jks/public/stroomp-ca-bundle.crt.\nNow if you are using a self signed certificate, you will need to set the Client Authentication to have a value of\nSSLVerifyClient optional_no_ca  noting that this may change if you actually use a CA. That is, changing\n... # Client Authentication (Type): # Client certificate verification type and depth. Types are # none, optional, require and optional_no_ca. Depth is a # number which specifies how deeply to verify the certificate # issuer chain before deciding the certificate is not valid. #SSLVerifyClient require #SSLVerifyDepth 10 # Access Control: # With SSLRequire you can do per-directory access control based ...  to\n... # Client Authentication (Type): # Client certificate verification type and depth. Types are # none, optional, require and optional_no_ca. Depth is a # number which specifies how deeply to verify the certificate # issuer chain before deciding the certificate is not valid. #SSLVerifyClient require #SSLVerifyDepth 10 # Stroom Change: Start - Set optional_no_ca (given we have a self signed certificate) SSLVerifyClient optional_no_ca # Stroom Change: End # Access Control: # With SSLRequire you can do per-directory access control based ...  ssl.conf: Servlet Protection - Single or Multi Node scenarios (not for Standalone/Forwarding Proxy) We now need to secure certain Stroom Application servlets, to ensure they are only accessed under appropriate control.\nThis set of servlets will be accessible by all nodes in the subnet 192.168.2 (as well as localhost). We achieve this by adding after the example Location directives\n\u003cLocation ~ \"stroom/(status|echo|sessionList|debug)\" \u003e Require all denied Require ip 127.0.0.1 192.168.2 \u003c/Location\u003e  We further restrict the clustercall and export servlets to just the localhost. If we had multiple Stroom processing nodes, you would specify each node, or preferably, the subnet they are on. In our multi node case this is 192.168.2.\n\u003cLocation ~ \"stroom/export/|stroom/remoting/clustercall.rpc\" \u003e Require all denied Require ip 127.0.0.1 192.168.2 \u003c/Location\u003e  That is, the following\n... # and %{TIME_WDAY} \u003e= 1 and %{TIME_WDAY} \u003c= 5 \\ # and %{TIME_HOUR} \u003e= 8 and %{TIME_HOUR} \u003c= 20 ) \\ # or %{REMOTE_ADDR} =~ m/^192\\.76\\.162\\.[0-9]+$/ #\u003c/Location\u003e # SSL Engine Options: # Set various options for the SSL engine. # o FakeBasicAuth: ...  changes to\n... # and %{TIME_WDAY} \u003e= 1 and %{TIME_WDAY} \u003c= 5 \\ # and %{TIME_HOUR} \u003e= 8 and %{TIME_HOUR} \u003c= 20 ) \\ # or %{REMOTE_ADDR} =~ m/^192\\.76\\.162\\.[0-9]+$/ #\u003c/Location\u003e # Stroom Change: Start - Lock access to certain servlets \u003cLocation ~ \"stroom/(status|echo|sessionList|debug)\" \u003e Require all denied Require ip 127.0.0.1 192.168.2 \u003c/Location\u003e # Lock these Servlets more securely - to just localhost and processing node(s) \u003cLocation ~ \"stroom/export/|stroom/remoting/clustercall.rpc\" \u003e Require all denied Require ip 127.0.0.1 192.168.2 \u003c/Location\u003e # Stroom Change: End # SSL Engine Options: # Set various options for the SSL engine. # o FakeBasicAuth: ...  ssl.conf: Log formats - All scenarios Finally, as we make use of the Black Box Apache log format, we replace the standard format\nCustomLog logs/ssl_request_log \\ \"%t %h %{SSL_PROTOCOL}x %{SSL_CIPHER}x \\\"%r\\\" %b\"  with\nCustomLog logs/ssl_request_log blackboxSSLUser  That is, change\n... # Per-Server Logging: # The home of a custom SSL log file. Use this when you want a # compact non-error SSL logfile on a virtual host basis. CustomLog logs/ssl_request_log \\ \"%t %h %{SSL_PROTOCOL}x %{SSL_CIPHER}x \\\"%r\\\" %b\" \u003c/VirtualHost\u003e  to\n... # Per-Server Logging: # The home of a custom SSL log file. Use this when you want a # compact non-error SSL logfile on a virtual host basis. # Stroom Change: Start - Change ssl_request log to use our BlackBox format # CustomLog logs/ssl_request_log \\ # \"%t %h %{SSL_PROTOCOL}x %{SSL_CIPHER}x \\\"%r\\\" %b\" CustomLog logs/ssl_request_log blackboxSSLUser # Stroom Change: End \u003c/VirtualHost\u003e  Remember, in the case of Multi node stroom Application servers, deploy this file on all servers.\nApache Mod_JK configuration Apache Mod_JK has two configuration files\n /etc/httpd/conf.d/mod_jk.conf - for the http server configuration /etc/httpd/conf/workers.properties - to configure the Tomcat workers  In multi node scenarios, /etc/httpd/conf.d/mod_jk.conf is the same on all servers, but the /etc/httpd/conf/workers.properties file is different. The contents of these two configuration files differ depending on the Stroom deployment. The following provide the various deployment scenarios.\nMod_JK Multi Node Application and Proxy Deployment For a Stroom Multi node Application and Proxy server,\n we configure /etc/httpd/conf.d/mod_jk.conf as per  F=/etc/httpd/conf.d/mod_jk.conf printf 'LoadModule jk_module modules/mod_jk.so\\n' \u003e ${F} printf 'JkWorkersFile conf/workers.properties\\n' \u003e\u003e ${F} printf 'JkLogFile logs/mod_jk.log\\n' \u003e\u003e ${F} printf 'JkLogLevel info\\n' \u003e\u003e ${F} printf 'JkLogStampFormat \"[%%a %%b %%d %%H:%%M:%%S %%Y]\"\\n' \u003e\u003e ${F} printf 'JkOptions +ForwardKeySize +ForwardURICompat +ForwardSSLCertChain -ForwardDirectories\\n' \u003e\u003e ${F} printf 'JkRequestLogFormat \"%%w %%V %%T\"\\n' \u003e\u003e ${F} printf 'JkMount /stroom* loadbalancer\\n' \u003e\u003e ${F} printf 'JkMount /stroom/remoting/cluster* local\\n' \u003e\u003e ${F} printf 'JkMount /stroom/datafeed* loadbalancer_proxy\\n' \u003e\u003e ${F} printf 'JkMount /stroom/remoting* loadbalancer_proxy\\n' \u003e\u003e ${F} printf 'JkMount /stroom/datafeed/direct* loadbalancer\\n' \u003e\u003e ${F} printf '# Note: Replaced JkShmFile logs/jk.shm due to SELinux issues. Refer to\\n' \u003e\u003e ${F} printf '# https://bugzilla.redhat.com/bugzilla/show_bug.cgi?id=225452\\n' \u003e\u003e ${F} printf '# The following makes use of the existing /run/httpd directory\\n' \u003e\u003e ${F} printf 'JkShmFile run/jk.shm\\n' \u003e\u003e ${F} printf '\u003cLocation /jkstatus/\u003e\\n' \u003e\u003e ${F} printf ' JkMount status\\n' \u003e\u003e ${F} printf ' Order deny,allow\\n' \u003e\u003e ${F} printf ' Deny from all\\n' \u003e\u003e ${F} printf ' Allow from 127.0.0.1\\n' \u003e\u003e ${F} printf '\u003c/Location\u003e\\n' \u003e\u003e ${F} chmod 640 ${F}   we configure /etc/httpd/conf/workers.properties as per  Since we are deploying for a cluster with load balancing, we need a workers.properties file per node. Executing the following will result in two files (workers.properties.stroomp00 and workers.properties.stroomp01) which should be deployed to their respective servers.\ncd /tmp # Set the list of nodes Nodes=\"stroomp00.strmdev00.org stroomp01.strmdev00.org\" for oN in ${Nodes}; do _n=`echo ${oN} | cut -f1 -d\\.` ( printf '# Workers.properties for Stroom Cluster member: %s %s\\n' ${oN} printf 'worker.list=loadbalancer,loadbalancer_proxy,local,local_proxy,status\\n' L_t=\"\" Lp_t=\"\" for FQDN in ${Nodes}; do N=`echo ${FQDN} | cut -f1 -d\\.` printf 'worker.%s.port=8009\\n' ${N} printf 'worker.%s.host=%s\\n' ${N} ${FQDN} printf 'worker.%s.type=ajp13\\n' ${N} printf 'worker.%s.lbfactor=1\\n' ${N} printf 'worker.%s.max_packet_size=65536\\n' ${N} printf 'worker.%s_proxy.port=9009\\n' ${N} printf 'worker.%s_proxy.host=%s\\n' ${N} ${FQDN} printf 'worker.%s_proxy.type=ajp13\\n' ${N} printf 'worker.%s_proxy.lbfactor=1\\n' ${N} printf 'worker.%s_proxy.max_packet_size=65536\\n' ${N} L_t=\"${L_t}${N},\" Lp_t=\"${Lp_t}${N}_proxy,\" done L=`echo $L_t | sed -e 's/.$//'` Lp=`echo $Lp_t | sed -e 's/.$//'` printf 'worker.loadbalancer.type=lb\\n' printf 'worker.loadbalancer.balance_workers=%s\\n' $L printf 'worker.loadbalancer.sticky_session=1\\n' printf 'worker.loadbalancer_proxy.type=lb\\n' printf 'worker.loadbalancer_proxy.balance_workers=%s\\n' $Lp printf 'worker.loadbalancer_proxy.sticky_session=1\\n' printf 'worker.local.type=lb\\n' printf 'worker.local.balance_workers=%s\\n' ${_n} printf 'worker.local.sticky_session=1\\n' printf 'worker.local_proxy.type=lb\\n' printf 'worker.local_proxy.balance_workers=%s_proxy\\n' ${_n} printf 'worker.local_proxy.sticky_session=1\\n' printf 'worker.status.type=status\\n' ) \u003e workers.properties.${_n} chmod 640 workers.properties.${_n} done  Now depending in the node you are on, copy the relevant workers.properties.nodename file to /etc/httpd/conf/workers.properties. The following command makes this simple.\ncp workers.properties.`hostname -s` /etc/httpd/conf/workers.properties  If you were to add an additional node to a multi node cluster, say the node stroomp02.strmdev00.org, then you would re-run the above script with\nNodes=\"stroomp00.strmdev00.org stroomp01.strmdev00.org stroomp02.strmdev00.org\"  then redeploy all three files to the respective servers. Also note, that for the newly created workers.properties files for the existing nodes to take effect you will need to restart the Apache Httpd service on both nodes.\nRemember, in multi node cluster deployments, the following files are the same and hence can be created on one node, but copied to the others not forgetting to backup the other node’s original files. That is, the files\n /var/www/html/index.html /etc/httpd/conf.d/mod_jk.conf /etc/httpd/conf/httpd.conf  are to be the same on all nodes. Only the /etc/httpd/conf.d/ssl.conf and /etc/httpd/conf/workers.properties files change.\nMod_JK Standalone or Forwarding Stroom Proxy Deployment For a Stroom Standalone or Forwarding proxy,\n we configure /etc/httpd/conf.d/mod_jk.conf as per  F=/etc/httpd/conf.d/mod_jk.conf printf 'LoadModule jk_module modules/mod_jk.so\\n' \u003e ${F} printf 'JkWorkersFile conf/workers.properties\\n' \u003e\u003e ${F} printf 'JkLogFile logs/mod_jk.log\\n' \u003e\u003e ${F} printf 'JkLogLevel info\\n' \u003e\u003e ${F} printf 'JkLogStampFormat \"[%%a %%b %%d %%H:%%M:%%S %%Y]\"\\n' \u003e\u003e ${F} printf 'JkOptions +ForwardKeySize +ForwardURICompat +ForwardSSLCertChain -ForwardDirectories\\n' \u003e\u003e ${F} printf 'JkRequestLogFormat \"%%w %%V %%T\"\\n' \u003e\u003e ${F} printf 'JkMount /stroom/datafeed* local_proxy\\n' \u003e\u003e ${F} printf '# Note: Replaced JkShmFile logs/jk.shm due to SELinux issues. Refer to\\n' \u003e\u003e ${F} printf '# https://bugzilla.redhat.com/bugzilla/show_bug.cgi?id=225452\\n' \u003e\u003e ${F} printf '# The following makes use of the existing /run/httpd directory\\n' \u003e\u003e ${F} printf 'JkShmFile run/jk.shm\\n' \u003e\u003e ${F} printf '\u003cLocation /jkstatus/\u003e\\n' \u003e\u003e ${F} printf ' JkMount status\\n' \u003e\u003e ${F} printf ' Order deny,allow\\n' \u003e\u003e ${F} printf ' Deny from all\\n' \u003e\u003e ${F} printf ' Allow from 127.0.0.1\\n' \u003e\u003e ${F} printf '\u003c/Location\u003e\\n' \u003e\u003e ${F} chmod 640 ${F}   we configure /etc/httpd/conf/workers.properties as per  The variable N in the script below is to be the node name (not FQDN) of your sever (i.e. stroomfp0).\nN=stroomfp0 FQDN=`hostname -f` F=/etc/httpd/conf/workers.properties printf 'worker.list=local_proxy,status\\n' \u003e ${F} printf 'worker.%s_proxy.port=9009\\n' ${N} \u003e\u003e ${F} printf 'worker.%s_proxy.host=%s\\n' ${N} ${FQDN} \u003e\u003e ${F} printf 'worker.%s_proxy.type=ajp13\\n' ${N} \u003e\u003e ${F} printf 'worker.%s_proxy.lbfactor=1\\n' ${N} \u003e\u003e ${F} printf 'worker.local_proxy.type=lb\\n' \u003e\u003e ${F} printf 'worker.local_proxy.balance_workers=%s_proxy\\n' ${N} \u003e\u003e ${F} printf 'worker.local_proxy.sticky_session=1\\n' \u003e\u003e ${F} printf 'worker.status.type=status\\n' \u003e\u003e ${F} chmod 640 ${F}  Mod_JK Single Node Application and Proxy Deployment For a Stroom Single node Application and Proxy server,\n we configure /etc/httpd/conf.d/mod_jk.conf as per  F=/etc/httpd/conf.d/mod_jk.conf printf 'LoadModule jk_module modules/mod_jk.so\\n' \u003e ${F} printf 'JkWorkersFile conf/workers.properties\\n' \u003e\u003e ${F} printf 'JkLogFile logs/mod_jk.log\\n' \u003e\u003e ${F} printf 'JkLogLevel info\\n' \u003e\u003e ${F} printf 'JkLogStampFormat \"[%%a %%b %%d %%H:%%M:%%S %%Y]\"\\n' \u003e\u003e ${F} printf 'JkOptions +ForwardKeySize +ForwardURICompat +ForwardSSLCertChain -ForwardDirectories\\n' \u003e\u003e ${F} printf 'JkRequestLogFormat \"%%w %%V %%T\"\\n' \u003e\u003e ${F} printf 'JkMount /stroom* local\\n' \u003e\u003e ${F} printf 'JkMount /stroom/remoting/cluster* local\\n' \u003e\u003e ${F} printf 'JkMount /stroom/datafeed* local_proxy\\n' \u003e\u003e ${F} printf 'JkMount /stroom/remoting* local_proxy\\n' \u003e\u003e ${F} printf 'JkMount /stroom/datafeed/direct* local\\n' \u003e\u003e ${F} printf '# Note: Replaced JkShmFile logs/jk.shm due to SELinux issues. Refer to\\n' \u003e\u003e ${F} printf '# https://bugzilla.redhat.com/bugzilla/show_bug.cgi?id=225452\\n' \u003e\u003e ${F} printf '# The following makes use of the existing /run/httpd directory\\n' \u003e\u003e ${F} printf 'JkShmFile run/jk.shm\\n' \u003e\u003e ${F} printf '\u003cLocation /jkstatus/\u003e\\n' \u003e\u003e ${F} printf ' JkMount status\\n' \u003e\u003e ${F} printf ' Order deny,allow\\n' \u003e\u003e ${F} printf ' Deny from all\\n' \u003e\u003e ${F} printf ' Allow from 127.0.0.1\\n' \u003e\u003e ${F} printf '\u003c/Location\u003e\\n' \u003e\u003e ${F} chmod 640 ${F}   we configure /etc/httpd/conf/workers.properties as per  The variable N in the script below is to be the node name (not FQDN) of your sever (i.e. stroomp00).\nN=stroomp00 FQDN=`hostname -f` F=/etc/httpd/conf/workers.properties printf 'worker.list=local,local_proxy,status\\n' \u003e ${F} printf 'worker.%s.port=8009\\n' ${N} \u003e\u003e ${F} printf 'worker.%s.host=%s\\n' ${N} ${FQDN} \u003e\u003e ${F} printf 'worker.%s.type=ajp13\\n' ${N} \u003e\u003e ${F} printf 'worker.%s.lbfactor=1\\n' ${N} \u003e\u003e ${F} printf 'worker.%s.max_packet_size=65536\\n' ${N} \u003e\u003e ${F} printf 'worker.%s_proxy.port=9009\\n' ${N} \u003e\u003e ${F} printf 'worker.%s_proxy.host=%s\\n' ${N} ${FQDN} \u003e\u003e ${F} printf 'worker.%s_proxy.type=ajp13\\n' ${N} \u003e\u003e ${F} printf 'worker.%s_proxy.lbfactor=1\\n' ${N} \u003e\u003e ${F} printf 'worker.%s_proxy.max_packet_size=65536\\n' ${N} \u003e\u003e ${F} printf 'worker.local.type=lb\\n' \u003e\u003e ${F} printf 'worker.local.balance_workers=%s\\n' ${N} \u003e\u003e ${F} printf 'worker.local.sticky_session=1\\n' \u003e\u003e ${F} printf 'worker.local_proxy.type=lb\\n' \u003e\u003e ${F} printf 'worker.local_proxy.balance_workers=%s_proxy\\n' ${N} \u003e\u003e ${F} printf 'worker.local_proxy.sticky_session=1\\n' \u003e\u003e ${F} printf 'worker.status.type=status\\n' \u003e\u003e ${F} chmod 640 ${F}  Final host configuration and web service enablement Now tidy up the SELinux context for access on all nodes and files via the commands\nsetsebool -P httpd_enable_homedirs on setsebool -P httpd_can_network_connect on chcon --reference /etc/httpd/conf.d/README /etc/httpd/conf.d/mod_jk.conf chcon --reference /etc/httpd/conf/magic /etc/httpd/conf/workers.properties  We also enable both http and https services via the firewall on all nodes. If you don’t want to present a http access point, then don’t enable it in the firewall setting. This is done with\nfirewall-cmd --zone=public --add-service=http --permanent firewall-cmd --zone=public --add-service=https --permanent firewall-cmd --reload firewall-cmd --zone=public --list-all  Finally enable then start the httpd service, correcting any errors. It should be noted that on any errors, the suggestion of a systemctl status or viewing the journal are good, but also review information in the httpd error logs found in /var/log/httpd/.\nsystemctl enable httpd.service systemctl start httpd.service  ","categories":"","description":"The following is a HOWTO to assist users in configuring Apache's HTTPD with Mod_JK for Stroom.\n","excerpt":"The following is a HOWTO to assist users in configuring Apache's HTTPD …","ref":"/stroom-docs/hugo-docsy/docs/howtos/install/installhttpdhowto/","tags":["httpd","installation"],"title":"Apache Httpd/Mod_JK configuration for Stroom"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/certificates/","tags":"","title":"certificates"},{"body":"A Reference Feed is a temporal set of data that a pipeline’s translation can look up to gain additional information to decorate the subject data of the translation. For example, an XML Event.\nA Reference Feed is temporal, in that, each time a new set of reference data is loaded into Stroom, the effective date (for the data) is also recorded. Thus by using a timestamp field with the subject data, the appropriate batch of reference data can be accessed.\nA typical reference data set to support the Stroom XML Event schema might be on that relates to devices. Such a data set can contain the device logical identifiers such as fully qualified domain name and ip address and their geographical location information such as country, site, building, room and timezone.\nThe following example will describe how to create a reference feed for such device data. we will call the reference feed GeoHost-V1.0-REFERENCE.\nReference Data Our reference data will be supplied in a  separated file containing the fields\n the device Fully Qualified Domain Name the device IP Address the device Country location (using ISO 3166-1 alpha-3 codes) the device Site location the device Building location the device TimeZone location (both standard then daylight timezone offsets from UTC)  For simplicity, our example will use a file with just 3 entries\nFQDN\tIPAddress\tCountry\tSite\tBuilding\tRoom\tTimeZones stroomnode00.strmdev00.org\t192.168.2.245\tGBR\tBristol-S00\tGZero\tR00\t+00:00/+01:00 stroomnode01.strmdev01.org\t192.168.3.117\tAUS\tSydney-S04\tR6\t5-134\t+10:00/+11:00 host01.company4.org\t192.168.4.220\tUSA\tLosAngeles-S19\tILM\tC5-54-2\t-08:00/-07:00  A copy of this sample data source can be found here. Save a copy of this data to your local environment for use later in this HOWTO. Save this file as a text document with ANSI encoding.\nCreation To create our Reference Event stream we need to create:\n the Feed a Pipeline to automatically process and store the Reference data a Text Parser to convert the text file into simple XML record format, and a Translation to create reference data maps  Create Feed First, within the Explorer pane, and with the cursor having selected the Event Sources group, right click the mouse to have the object context menu appear.\nNew Feed    If you hover over the New Item    icon then the New sub-context menu will be revealed.\nNow hover the mouse over the FeedItem    icon and right click to select.\nNew Feed Selection window    When the New Feed selection windows comes up, navigate to the Event Sources system group. Then enter the name of the reference feed GeoHost-V1.0-REFERENCE onto the Name: text entry box. On pressing the OK button we will see the following Feed configuration tab appear.\nNew Feed Data tab    Click on the Settings sub-item in the GeoHost-V1.0-REFERENCE Feed tab to populate the initial Settings configuration. Enter an appropriate description, classification and click on the Reference Feed check box\nNew Feed Settings tab    and we then use the Stream Type drop-down menu to set the stream type as Raw Reference. At this point we save our configuration so far, by clicking on the Save    icon. The save icon becomes ghosted and our feed configuration has been saved.\nNew Feed Settings window configuration    Load sample Reference data At this point we want to load our sample reference data, in order to develop our reference feed. We can do this two ways - posting the file to our Stroom web server, or directly upload the data using the user interface. For this example we will use Stroom’s user interface upload facility.\nFirst, open the Data sub-item in the GeoHost-V1.0-REFERENCE feed configuration tab to reveal\nReference Data configuration tab    Note the Upload icon Upload    in the bottom left of the Stream table (top pane). On clicking the Upload icon, we are presented with the data upload selection window.\nUpload Selection window    Naturally, as this is a reference feed we are creating and this is raw data we are uploading, we select a Stream Type: of Raw Reference. We need to set the Effective: date (really a timestamp) for this specific stream of reference data. Clicking in the Effective: entry box will cause a calendar selection window to be displayed (initially set to the current date).\nUpload data settings    We are going to set the effective date to be late in 2019. Normally, you would choose a time stamp that matches the generation of the reference data. Click on the blue Previous Month icon (a less than symbol \u003c) on the Year/Month line to move back to December 2019.\nCalendar Effective Date Selection    Select the 1st (clicking on 1) at which point the calendar selection window will disappear and a time of 2019-12-01T00:00:00.000Z is displayed. This is the default whenever using the calendar selection window in Stroom - the resultant timestamp is that of the day selected at 00:00:00 (Zulu time). To get the calendar selection window to disappear, click anywhere outside of the timestamp entry box.\nUpload data choose file    Note, if you happen to click on the OK button before selecting the File (or Stream Type for that matter), an appropriate Alert dialog box will be displayed\nUpload Data No file set    We don’t need to set Meta Data for this stream of reference data, but we (obviously) need to select the file. For the purposes of this example, we will utilise the file GeoHostReference.log you downloaded earlier in the Reference Data section of this document. This file contains a header and three lines of reference data as per\nFQDN\tIPAddress\tCountry\tSite\tBuilding\tRoom\tTimeZones stroomnode00.strmdev00.org\t192.168.2.245\tGBR\tBristol-S00\tGZero\tR00\t+00:00/+01:00 stroomnode01.strmdev01.org\t192.168.3.117\tAUS\tSydney-S04\tR6\t5-134\t+10:00/+11:00 host01.company4.org\t192.168.4.220\tUSA\tLosAngeles-S19\tILM\tC5-54-2\t-08:00/-07:00  When we construct the pipeline for this reference feed, we will see how to make use of the header line.\nSo, click on the Choose File button to bring up a file selector window. Navigate within the selector window to the location on your location machine where you have saved the GeoHostReference.log file. On clicking Open we return to the Upload window with the file selected.\nUpload Reference Data - File chosen    On clicking OK we get an Alert dialog window to advise a file has been uploaded.\nUpload Alert window    at which point we press Close.\nAt this point, the Upload selection window closes, and we see our file displayed in the GeoHost-V1.0-REFERENCE Data stream table.\nUpload Display raw reference stream    When we click on the newly up-loaded stream in the Stream Table pane we see the other two panes fill with information.\nUpload Selected stream    The middle pane shows the selected or Specific feed and any linked streams. A linked stream could be the resultant Reference data set generated from a Raw Reference stream. If errors occur during processing of the stream, then a linked stream could be an Error stream.\nThe bottom pane displays the selected stream’s data or meta-data. If we click on the Meta link at the top of this pane, we will see the Metadata associated with this stream. We also note that the Meta link at the bottom of the pane is now embolden.\nUpload Selected stream - meta-data    We can see the metadata we set - the EffectiveTime, and implicitly, the Feed but we also see additional fields that Stroom has added that provide more detail about the data and its delivery to Stroom such as how and when it was received. We now need to switch back to the Data display as we need to author our reference feed translation.\nCreate Pipeline We now need to create the pipeline for our reference feed so that we can create our translation and hence create reference data for our feed.\nWithin the Explorer pane, and having selected the Event Sources system group, right click to bring up the object context menu, then the New sub-context menu. Move to the PipelineItem    and left click to select. When the New Pipeline selection window appears, navigate to, then select the Feeds and Translations system group then enter the name of the reference feed, GeoHost-V1.0-REFERENCE in the Name: text entry box.\nNew Pipeline - GeoHost-V1.0-REFERENCE    On pressing the OK button you will be presented with the new pipeline’s configuration tab\nNew Pipeline - Configuration tab    Within Settings, enter an appropriate description as per\nNew Pipeline - Configured settings    We now need to select the structure this pipeline will use. We need to move from the Settings sub-item on the pipeline configuration tab to the Structure sub-item. This is done by clicking on the Structure link, at which we will see\nNew Pipeline - Structure configuration    As this pipeline will be processing reference data, we would use a Reference Data pipeline. This is done by inheriting it from a defined set of Standard Pipelines. To do this, click on the menu selection icon Menu Selection    to the right of the Inherit From: test display box.\nWhen the Choose item selection window appears, navigate to Template Pipelines system group (if not already displayed), and select (left click) the Pipeline    Reference Data pipeline\nNew Pipeline - Reference Data pipeline inherited    then press OK. At this we will see the inherited pipeline structure of\nNew Pipeline - Inherited set    Noting that this pipeline has not yet been saved - indicated by the * in the tab label and the highlighted Save    to save, which results in\nNew Pipeline - saved    This ends the first stage of the pipeline creation. We need to author the feed’s translation.\nCreate Text Converter To turn our tab delimited data in Stroom reference data, we first need to convert the text into simple XML. We do this using a Text Converter. Test Converters use a Stroom Data Splitter to convert text into simple XML.\nWithin the Explorer pane, and having selected the Event Sources system group, right click to bring up the object context menu. Navigate to the Text Converter    item and left click to select.\nWhen the New Text Converter selection window comes up, navigate to and select Event Sources system group, then enter the name of the feed, GeoHost-V1.0-REFERENCE into the Name: text entry box as per\nNew TextConverter    On pressing the OK button we see the next text converter’s configuration tab displayed.\nNew TextConverter Settings    Enter an appropriate description into the Description: text entry box, for instance\nText converter for device Logical and Geographic reference feed holding FQDN, IPAddress, Country, Site, Building, Room and Time Zones. Feed has a header and is tab separated.  Set the Converter Type: to be Data Splitter from the drop-down menu.\nNew TextConverter Settings configured    We next press the Conversion sub-item on the TextConverter tab to bring up the Data Splitter editing window.\nThe following is our Data Splitter code (see Data Splitter documentation for more complete details)\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.1.xsd\" version=\"3.0\"\u003e \u003c!-- GEOHOST REFERENCE FEED: CHANGE HISTORY v1.0.0 - 2020-02-09 John Doe This is a reference feed for device Logical and Geographic data. The feed provides for each device * the device FQDN * the device IP Address * the device Country location (using ISO 3166-1 alpha-3 codes) * the device Site location * the device Building location * the device Room location *the device TimeZone location (both standard then daylight timezone offsets from UTC) The data is a TAB delimited file with the first line providing headings. Example data: FQDN\tIPAddress\tCountry\tSite\tBuilding\tRoom\tTimeZones stroomnode00.strmdev00.org\t192.168.2.245\tGBR\tBristol-S00\tGZero\tR00\t+00:00/+01:00 stroomnode01.strmdev01.org\t192.168.3.117\tAUS\tSydney-S04\tR6\t5-134\t+10:00/+11:00 host01.company4.org\t192.168.4.220\tUSA\tLosAngeles-S19\tILM\tC5-54-2\t-08:00/-07:00 --\u003e \u003c!-- Match the heading line - split on newline and match a maximum of one line --\u003e \u003csplit delimiter=\"\\n\" maxMatch=\"1\"\u003e \u003c!-- Store each heading and note we split fields on the TAB (\u0026#9;) character --\u003e \u003cgroup\u003e \u003csplit delimiter=\"\u0026#9;\"\u003e \u003cvar id=\"heading\"/\u003e \u003c/split\u003e \u003c/group\u003e \u003c/split\u003e \u003c!-- Match all other data lines - splitting on newline --\u003e \u003csplit delimiter=\"\\n\"\u003e \u003cgroup\u003e \u003c!-- Store each field using the column heading number for each column ($heading$1) and note we split fields on the TAB (\u0026#9;) character --\u003e \u003csplit delimiter=\"\u0026#9;\"\u003e \u003cdata name=\"$heading$1\" value=\"$1\"/\u003e \u003c/split\u003e \u003c/group\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  At this point we want to save our Text Converter, so click on the Save    icon.\nA copy of this data splitter can be found here.\nAssign Text Converter to Pipeline To test our Text Converter, we need to modify our GeoHost-V1.0-REFERENCE pipeline to use it. Select the GeoHost-V1.0-REFERENCE pipeline tab and then select the Structure sub-item\nAssociated text converter with pipeline    To associate our new Text Converter with the pipeline, click on the combinedParser    icon then move the cursor to the Property (middle) pane then double click on the textConverter Property Name to allow you to edit the property as per\ntextConverter Edit property    We leave the Property Source: as Inherit but we need to change the Property Value: from None to be our newly created GeoHost-V1.0-REFERENCE text Converter\ntextConverter select GeoHost-V1.0-REFERENCE    then press OK. At this we will see the Property Value set\ntextConverter set Property Value    Again press OK to finish editing this property and we then see that the textConverter property has been set to GeoHost-V1.0-REFERENCE. Similarly set the type property Value to “Data Splitter”.\nAt this point, we should save our changes, by clicking on the highlighted Save    icon. The combined Parser window panes should now look like\ntextConverter set Property Value type    Test Text Converter To test our Text Converter, we select the GeoHost-V1.0-REFERENCE Feed tab GeoHost-V1.0-REFERENCE Feed    then click on our uploaded stream in the Stream Table pane, then click the check box of the Raw Reference stream in the Specific Stream table (middle pane)\ntextConverter - select raw reference data    We now want to step our data through the Text Converter. We enter Stepping Mode by pressing the stepping button Enter Stepping    found at the bottom of the right of the stream Raw Data display.\nYou will then be requested to choose a pipeline to step with, at which, you should navigate to the GeoHost-V1.0-REFERENCE pipeline as per\ntextConverter - select pipeline to step with    then press OK.\nAt this point we enter the pipeline Stepping tab\ntextConverter - stepping tab    which initially displays the Raw Reference data from our stream.\nWe click on the combinedParser    icon, to display.\ntextConverter - stepping editor workspace    This stepping window is divided into three sub-panes. the top one is the Text Converter editor and it will allow you to adjust the text conversion should you wish too. The bottom left window displays the input to the Text Converter. The bottom right window displays the output from the Text Converter for the given input.\nWe now click on the pipeline Step Forward button Step Forward    to single step the Raw reference data throughout text converter. We see that the Stepping function has displayed the heading and first data line of our raw reference data in the input sub-pane and the resultant simple records XML (adhering to the Stroom records v2.0 schema) in the output pane.\ntextConverter - pipeline stepping - 1st record    If we again press the Step Forward    button we see the second line in our raw reference data in the input sub-pane and the resultant simple records XML in the output pane.\ntextConverter - pipeline stepping - 2nd record    Pressing the Step Forward button Step Forward    again displays our third and last line of our raw and converted data.\ntextConverter - pipeline stepping - 3rd record    We have now successfully tested the Text Converter for our reference feed. Our next step is to author our translation to generate reference data records that Stroom can use.\nCreate XSLT Translation We now need to create our translation. This XSLT translation will convert simple records XML data into ReferenceData records - see the Stroom reference-data v2.0.1 Schema for details.\nWe first need to create an XSLT translation for our feed. Move back to the Explorer window, and with the cursor having selected the Event Sources system group, right click the mouse to display the object context menu, select New\ntranslation object sub-contect menu    and then move the cursor to the XSLT Item    item, then left click to select.\nWhen the New XSLT selection window comes up, navigate to the Event Sources system group and enter the name of the reference feed - GeoHost-V1.0-REFERENCE into the Name: text entry box as per\nNew xslt Translation selection window    On pressing the OK button we see the XSL tab for our translation and as previously, we enter an appropriate description before selecting the XSLT sub-item.\nNew xslt - Configuration tab    On selection of the XSLT sub-item, we are presented with the XSLT editor window\nxslt Translation - XSLT editor    At this point, rather than edit the translation in this editor and then assign this translation to the GeoHost-V1.0-REFERENCE pipeline, we will first make the assignment in the pipeline and then develop the translation whilst stepping through the raw data. This is to demonstrate there are a number of ways to develop a translation.\nSo, to start, save the XSLT by clicking on the GeoHost-V1.0-REFERENCE Pipeline    tab to raise the GeoHost-V1.0-REFERENCE pipeline. Then select the Structure sub-item followed by selecting the XSL translationFilter icon. We now see the XSL translationFilter Property Table for our pipeline in the middle pane.\nxslt translation element - property pane    To associate our new translation with the pipeline, move the cursor to the Property Table, click on the grayed out xslt Property Name and then click on the Edit Property Edit Property    icon to allow you to edit the property as per\nxslt -property editor    We leave the Property Source: as Inherit and we need to change the Property Value: from None to be our newly created GeoHost-V1.0-REFERENCE XSL translation. To do this, position the cursor over the menu selection icon Menu Selection    of the Value: chooser and right click, at which the Choose item selection window appears. Navigate to the Event Sources system group then select the GeoHost-V1.0-REFERENCE xsl translation.\nxslt - value selection    then press OK. At this point we will see the property Value: set\nxslt - value selected    Again press OK to finish editing this property and we see that the xslt property has been set to GeoHost-V1.0-REFERENCE.\nxslt - property set    At this point, we should save our changes, by clicking on the highlighted Save    icon.\nTest XSLT Translation We now go back to the GeoHost-V1.0-REFERENCE Feed tab GeoHost-V1.0-REFERENCE Feed    then click on our uploaded stream in the Stream Table pane. Next click the check box of the Raw Reference stream in the Specific Stream table (middle pane) as per\nGeoHost-V1.0-REFERENCE feedTab - Specific Stream    We now want to step our data through the xslt Translation. We enter Stepping Mode by pressing the stepping button Enter Stepping    found at the bottom of the right of the stream Raw Data display.\nYou will then be requested to choose a pipeline to step with, at which, you should navigate to the GeoHost-V1.0-REFERENCE pipeline as per\nxslt Translation - select pipeline to step with    then press OK.\nAt this point we enter the pipeline through the Stepping tab xslt Translation - stepping tab    which initially displays the Raw Reference data from our stream.\nWe click on the translationFilter    icon to enter the xslt Translation stepping window and all panes are empty.\nxslt Translation - editor    As for the Text Converter, this translation stepping window is divided into three sub-panes. The top one is the XSLT Translation. The bottom right window displays the output from the XSLT Translation for the given input.\nWe now click on the pipeline Step Forward button Step Forward    to single step the Raw reference data through our translation. We see that the Stepping function has displayed the first records XML entry in the input sub-pane and the same data is displayed in the output sub-pane.\nxslt Translation - editor 1st record    But we also note if we move along the pipeline structure to the schemaFilter Error    .\nxslt Translation - schema fault    In essence, since the translation has done nothing, and the data is simple records XML, the system is indicating that it expects the output data to be in the reference-data v2.0.1 format.\nWe can correct this by adding the skeleton xslt translation for reference data into our translationFilter. Move back to the translationFilter    icon on the pipeline structure and add the following to the xsl window\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\" ?\u003e \u003cxsl:stylesheet xpath-default-namespace=\"records:2\" xmlns=\"reference-data:2\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:stroom=\"stroom\" xmlns:evt=\"event-logging:3\" version=\"2.0\"\u003e \u003cxsl:template match=\"records\"\u003e \u003creferenceData xmlns=\"reference-data:2\" xsi:schemaLocation=\"reference-data:2 file://reference-data-v2.0.xsd\" version=\"2.0.1\"\u003e xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \u003cxsl:apply-templates/\u003e \u003c/referenceData\u003e \u003c/xsl:template\u003e \u003c!-- MAIN TEMPLATE --\u003e \u003cxsl:template match=\"record\"\u003e \u003creference\u003e \u003cmap\u003e\u003c/map\u003e \u003ckey\u003e\u003c/key\u003e \u003cvalue\u003e\u003c/value\u003e \u003c/reference\u003e \u003c/xsl:template\u003e \u003c/xsl:stylesheet\u003e  And on pressing the refresh button Step Refresh Button    we see that the output window is an empty ReferenceData element.\nxslt Translation - null translation    Also note that if we move to the schemaFilter    icon on the pipeline structure, we no longer have an “Invalid Schema Location” error.\nWe next extend the translation to actually generate reference data. The translation will now look like\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\" ?\u003e \u003cxsl:stylesheet xpath-default-namespace=\"records:2\" xmlns=\"reference-data:2\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:stroom=\"stroom\" xmlns:evt=\"event-logging:3\" version=\"2.0\"\u003e \u003c!-- GEOHOST REFERENCE FEED: CHANGE HISTORY v1.0.0 - 2020-02-09 John Doe This is a reference feed for device Logical and Geographic data. The feed provides for each device * the device FQDN * the device IP Address * the device Country location (using ISO 3166-1 alpha-3 codes) * the device Site location * the device Building location * the device Room location *the device TimeZone location (both standard then daylight timezone offsets from UTC) The reference maps are FQDN_TO_IP - Fully Qualified Domain Name to IP Address IP_TO_FQDN - IP Address to FQDN (HostName) FQDN_TO_LOC - Fully Qualified Domain Name to Location element --\u003e \u003cxsl:template match=\"records\"\u003e \u003creferenceData xmlns=\"reference-data:2\" xsi:schemaLocation=\"reference-data:2 file://reference-data-v2.0.xsd\" version=\"2.0.1\"\u003e \u003cxsl:apply-templates/\u003e \u003c/referenceData\u003e \u003c/xsl:template\u003e \u003c!-- MAIN TEMPLATE --\u003e \u003cxsl:template match=\"record\"\u003e \u003c!-- FQDN_TO_IP map --\u003e \u003creference\u003e \u003cmap\u003eFQDN_TO_IP\u003c/map\u003e \u003ckey\u003e \u003cxsl:value-of select=\"lower-case(data[@name='FQDN']/@value)\" /\u003e \u003c/key\u003e \u003cvalue\u003e \u003cIPAddress\u003e \u003cxsl:value-of select=\"data[@name='IPAddress']/@value\" /\u003e \u003c/IPAddress\u003e \u003c/value\u003e \u003c/reference\u003e \u003c!-- IP_TO_FQDN map --\u003e \u003creference\u003e \u003cmap\u003eIP_TO_FQDN\u003c/map\u003e \u003ckey\u003e \u003cxsl:value-of select=\"lower-case(data[@name='IPAddress']/@value)\" /\u003e \u003c/key\u003e \u003cvalue\u003e \u003cHostName\u003e \u003cxsl:value-of select=\"data[@name='FQDN']/@value\" /\u003e \u003c/HostName\u003e \u003c/value\u003e \u003c/reference\u003e \u003c/xsl:template\u003e \u003c/xsl:stylesheet\u003e  and when we refresh, by pressing the Refresh Current Step button Step Refresh Button    we see that the output window now has Reference elements within the parent ReferenceData element\nxslt Translation - basic translation    If we press the Step Forward button Step Forward    we see the second record of our raw reference data in the input sub-pane and the resultant Reference elements\nxslt Translation - basic translation next record    At this point it would be wise to save our translation. This is done by clicking on the highlighted save    icon in the top left-hand area of the window under the tabs.\nWe can now further our Reference by adding a Fully Qualified Domain Name to Location reference - FQDN_TO_LOC and so now the translation looks like\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\" ?\u003e \u003cxsl:stylesheet xpath-default-namespace=\"records:2\" xmlns=\"reference-data:2\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:stroom=\"stroom\" xmlns:evt=\"event-logging:3\" version=\"2.0\"\u003e \u003c!-- GEOHOST REFERENCE FEED: CHANGE HISTORY v1.0.0 - 2020-02-09 John Doe This is a reference feed for device Logical and Geographic data. The feed provides for each device * the device FQDN * the device IP Address * the device Country location (using ISO 3166-1 alpha-3 codes) * the device Site location * the device Building location * the device Room location *the device TimeZone location (both standard then daylight timezone offsets from UTC) The reference maps are FQDN_TO_IP - Fully Qualified Domain Name to IP Address IP_TO_FQDN - IP Address to FQDN (HostName) FQDN_TO_LOC - Fully Qualified Domain Name to Location element --\u003e \u003cxsl:template match=\"records\"\u003e \u003creferenceData xmlns=\"reference-data:2\" xsi:schemaLocation=\"reference-data:2 file://reference-data-v2.0.xsd\" version=\"2.0.1\"\u003e \u003cxsl:apply-templates/\u003e \u003c/referenceData\u003e \u003c/xsl:template\u003e \u003c!-- MAIN TEMPLATE --\u003e \u003cxsl:template match=\"record\"\u003e \u003c!-- FQDN_TO_IP map --\u003e \u003creference\u003e \u003cmap\u003eFQDN_TO_IP\u003c/map\u003e \u003ckey\u003e \u003cxsl:value-of select=\"lower-case(data[@name='FQDN']/@value)\" /\u003e \u003c/key\u003e \u003cvalue\u003e \u003cIPAddress\u003e \u003cxsl:value-of select=\"data[@name='IPAddress']/@value\" /\u003e \u003c/IPAddress\u003e \u003c/value\u003e \u003c/reference\u003e \u003c!-- IP_TO_FQDN map --\u003e \u003creference\u003e \u003cmap\u003eIP_TO_FQDN\u003c/map\u003e \u003ckey\u003e \u003cxsl:value-of select=\"lower-case(data[@name='IPAddress']/@value)\" /\u003e \u003c/key\u003e \u003cvalue\u003e \u003cHostName\u003e \u003cxsl:value-of select=\"data[@name='FQDN']/@value\" /\u003e \u003c/HostName\u003e \u003c/value\u003e \u003c/reference\u003e \u003c!-- FQDN_TO_LOC map --\u003e \u003creference\u003e \u003cmap\u003eFQDN_TO_LOC\u003c/map\u003e \u003ckey\u003e \u003cxsl:value-of select=\"lower-case(data[@name='FQDN']/@value)\" /\u003e \u003c/key\u003e \u003cvalue\u003e \u003c!-- Note, when mapping to a XML node set, we make use of the Event namespace - i.e. evt: defined on our stylesheet element. This is done, so that, when the node set is returned, it is within the correct namespace. --\u003e \u003cevt:Location\u003e \u003cevt:Country\u003e \u003cxsl:value-of select=\"data[@name='Country']/@value\" /\u003e \u003c/evt:Country\u003e \u003cevt:Site\u003e \u003cxsl:value-of select=\"data[@name='Site']/@value\" /\u003e \u003c/evt:Site\u003e \u003cevt:Building\u003e \u003cxsl:value-of select=\"data[@name='Building']/@value\" /\u003e \u003c/evt:Building\u003e \u003cevt:Room\u003e \u003cxsl:value-of select=\"data[@name='Room']/@value\" /\u003e \u003c/evt:Room\u003e \u003cevt:TimeZone\u003e \u003cxsl:value-of select=\"data[@name='TimeZones']/@value\" /\u003e \u003c/evt:TimeZone\u003e \u003c/evt:Location\u003e \u003c/value\u003e \u003c/reference\u003e \u003c/xsl:template\u003e \u003c/xsl:stylesheet\u003e  and our second ReferenceData element would now look like\nxslt Translation - complete translation 2nd record    We have completed the translation and have hence completed the development of our GeoHost-V1.0-REFERENCE reference feed.\nAt this point, the reference feed is set up to accept Raw Reference data, but it will not automatically process the raw data and hence it will not place reference data into the reference data store. To have Stroom automatically process Raw Reference streams, you will need to enable Processors for this pipeline.\nEnabling the Reference Feed Processors We now create the pipeline Processors for this feed, so that the raw reference data will be transformed into Reference Data on ingest and save to Reference Data stores.\nOpen the reference feed pipeline by selecting the GeoHost-V1.0-REFERENCE pipeline GeoHost-V1.0-REFERENCE Pipeline    tab to raise the GeoHost-V1.0-REFERENCE pipeline. Then select the Processors sub-item to show\npipeline Processors    This configuration tab is divided into two panes. The top pane shows the current enabled Processors and any recently processed streams and the bottom pane provides meta-data about each Processor or recently processed streams.\nFirst, move the mouse to the Add Processor Add    icon at the top left of the top pane. Select by left clicking this icon to have displayed the Add Filter selection window\npipeline Processors - Add Filter    This selection window allows us to filter what set of data streams we want our Processor to process. As our intent is to enable processing for all GeoHost-V1.0-REFERENCE streams, both already received and yet to be received, then our filtering criteria is just to process all Raw Reference for this feed, ignoring all other conditions.\nTo do this, first click on the Add Term Menu button    icon to navigate to the desired feed name (GeoHost-V1.0-REFERENCE) object\npipeline Processors - Choose Feed name    and press OK to make the selection.\nNext, we select the required stream type. To do this click on the Add Term Add    icon again. Click on the down arrow to change the Term selection from Feed to Type. Click in the Value position on the highlighted line (it will be currently empty). Once you have clicked here a drop-down box will appear as per\npipeline Processors - Choose Stream Type    at which point, select the Stream Type of Raw Referenceand then press OK. At this we return to the Add Processor selection window to see that the Raw Reference stream type has been added.\npipeline Processors - pipeline criteria set    Note the Processor has been added but it is in a disabled state. We enable both pipeline processor and the processor filter\npipeline Processors - Enable    Note - if this is the first time you have set up pipeline processing on your Stroom instance you may need to check that the Stream Processor job is enabled on your Stroom instance. To do this go to the Stroom main menu and select Monitoring\u003eJobs\u003e Check the status of the Stream Processor job and enable if required. If you need to enable the job also ensure you enable the job on the individual nodes as well (go to the bottom window pane and select the enable box on the far right)\npipeline Processors - Enable node processing    pipeline Processors - Enable    Returning to the GeoHost-V1.0-REFERENCE Pipeline    tab, Processors sub-item, if everything is working on your Stroom instance you should now see that Raw Reference streams are being processed by your processor - the Streams count is incrementing and the Tracker% is incrementing (when the Tracker% is 100% then all streams you selected (Filtered for) have been processed)\npipeline Processors - Enable    Navigating back to the Data sub-item and clicking on the reference feed stream in the Stream Table we see\npipeline Display Data    In the top pane, we see the Streams table as per normal, but in the Specific stream table we see that we have both a Raw Reference stream and its child Reference stream. By clicking on and highlighting the Reference stream we see its content in the bottom pane.\nThe complete ReferenceData for this stream is\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\"?\u003e \u003creferenceData xmlns=\"reference-data:2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:stroom=\"stroom\" xmlns:evt=\"event-logging:3\" xsi:schemaLocation=\"reference-data:2 file://reference-data-v2.0.xsd\" version=\"2.0.1\"\u003e \u003creference\u003e \u003cmap\u003eFQDN_TO_IP\u003c/map\u003e \u003ckey\u003estroomnode00.strmdev00.org\u003c/key\u003e \u003cvalue\u003e \u003cIPAddress\u003e192.168.2.245\u003c/IPAddress\u003e \u003c/value\u003e \u003c/reference\u003e \u003creference\u003e \u003cmap\u003eIP_TO_FQDN\u003c/map\u003e \u003ckey\u003e192.168.2.245\u003c/key\u003e \u003cvalue\u003e \u003cHostName\u003estroomnode00.strmdev00.org\u003c/HostName\u003e \u003c/value\u003e \u003c/reference\u003e \u003creference\u003e \u003cmap\u003eFQDN_TO_LOC\u003c/map\u003e \u003ckey\u003estroomnode00.strmdev00.org\u003c/key\u003e \u003cvalue\u003e \u003cevt:Location\u003e \u003cevt:Country\u003eGBR\u003c/evt:Country\u003e \u003cevt:Site\u003eBristol-S00\u003c/evt:Site\u003e \u003cevt:Building\u003eGZero\u003c/evt:Building\u003e \u003cevt:Room\u003eR00\u003c/evt:Room\u003e \u003cevt:TimeZone\u003e+00:00/+01:00\u003c/evt:TimeZone\u003e \u003c/evt:Location\u003e \u003c/value\u003e \u003c/reference\u003e \u003creference\u003e \u003cmap\u003eFQDN_TO_IP\u003c/map\u003e \u003ckey\u003estroomnode01.strmdev01.org\u003c/key\u003e \u003cvalue\u003e \u003cIPAddress\u003e192.168.3.117\u003c/IPAddress\u003e \u003c/value\u003e \u003c/reference\u003e \u003creference\u003e \u003cmap\u003eIP_TO_FQDN\u003c/map\u003e \u003ckey\u003e192.168.3.117\u003c/key\u003e \u003cvalue\u003e \u003cHostName\u003estroomnode01.strmdev01.org\u003c/HostName\u003e \u003c/value\u003e \u003c/reference\u003e \u003creference\u003e \u003cmap\u003eFQDN_TO_LOC\u003c/map\u003e \u003ckey\u003estroomnode01.strmdev01.org\u003c/key\u003e \u003cvalue\u003e \u003cevt:Location\u003e \u003cevt:Country\u003eAUS\u003c/evt:Country\u003e \u003cevt:Site\u003eSydeny-S04\u003c/evt:Site\u003e \u003cevt:Building\u003eR6\u003c/evt:Building\u003e \u003cevt:Room\u003e5-134\u003c/evt:Room\u003e \u003cevt:TimeZone\u003e+10:00/+11:00\u003c/evt:TimeZone\u003e \u003c/evt:Location\u003e \u003c/value\u003e \u003c/reference\u003e \u003creference\u003e \u003cmap\u003eFQDN_TO_IP\u003c/map\u003e \u003ckey\u003ehost01.company4.org\u003c/key\u003e \u003cvalue\u003e \u003cIPAddress\u003e192.168.4.220\u003c/IPAddress\u003e \u003c/value\u003e \u003c/reference\u003e \u003creference\u003e \u003cmap\u003eIP_TO_FQDN\u003c/map\u003e \u003ckey\u003e192.168.4.220\u003c/key\u003e \u003cvalue\u003e \u003cHostName\u003ehost01.company4.org\u003c/HostName\u003e \u003c/value\u003e \u003c/reference\u003e \u003creference\u003e \u003cmap\u003eFQDN_TO_LOC\u003c/map\u003e \u003ckey\u003ehost01.company4.org\u003c/key\u003e \u003cvalue\u003e \u003cevt:Location\u003e \u003cevt:Country\u003eUSA\u003c/evt:Country\u003e \u003cevt:Site\u003eLosAngeles-S19\u003c/evt:Site\u003e \u003cevt:Building\u003eILM\u003c/evt:Building\u003e \u003cevt:Room\u003eC5-54-2\u003c/evt:Room\u003e \u003cevt:TimeZone\u003e-08:00/-07:00\u003c/evt:TimeZone\u003e \u003c/evt:Location\u003e \u003c/value\u003e \u003c/reference\u003e \u003c/referenceData\u003e  If we go back to the reference feed itself (and click on the Refresh    button on the far right of the top and middle panes), we now see both the Reference and Raw Reference streams in the Streams Table pane.\nreference feed - Data tab    Selecting the Reference stream in the Stream Table will result in the Specific stream pane displaying the Raw Reference and its child Reference stream (highlighted) and the actual ReferenceData output in the Data pane at the bottom.\nreference feed - Select reference    Selecting the Raw Reference stream in the Streams Table will result in the Specific stream pane displaying the Raw Reference and its child Reference stream as before, but with the Raw Reference stream highlighted and the actual Raw Reference input data displayed in the Data pane at the bottom.\nreference feed - Select raw reference    The creation of the Raw Reference is now complete.\nAt this point you may wish to organise the resources you have created within the Explorer pane to a more appropriate location such as Reference/GeoHost. Because Stroom Explorer is a flat structure you can move resources around to reorganise the content without any impact on directory paths, configurations etc.\nreference feed - Organise Resources    Now you have created the new folder structure you can move the various GeoHost resources to this location. Select all four resources by using the mouse right-click button while holding down the Shift key. Then right click on the highlighted group to display the action menu\nOrganise Resources - move content    Select move and the Move Multiple Items window will display. Navigate to the Reference/GeoHost folder to move the items to this destination.\nOrganise Resources - select destination    The final structure is seen below\nOrganise Resources - finished    ","categories":"","description":"How to create a reference feed for decorating event data using reference data lookups.\n","excerpt":"How to create a reference feed for decorating event data using …","ref":"/stroom-docs/hugo-docsy/docs/howtos/referencefeeds/createsimplereferencefeeds/","tags":["reference-data"],"title":"Create a Simple Reference Feed"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/database/","tags":"","title":"database"},{"body":"Following this HOWTO will produce a simple, minimally secured database deployment. In a production environment consideration needs to be made for redundancy, better security, data-store location, increased memory usage, and the like.\nStroom has two databases. The first, stroom, is used for management of Stroom itself and the second, statistics is used for the Stroom Statistics capability. There are many ways to deploy these two databases. One could\n have a single database instance and serve both databases from it have two database instances on the same server and serve one database per instance have two separate nodes, each with it’s own database instance the list goes on.  In this HOWTO, we describe the deployment of two database instances on the one node, each serving a single database. We provide example deployments using either the MariaDB (external link) or MySQL Community (external link) versions of MySQL.\nAssumptions  we are installing the MariaDB or MySQL Community RDBMS software. the primary database node is ‘stroomdb0.strmdev00.org’. installation is on a fully patched minimal Centos 7.3 instance. we are installing BOTH databases (stroom and statistics) on the same node - ‘stroomdb0.stromdev00.org’ but with two distinct database engines. The first database will communicate on port 3307 and the second on 3308. we are deploying with SELinux in enforcing mode. any scripts or commands that should run are in code blocks and are designed to allow the user to cut then paste the commands onto their systems. in this document, when a textual screen capture is documented, data entry is identified by the data surrounded by ‘\u003c’ ‘\u003e’ . This excludes enter/return presses.  Installation of Software MariaDB Server Installation As MariaDB is directly supported by Centos 7, we simply install the database server software and SELinux policy files, as per\nsudo yum -y install policycoreutils-python mariadb-server  MySQL Community Server Installation As MySQL is not directly supported by Centos 7, we need to install it’s repository files prior to installation. We get the current MySQL Community release repository rpm and validate it’s MD5 checksum against the published value found on the MySQL Yum Repository (external link) site.\nwget https://repo.mysql.com/mysql57-community-release-el7.rpm md5sum mysql57-community-release-el7.rpm  On correct validation of the MD5 checksum, we install the repository files via\nsudo yum -y localinstall mysql57-community-release-el7.rpm  NOTE: Stroom currently does not support the latest production MySQL version - 5.7. You will need to install MySQL Version 5.6.\nNow since we must use MySQL Version 5.6 you will need to edit the MySQL repo file /etc/yum.repos.d/mysql-community.repo to disable the mysql57-community channel and enable the mysql56-community channel. We start by, backing up the repo file with\nsudo cp /etc/yum.repos.d/mysql-community.repo /etc/yum.repos.d/mysql-community.repo.ORIG  Then edit the file to change\n... # Enable to use MySQL 5.6 [mysql56-community] name=MySQL 5.6 Community Server baseurl=http://repo.mysql.com/yum/mysql-5.6-community/el/7/$basearch/ enabled=0 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql [mysql57-community] name=MySQL 5.7 Community Server baseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/7/$basearch/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql ...  to become\n... # Enable to use MySQL 5.6 [mysql56-community] name=MySQL 5.6 Community Server baseurl=http://repo.mysql.com/yum/mysql-5.6-community/el/7/$basearch/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql [mysql57-community] name=MySQL 5.7 Community Server baseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/7/$basearch/ enabled=0 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql ...  Next we install server software and SELinux policy files, as per\nsudo yum -y install policycoreutils-python mysql-community-server  Preparing the Database Deployment MariaDB Variant Create and instantiate both database instances To set up two MariaDB database instances on the one node, we will use mysql_multi and systemd service templates. The mysql_multi utility is a capability that manages multiple MariaDB databases on the same node and systemd service templates manage multiple services from one configuration file. A systemd service template is unique in that it has an @ character before the .service suffix.\nTo use this multiple-instance capability, we need to create two data directories for each database instance and also replace the main MariaDB configuration file, /etc/my.cnf, with one that includes configuration of key options for each instance. We will name our instances, mysqld0 and mysqld1. We will also create specific log files for each instance.\nWe will use the directories, /var/lib/mysql-mysqld0 and /var/lib/mysql-mysqld1 for the data directories and /var/log/mariadb/mysql-mysqld0.log and /var/log/mariadb/mysql-mysqld1.log for the log files. Note you should modify /etc/logrotate.d/mariadb to manage these log files. Note also, we need to set the appropriate SELinux file contexts on the created directories and any files.\nWe create the data directories and log files and set their respective SELinux contexts via\nsudo mkdir /var/lib/mysql-mysqld0 sudo chown mysql:mysql /var/lib/mysql-mysqld0 sudo semanage fcontext -a -t mysqld_db_t \"/var/lib/mysql-mysqld0(/.*)?\" sudo restorecon -Rv /var/lib/mysql-mysqld0 sudo touch /var/log/mariadb/mysql-mysqld0.log sudo chown mysql:mysql /var/log/mariadb/mysql-mysqld0.log sudo chcon --reference=/var/log/mariadb/mariadb.log /var/log/mariadb/mysql-mysqld0.log sudo mkdir /var/lib/mysql-mysqld1 sudo chown mysql:mysql /var/lib/mysql-mysqld1 sudo semanage fcontext -a -t mysqld_db_t \"/var/lib/mysql-mysqld1(/.*)?\" sudo restorecon -Rv /var/lib/mysql-mysqld1 sudo touch /var/log/mariadb/mysql-mysqld1.log sudo chown mysql:mysql /var/log/mariadb/mysql-mysqld1.log sudo chcon --reference=/var/log/mariadb/mariadb.log /var/log/mariadb/mysql-mysqld1.log  We now initialise the our two database data directories via\nsudo mysql_install_db --user=mysql --datadir=/var/lib/mysql-mysqld0 sudo mysql_install_db --user=mysql --datadir=/var/lib/mysql-mysqld1  We now replace the MySQL configuration file to set the options for each instance. Note that we will serve mysqld0 and mysqld1 via TCP ports 3307 and 3308 respectively. First backup the existing configuration file with\nsudo cp /etc/my.cnf /etc/my.cnf.ORIG  then setup /etc/my.cnf as per\nsudo bash F=/etc/my.cnf printf '[mysqld_multi]\\n' \u003e ${F} printf 'mysqld = /usr/bin/mysqld_safe --basedir=/usr\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf '[mysqld0]\\n' \u003e\u003e ${F} printf 'port=3307\\n' \u003e\u003e ${F} printf 'mysqld = /usr/bin/mysqld_safe --basedir=/usr\\n' \u003e\u003e ${F} printf 'datadir=/var/lib/mysql-mysqld0/\\n' \u003e\u003e ${F} printf 'socket=/var/lib/mysql-mysqld0/mysql.sock\\n' \u003e\u003e ${F} printf 'pid-file=/var/run/mariadb/mysql-mysqld0.pid\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf 'log-error=/var/log/mariadb/mysql-mysqld0.log\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf '# Disabling symbolic-links is recommended to prevent assorted security\\n' \u003e\u003e ${F} printf '# risks\\n' \u003e\u003e ${F} printf 'symbolic-links=0\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf '[mysqld1]\\n' \u003e\u003e ${F} printf 'mysqld = /usr/bin/mysqld_safe --basedir=/usr\\n' \u003e\u003e ${F} printf 'port=3308\\n' \u003e\u003e ${F} printf 'datadir=/var/lib/mysql-mysqld1/\\n' \u003e\u003e ${F} printf 'socket=/var/lib/mysql-mysqld1/mysql.sock\\n' \u003e\u003e ${F} printf 'pid-file=/var/run/mariadb/mysql-mysqld1.pid\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf 'log-error=/var/log/mariadb/mysql-mysqld1.log\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf '# Disabling symbolic-links is recommended to prevent assorted security risks\\n' \u003e\u003e ${F} printf 'symbolic-links=0\\n' \u003e\u003e ${F} exit # To exit the root shell  We also need to associate the ports with the mysqld_port_t SELinux context as per\nsudo semanage port -a -t mysqld_port_t -p tcp 3307 sudo semanage port -a -t mysqld_port_t -p tcp 3308  We next create the systemd service template as per\nsudo bash F=/etc/systemd/system/mysqld@.service printf '# Install in /etc/systemd/system\\n' \u003e ${F} printf '# Enable via systemctl enable mysqld@0 or systemctl enable mysqld@1\\n' \u003e\u003e ${F} printf '[Unit]\\n' \u003e\u003e ${F} printf 'Description=MySQL Multi Server for instance %%i\\n' \u003e\u003e ${F} printf 'After=syslog.target\\n' \u003e\u003e ${F} printf 'After=network.target\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf '[Service]\\n' \u003e\u003e ${F} printf 'User=mysql\\n' \u003e\u003e ${F} printf 'Group=mysql\\n' \u003e\u003e ${F} printf 'Type=forking\\n' \u003e\u003e ${F} printf 'ExecStart=/usr/bin/mysqld_multi start %%i\\n' \u003e\u003e ${F} printf 'ExecStop=/usr/bin/mysqld_multi stop %%i\\n' \u003e\u003e ${F} printf 'Restart=always\\n' \u003e\u003e ${F} printf 'PrivateTmp=true\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf '[Install]\\n' \u003e\u003e ${F} printf 'WantedBy=multi-user.target\\n' \u003e\u003e ${F} chmod 644 ${F} exit; # to exit the root shell  We next enable and start both instances via\nsudo systemctl enable mysqld@0 sudo systemctl enable mysqld@1 sudo systemctl start mysqld@0 sudo systemctl start mysqld@1  At this we should have both instances running. One should check each instance’s log file for any errors.\nSecure each database instance We secure each database engine by running the mysql_secure_installation script. One should accept all defaults, which means the only entry (aside from pressing returns) is the administrator (root) database password. Make a note of the password you use. In this case we will use Stroom5User@. The utility mysql_secure_installation expects to find the Linux socket file to access the database it’s securing at /var/lib/mysql/mysql.sock. Since we have used other locations, we temporarily link the real socket file to /var/lib/mysql/mysql.sock for each invocation of the utility. Thus we execute\nsudo ln /var/lib/mysql-mysqld0/mysql.sock /var/lib/mysql/mysql.sock sudo mysql_secure_installation  to see\nNOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY! In order to log into MariaDB to secure it, we'll need the current password for the root user. If you've just installed MariaDB, and you haven't set the root password yet, the password will be blank, so you should just press enter here. Enter current password for root (enter for none): OK, successfully used password, moving on... Setting the root password ensures that nobody can log into the MariaDB root user without the proper authorisation. Set root password? [Y/n] New password: \u003c__ Stroom5User@ __\u003e Re-enter new password: \u003c__ Stroom5User@ __\u003e Password updated successfully! Reloading privilege tables.. ... Success! By default, a MariaDB installation has an anonymous user, allowing anyone to log into MariaDB without having to have a user account created for them. This is intended only for testing, and to make the installation go a bit smoother. You should remove them before moving into a production environment. Remove anonymous users? [Y/n] ... Success! Normally, root should only be allowed to connect from 'localhost'. This ensures that someone cannot guess at the root password from the network. Disallow root login remotely? [Y/n] ... Success! By default, MariaDB comes with a database named 'test' that anyone can access. This is also intended only for testing, and should be removed before moving into a production environment. Remove test database and access to it? [Y/n] - Dropping test database... ... Success! - Removing privileges on test database... ... Success! Reloading the privilege tables will ensure that all changes made so far will take effect immediately. Reload privilege tables now? [Y/n] ... Success! Cleaning up... All done! If you've completed all of the above steps, your MariaDB installation should now be secure. Thanks for using MariaDB!  then we execute\nsudo rm /var/lib/mysql/mysql.sock sudo ln /var/lib/mysql-mysqld1/mysql.sock /var/lib/mysql/mysql.sock sudo mysql_secure_installation sudo rm /var/lib/mysql/mysql.sock  and process as before (for when running mysql_secure_installation). At this both database instances should be secure.\nMySQL Community Variant Create and instantiate both database instances To set up two MySQL database instances on the one node, we will use mysql_multi and systemd service templates. The mysql_multi utility is a capability that manages multiple MySQL databases on the same node and systemd service templates manage multiple services from one configuration file. A systemd service template is unique in that it has an @ character before the .service suffix.\nTo use this multiple-instance capability, we need to create two data directories for each database instance and also replace the main MySQL configuration file, /etc/my.cnf, with one that includes configuration of key options for each instance. We will name our instances, mysqld0 and mysqld1. We will also create specific log files for each instance.\nWe will use the directories, /var/lib/mysql-mysqld0 and /var/lib/mysql-mysqld1 for the data directories and /var/log/mysql-mysqld0.log and /var/log/mysql-mysqld1.log for the log directories. Note you should modify /etc/logrotate.d/mysql to manage these log files. Note also, we need to set the appropriate SELinux file context on the created directories and files.\nsudo mkdir /var/lib/mysql-mysqld0 sudo chown mysql:mysql /var/lib/mysql-mysqld0 sudo semanage fcontext -a -t mysqld_db_t \"/var/lib/mysql-mysqld0(/.*)?\" sudo restorecon -Rv /var/lib/mysql-mysqld0 sudo touch /var/log/mysql-mysqld0.log sudo chown mysql:mysql /var/log/mysql-mysqld0.log sudo chcon --reference=/var/log/mysqld.log /var/log/mysql-mysqld0.log sudo mkdir /var/lib/mysql-mysqld1 sudo chown mysql:mysql /var/lib/mysql-mysqld1 sudo semanage fcontext -a -t mysqld_db_t \"/var/lib/mysql-mysqld1(/.*)?\" sudo restorecon -Rv /var/lib/mysql-mysqld1 sudo touch /var/log/mysql-mysqld1.log sudo chown mysql:mysql /var/log/mysql-mysqld1.log sudo chcon --reference=/var/log/mysqld.log /var/log/mysql-mysqld1.log  We now initialise the our two database data directories via\nsudo mysql_install_db --user=mysql --datadir=/var/lib/mysql-mysqld0 sudo mysql_install_db --user=mysql --datadir=/var/lib/mysql-mysqld1  Disable the default database via\nsudo systemctl disable mysqld  We now modify the MySQL configuration file to set the options for each instance. Note that we will serve mysqld0 and mysqld1 via TCP ports 3307 and 3308 respectively. First backup the existing configuration file with\nsudo cp /etc/my.cnf /etc/my.cnf.ORIG  then setup /etc/my.cnf as per\nsudo bash F=/etc/my.cnf printf '[mysqld_multi]\\n' \u003e ${F} printf 'mysqld = /usr/bin/mysqld_safe --basedir=/usr\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf '[mysqld0]\\n' \u003e\u003e ${F} printf 'port=3307\\n' \u003e\u003e ${F} printf 'mysqld = /usr/bin/mysqld_safe --basedir=/usr\\n' \u003e\u003e ${F} printf 'datadir=/var/lib/mysql-mysqld0/\\n' \u003e\u003e ${F} printf 'socket=/var/lib/mysql-mysqld0/mysql.sock\\n' \u003e\u003e ${F} printf 'pid-file=/var/run/mysqld/mysql-mysqld0.pid\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf 'log-error=/var/log/mysql-mysqld0.log\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf '# Disabling symbolic-links is recommended to prevent assorted security\\n' \u003e\u003e ${F} printf '# risks\\n' \u003e\u003e ${F} printf 'symbolic-links=0\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf '[mysqld1]\\n' \u003e\u003e ${F} printf 'mysqld = /usr/bin/mysqld_safe --basedir=/usr\\n' \u003e\u003e ${F} printf 'port=3308\\n' \u003e\u003e ${F} printf 'datadir=/var/lib/mysql-mysqld1/\\n' \u003e\u003e ${F} printf 'socket=/var/lib/mysql-mysqld1/mysql.sock\\n' \u003e\u003e ${F} printf 'pid-file=/var/run/mysqld/mysql-mysqld1.pid\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf 'log-error=/var/log/mysql-mysqld1.log\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf '# Disabling symbolic-links is recommended to prevent assorted security risks\\n' \u003e\u003e ${F} printf 'symbolic-links=0\\n' \u003e\u003e ${F} exit # To exit the root shell  We also need to associate the ports with the mysqld_port_t SELinux context as per\nsudo semanage port -a -t mysqld_port_t -p tcp 3307 sudo semanage port -a -t mysqld_port_t -p tcp 3308  We next create the systemd service template as per\nsudo bash F=/etc/systemd/system/mysqld@.service printf '# Install in /etc/systemd/system\\n' \u003e ${F} printf '# Enable via systemctl enable mysqld@0 or systemctl enable mysqld@1\\n' \u003e\u003e ${F} printf '[Unit]\\n' \u003e\u003e ${F} printf 'Description=MySQL Multi Server for instance %%i\\n' \u003e\u003e ${F} printf 'After=syslog.target\\n' \u003e\u003e ${F} printf 'After=network.target\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf '[Service]\\n' \u003e\u003e ${F} printf 'User=mysql\\n' \u003e\u003e ${F} printf 'Group=mysql\\n' \u003e\u003e ${F} printf 'Type=forking\\n' \u003e\u003e ${F} printf 'ExecStart=/usr/bin/mysqld_multi start %%i\\n' \u003e\u003e ${F} printf 'ExecStop=/usr/bin/mysqld_multi stop %%i\\n' \u003e\u003e ${F} printf 'Restart=always\\n' \u003e\u003e ${F} printf 'PrivateTmp=true\\n' \u003e\u003e ${F} printf '\\n' \u003e\u003e ${F} printf '[Install]\\n' \u003e\u003e ${F} printf 'WantedBy=multi-user.target\\n' \u003e\u003e ${F} chmod 644 ${F} exit; # to exit the root shell  We next enable and start both instances via\nsudo systemctl enable mysqld@0 sudo systemctl enable mysqld@1 sudo systemctl start mysqld@0 sudo systemctl start mysqld@1  At this we should have both instances running. One should check each instance’s log file for any errors.\nSecure each database instance We secure each database engine by running the mysql_secure_installation script. One should accept all defaults, which means the only entry (aside from pressing returns) is the administrator (root) database password. Make a note of the password you use. In this case we will use Stroom5User@. The utility mysql_secure_installation expects to find the Linux socket file to access the database it’s securing at /var/lib/mysql/mysql.sock. Since we have used other locations, we temporarily link the real socket file to /var/lib/mysql/mysql.sock for each invocation of the utility. Thus we execute\nsudo ln /var/lib/mysql-mysqld0/mysql.sock /var/lib/mysql/mysql.sock sudo mysql_secure_installation  to see\nNOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MySQL SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY! In order to log into MySQL to secure it, we'll need the current password for the root user. If you've just installed MySQL, and you haven't set the root password yet, the password will be blank, so you should just press enter here. Enter current password for root (enter for none): OK, successfully used password, moving on... Setting the root password ensures that nobody can log into the MySQL root user without the proper authorisation. Set root password? [Y/n] y New password: \u003c__ Stroom5User@ __\u003e Re-enter new password: \u003c__ Stroom5User@ __\u003e Password updated successfully! Reloading privilege tables.. ... Success! By default, a MySQL installation has an anonymous user, allowing anyone to log into MySQL without having to have a user account created for them. This is intended only for testing, and to make the installation go a bit smoother. You should remove them before moving into a production environment. Remove anonymous users? [Y/n] ... Success! Normally, root should only be allowed to connect from 'localhost'. This ensures that someone cannot guess at the root password from the network. Disallow root login remotely? [Y/n] ... Success! By default, MySQL comes with a database named 'test' that anyone can access. This is also intended only for testing, and should be removed before moving into a production environment. Remove test database and access to it? [Y/n] - Dropping test database... ERROR 1008 (HY000) at line 1: Can't drop database 'test'; database doesn't exist ... Failed! Not critical, keep moving... - Removing privileges on test database... ... Success! Reloading the privilege tables will ensure that all changes made so far will take effect immediately. Reload privilege tables now? [Y/n] ... Success! All done! If you've completed all of the above steps, your MySQL installation should now be secure. Thanks for using MySQL! Cleaning up...  then we execute\nsudo rm /var/lib/mysql/mysql.sock sudo ln /var/lib/mysql-mysqld1/mysql.sock /var/lib/mysql/mysql.sock sudo mysql_secure_installation sudo rm /var/lib/mysql/mysql.sock  and process as before (for when running mysql_secure_installation). At this both database instances should be secure.\nCreate the Databases and Enable access by the Stroom processing users We now create the stroom database within the first instance, mysqld0 and the statistics database within the second instance mysqld1. It does not matter which database variant used as all commands are the same for both.\nAs well as creating the databases, we also need to establish the Stroom processing users that the Stroom processing nodes will use to access each database. For the stroom database, we will use the database user stroomuser with a password of Stroompassword1@ and for the statistics database, we will use the database user stroomstats with a password of Stroompassword2@. One identifies a processing user as \u003cuser\u003e@\u003chost\u003e on a grant SQL command.\nIn the stroom database instance, we will grant access for\n stroomuser@localhost for local access for maintenance etc. stroomuser@stroomp00.strmdev00.org for access by processing node stroomp00.strmdev00.org stroomuser@stroomp01.strmdev00.org for access by processing node stroomp01.strmdev00.org  and in the statistics database instance, we will grant access for\n stroomstats@localhost for local access for maintenance etc. stroomstats@stroomp00.strmdev00.org for access by processing node stroomp00.strmdev00.org stroomstats@stroomp01.strmdev00.org for access by processing node stroomp01.strmdev00.org  Thus for the stroom database we execute\nmysql --user=root --port=3307 --socket=/var/lib/mysql-mysqld0/mysql.sock --password  and on entering the administrator’s password, we arrive at the MariaDB [(none)]\u003e or mysql\u003e prompt. At this we create the database with\ncreate database stroom;  and then to establish the users, we execute\ngrant all privileges on stroom.* to stroomuser@localhost identified by 'Stroompassword1@'; grant all privileges on stroom.* to stroomuser@stroomp00.strmdev00.org identified by 'Stroompassword1@'; grant all privileges on stroom.* to stroomuser@stroomp01.strmdev00.org identified by 'Stroompassword1@';  then\nquit;  to exit.\nAnd for the statistics database\nmysql --user=root --port=3308 --socket=/var/lib/mysql-mysqld1/mysql.sock --password  with\ncreate database statistics;  and then to establish the users, we execute\ngrant all privileges on statistics.* to stroomstats@localhost identified by 'Stroompassword2@'; grant all privileges on statistics.* to stroomstats@stroomp00.strmdev00.org identified by 'Stroompassword2@'; grant all privileges on statistics.* to stroomstats@stroomp01.strmdev00.org identified by 'Stroompassword2@';  then\nquit;  to exit.\nClearly if we need to add more processing nodes, additional grant commands would be used. Further, if we were installing the databases in a single node Stroom environment, we would just have the first two pairs of grants.\nConfigure Firewall Next we need to modify our firewall to allow remote access to our databases which listens on ports 3307 and 3308. The simplest way to achieve this is with the commands\nsudo firewall-cmd --zone=public --add-port=3307/tcp --permanent sudo firewall-cmd --zone=public --add-port=3308/tcp --permanent sudo firewall-cmd --reload sudo firewall-cmd --zone=public --list-all  Note That this allows ANY node to connect to your databases. You should give consideration to restricting this to only allowing processing node access.  Debugging of Mariadb for Stroom If there is a need to debug the Mariadb database and Stroom interaction, one can turn on auditing for the Mariadb service. To do so, log onto the relevant database as the administrative user as per\nmysql --user=root --port=3307 --socket=/var/lib/mysql-mysqld0/mysql.sock --password or mysql --user=root --port=3308 --socket=/var/lib/mysql-mysqld1/mysql.sock --password  and at the MariaDB [(none)]\u003e  prompt enter\ninstall plugin server_audit SONAME 'server_audit'; set global server_audit_file_path='/var/log/mariadb/mysqld-mysqld0_server_audit.log'; or set global server_audit_file_path='/var/log/mariadb/mysqld-mysqld1_server_audit.log'; set global server_audit_logging=ON; set global server_audit_file_rotate_size=10485760; install plugin SQL_ERROR_LOG soname 'sql_errlog'; quit;  The above will generate two log files,\n /var/log/mariadb/mysqld-mysqld0_server_audit.log or /var/log/mariadb/mysqld-mysqld1_server_audit.log which records all commands the respective databases run. We have configured the log file will rotate at 10MB in size. /var/lib/mysql-mysqld0/sql_errors.log or /var/lib/mysql-mysqld1/sql_errors.log which records all erroneous SQL commands. This log file will rotate at 10MB in size. Note we cannot set this filename via the UI, but it will be appear in the data directory.  All files will, by default, generate up to 9 rotated files.\nIf you wish to rotate a log file manually, log into the database as the administrative user and execute either\n set global server_audit_file_rotate_now=1; to rotate the audit log file set global sql_error_log_rotate=1; to rotate the sql_errlog log file  Initial Database Access It should be noted that if you monitor the sql_errors.log log file on a new Stooom deployment, when the Stoom Application first starts, it’s initial access to the stroom database will result in the following attempted sql statements.\n2017-04-16 16:24:50 stroomuser[stroomuser] @ stroomp00.strmdev00.org [192.168.2.126] ERROR 1146: Table 'stroom.schema_version' doesn't exist : SELECT version FROM schema_version ORDER BY installed_rank DESC 2017-04-16 16:24:50 stroomuser[stroomuser] @ stroomp00.strmdev00.org [192.168.2.126] ERROR 1146: Table 'stroom.STROOM_VER' doesn't exist : SELECT VER_MAJ, VER_MIN, VER_PAT FROM STROOM_VER ORDER BY VER_MAJ DESC, VER_MIN DESC, VER_PAT DESC LIMIT 1 2017-04-16 16:24:50 stroomuser[stroomuser] @ stroomp00.strmdev00.org [192.168.2.126] ERROR 1146: Table 'stroom.FD' doesn't exist : SELECT ID FROM FD LIMIT 1 2017-04-16 16:24:50 stroomuser[stroomuser] @ stroomp00.strmdev00.org [192.168.2.126] ERROR 1146: Table 'stroom.FEED' doesn't exist : SELECT ID FROM FEED LIMIT 1  After this access the application will realise the database does not exist and it will initialise the database.\nIn the case of the statistics database you may note the following attempted access\n2017-04-16 16:25:09 stroomstats[stroomstats] @ stroomp00.strmdev00.org [192.168.2.126] ERROR 1146: Table 'statistics.schema_version' doesn't exist : SELECT version FROM schema_version ORDER BY installed_rank DESC  Again, at this point the application will initialise this database.\n","categories":"","description":"This HOWTO describes the installation of the Stroom databases.\n","excerpt":"This HOWTO describes the installation of the Stroom databases.\n","ref":"/stroom-docs/hugo-docsy/docs/howtos/install/installdatabasehowto/","tags":["database","installation"],"title":"Database Installation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/elastic-search/","tags":"","title":"elastic-search"},{"body":"Introduction Stroom v6.1 can pass data to Elasticsearch for indexing. Indices created using this process (i.e. those containing a StreamId and EventId corresponding to a particular Stroom instance) are searchable via a Stroom dashboard, much like a Stroom Lucene index.\nThis integration provides operators with the flexibility to utilise the additional capabilities of Elasticsearch, (like clustering and replication) and expose indexed data for consumption by external analytic or processing tools.\nThis guide will take you through creating an Elasticsearch index, setting up an indexing pipeline, activating a stream processor and searching the indexed data in both Stroom and Elasticsearch.\nAssumptions  You have created an Elasticsearch cluster. For test purposes, you can quickly create a single-node cluster using Docker by following the steps in the Elasticsearch Docs (external link). The Elasticsearch cluster is reachable via HTTP/S from all Stroom nodes participating in stream processing. Elasticsearch security is disabled. You have a feed containing Event data.  Key differences  Unlike with Solr indexing, Elasticsearch field mappings are managed outside of Stroom, usually via the REST API (external link). Aside from creating the mandatory StreamId and EventId field mappings, explicitly defining mappings for other fields is optional. It is however, considered good practice to define these mappings, to ensure each field’s data type is correctly parsed and represented. For text fields, it also pays to ensure that the appropriate mapping parameters are used (external link), in order to satisfy your search and analysis requirements - and meet system resource constraints. Unlike both Solr and Lucene indexing, it is not necessary to mark a field as stored (i.e. storing its raw value in the inverted index). This is because Elasticsearch stores the content of the original document in the _source field (external link), which is retrieved when populating search results. Provided the _source field is enabled (as it is by default), a field is treated as stored in Stroom and its value doesn’t need to be retrieved via an extraction pipeline.  Indexing data Creating an index in Elasticsearch The following cURL command creates an index named stroom_test in Elasticsearch cluster http://localhost:9200 consisting of the following fields:\n StreamId (mandatory, must be of data type long) EventId (mandatory, must also be long) Name (text). Uses the default analyzer, which tokenizes the text for matching on terms. fielddata is enabled, which allows for aggregating on these terms (external link). State (keyword). Supports exact matching.  The created index consists of 5 shards. Note that the shard count cannot be changed after index creation, without a reindex. See this guide (external link) on shard sizing.\ncurl -X PUT \"http://localhost:9200/stroom_test?pretty\" -H 'Content-Type: application/json' -d' { \"settings\": { \"number_of_shards\": 5 }, \"mappings\": { \"properties\": { \"StreamId\": { \"type\": \"long\" }, \"EventId\": { \"type\": \"long\" }, \"Name\": { \"type\": \"text\", \"fielddata\": true }, \"State\": { \"type\": \"text\", \"fielddata\": true } } } } '  After creating the index, you can add additional field mappings. Note the limitations (external link) in doing so, particularly the fact that it will not cause existing documents to be re-indexed. It is worthwhile to test index mappings on a subset of data before committing to indexing a large event feed, to ensure the resulting search experience meets your requirements.\nRegistering the index in Stroom This step creates an Elasticsearch Index in the Stroom Tree and tells Stroom how to connect to your Elasticsearch cluster and index. Note that this process needs to be repeated for each index you create.\nSteps  Right-click on the folder in the Explorer Tree where you wish to create the index Select New / Elasticsearch Index Enter a valid name for the index. It is a good idea to choose one that reflects either the feed name being indexed, or if indexing multiple feeds, the nature of data they represent. In the index tab that just opened:  Select the Settings tab Set the Index to the name of the index in Elasticsearch (e.g. stroom_test from the previous example) Set the Connection URLs to one or more Elasticsearch node URLs. If multiple, separate each URL with ,. For example, a URL like http://data-0.elastic:9200,http://data-1.elastic:9200 will balance requests to two data nodes within an Elasticsearch cluster. See this document for guidance on node roles. Click Test Connection. If the connection succeeds, and the index is found, a dialog is shown indicating the test was successful. Otherwise, an error message is displayed. If the test succeeded, click the save button in the top-left. The Fields tab will now be populated with fields from the Elasticsearch index.    Note The field mappings list is only updated when index settings are changed, or a Stroom indexing or search task begins. The refresh button in the Fields tab does not have any effect.  Setting index retention As with Solr indexing, index document retention is determined by defining a Stroom query.\nSetting a retention query is optional and by default, documents will be retained in an index indefinitely.\nIt is recommended for indices containing events spanning long periods of time, that Elasticsearch Index Lifecycle Management (external link) be used instead. The capabilities provided, such as automatic rollover to warm or cold storage tiers, are well worth considering, especially in high-volume production clusters.\nConsiderations when implementing ILM  It is recommended that data streams are used when indexing data. These allow easier rollover and work well with ILM policies. A data stream is essentially a container for multiple date-based indices and to a search client such as Stroom, appears and is searchable like a normal Elasticsearch index. Use of data streams requires that a @timestamp field of type date be defined for each document (instead of say, EventTime). Implementing ILM policies requires careful capacity planning, including anticipating search and retention requirements.  Creating an indexing pipeline As with Lucene and Solr indexing pipelines, indexing data using Elasticsearch uses a pipeline filter. This filter accepts \u003crecord\u003e elements and for each, sends a document to Elasticsearch for indexing.\nEach \u003cdata\u003e element contained within a \u003crecord\u003e sets the document field name and value. You should ensure the name attribute of each \u003cdata\u003e element exactly matches the mapping property of the Elasticsearch index you created.\nSteps  Create a pipeline inheriting from the built-in Indexing template. Modify the xsltFilter pipeline stage to output the correct \u003crecords\u003e XML (see the Quick-Start Guide. Delete the default indexingFilter and in its place, create an ElasticIndexingFilter (see screenshot below). Review and set the following properties:  batchSize (default: 10,000). Number of documents to send in a single request to the Elasticsearch Bulk API (external link). Should usually be set to 1,000 or more. The higher the number, the more memory is required by both Stroom and Elasticsearch when sending or receiving the request. index (required). Set this to the target Elasticsearch index in the Stroom Explorer Tree. refreshAfterEachBatch (default: false). Refreshes the Elasticsearch index after each batch has finished processing. This makes any documents ingested in the batch available for searching. Unless search results are needed in near-real-time, it is recommended this be set to false and the index refresh interval be set to an appropriate value. See this document (external link) for guidance on optimising indexing performance.    Elasticsearch indexing filter    Creating and activating a stream processor Follow the steps as in this guide.\nChecking data has been indexed Query Elasticsearch, checking the fields you expect are there, and of the correct data type:\nThe following query displays five results:\ncurl -X GET \"http://localhost:9200/stroom_test/_search?size=5\"  You can also get an exact document count, to ensure this matches the number of events you are expecting:\ncurl -X GET \"http://localhost:9200/stroom_test/_count\"  For more information, see the Elasticsearch Search API documentation (external link).\nReindexing data By default, the original document values are stored in an Elasticsearch index and may be used later on to re-index data (such as when a change is made to field mappings). This is done via the Reindex API (external link). Provided these values have not changed, it would likely be more efficient to use this API to perform a re-index, instead of processing data from scratch using a Stroom stream processor.\nOn the other hand, if the content of documents being output to Elasticsearch has changed, the Elasticsearch index will need to be re-created and the stream re-processed. Examples of where this would be required include:\n A new field is added to the indexing filter, which previously didn’t exist. That field needs to be searchable for all historical events. A field is renamed A field data type is changed  If a field is omitted from the indexing translation, there is no need for a re-index, unless you wish to reclaim the space occupied by that field.\nReindexing using a pipeline processor  Delete the index. While it is possible to delete by query (external link), it is more efficient to drop the index. Additionally, deleting by query doesn’t actually remove data from disk, until segments are merged. curl -X DELETE \"http://localhost:9200/stroom_test\"   Re-create the index (as shown earlier) Create a new pipeline processor to index the documents  Searching Once indexed in Elasticsearch, you can search either using the Stroom Dashboard user interface, or directly against the Elasticsearch cluster.\nThe advantage of using Stroom to search is that it allows access to the raw source data (i.e. it is not limited to what’s stored in the index). It can also use extraction pipelines to enrich search results for export in a table.\nElasticsearch on the other hand, provides a rich Search REST API (external link) with powerful aggregations that can be used to generate reports and discover patterns and anomalies. It can also be readily queried using third-party tools.\nStroom See the Dashboard page in the Quick-Start Guide.\nInstead of selecting a Lucene index, set the target data source to the desired Elasticsearch index in the Stroom Explorer Tree.\nOnce the target data source has been set, the Dashboard can be used as with a Lucene or Solr index data source.\nElasticsearch Elasticsearch queries can be performed directly against the cluster using the Search API (external link).\nAlternatively, there are tools that make search and discovery easier and more intuitive, like Kibana (external link).\nSecurity It is important to note that Elasticsearch data is not encrypted at rest, unless this feature is enabled and the relevant licensing tier (external link) is purchased. Therefore, appropriate measures should be taken to control access to Elasticsearch user data at the file level.\nFor production clusters, the Elasticsearch security guidelines (external link) should be followed, in order to control access and ensure requests are audited.\nYou might want to consider implementing role-based access control (external link) to prevent unauthorised users of the native Elasticsearch API or tools like Kibana, from creating, modifying or deleting data within sensitive indices.\n","categories":"","description":"How to integrate Stroom with Elastic Search\n","excerpt":"How to integrate Stroom with Elastic Search\n","ref":"/stroom-docs/hugo-docsy/docs/howtos/search/elasticsearch/","tags":["search","elastic-search"],"title":"Elasticsearch integration"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/httpd/","tags":"","title":"httpd"},{"body":"","categories":"","description":"Various How Tos convering installation of Stroom and its dependencies\n","excerpt":"Various How Tos convering installation of Stroom and its dependencies\n","ref":"/stroom-docs/hugo-docsy/docs/howtos/install/","tags":["installation"],"title":"Installation"},{"body":"Assumptions The following assumptions are used in this document.\n the user has reasonable RHEL/Centos System administration skills. installations are on Centos 7.3 minimal systems (fully patched). the term ‘node’ is used to reference the ‘host’ a service is running on. the Stroom Proxy and Application software runs as user ‘stroomuser’ and will be deployed in this user’s home directory data will reside in a directory tree referenced via ‘/stroomdata’. It is up to the user to provision a filesystem here, noting sub-directories of it will be NFS shared in Multi Node Stroom Deployments any scripts or commands that should run are in code blocks and are designed to allow the user to cut then paste the commands onto their systems in this document, when a textual screen capture is documented, data entry is identified by the data surrounded by ‘\u003c’ ‘\u003e’ . This excludes enter/return presses. better security of password choices, networking, firewalls, data stores, etc. can and should be achieved in various ways, but these HOWTOs are just a quick means of getting a working system, so only limited security is applied better configuration of the database (e.g. more memory. redundancy) should be considered in production environments the use of self signed certificates is appropriate for test systems, but users should consider appropriate CA infrastructure in production environments the user has access to a Chrome (external link) web browser as Stroom is optimised for this browser.  Introduction This HOWTO provides guidance on a variety of simple Stroom deployments.\n Multi Node Stroom Cluster (Proxy and Application)  for an environment where multiple nodes are required to handle the processing load.\n Forwarding Stroom Proxy  for extensive networks where one wants to aggregate data through a proxy before sending data to the central Stroom processing systems.\n Standalone Stroom Proxy  for disconnected networks where collected data can be manually transferred to a Stroom processing service.\n Addition of a Node to Stroom Cluster  for when one needs to add an additional node to an existing cluster.\nNodename Nomenclature For simplicity sake, the nodenames used in this HOWTO are geared towards the Multi Node Stroom Cluster deployment. That is,\n the database nodename is stroomdb0.strmdev00.org the processing nodenames are stroomp00.strmdev00.org, stroomp01.strmdev00.org, and stroomp02.strmdev00.org the first node in our cluster, stroomp00.strmdev00.org, also has the CNAME stroomp.strmdev00.org  In the case of the Proxy only deployments,\n the forwarding Stroom proxy nodename is stoomfp0.strmdev00.org the standalone nodename will be stroomp00.strmdev00.org  Storage Both the Stroom Proxy and Application store data. The typical requirement is\n directory for Stroom proxy to store inbound data files directory for Stroom application permanent data files (events, etc.) directory for Stroom application index data files directory for Stroom application working files (temporary files, output, etc.)  Where multiple processing nodes are involved, the application’s permanent data directories need to be accessible by all participating nodes.\nThus a hierarchy for a Stroom Proxy might by\n /stroomdata/stroom-proxy  and for an Application node\n /stroomdata/stroom-data /stroomdata/stroom-index /stroomdata/stroom-working  In the following examples, the storage hierarchy proposed will more suited for a multi node Stroom cluster, including the Forwarding or Standalone proxy deployments. This is to simplify the documentation. Thus, the above structure is generalised into\n /stroomdata/stroom-working-p_nn_/proxy  and\n /stroomdata/stroom-data-p_nn_ /stroomdata/stroom-index-p_nn_ /stroomdata/stroom-working-p_nn_  where nn is a two digit node number. The reason for placing the proxy directory within the Application working area will be explained later.\nAll data should be owned by the Stroom processing user. In this HOWTO, we will use stroomuser\nMulti Node Stroom Cluster (Proxy and Application) Deployment In this deployment we will install the database on a given node then deploy both the Stroom Proxy and Stroom Application software to both our processing nodes. At this point we will then integrate a web service to run ‘in-front’ of our Stroom software and then perform the initial configuration of Stroom via the user interface.\nDatabase Installation The Stroom capability requires access to two MySQL/MariaDB databases. The first is for persisting application configuration and metadata information, and the second is for the Stroom Statistics capability. Instructions for installation of the Stroom databases can be found here. Although these instructions describe the deployment of the databases to their own node, there is no reason why one can’t just install them both on the first (or only) Stroom node.\nPrerequisite Software Installation Certain software packages are required for either the Stroom Proxy or Stroom Application to run.\nThe core software list is\n java-1.8.0-openjdk java-1.8.0-openjdk-devel policycoreutils-python unzip zip mariadb or mysql client  Most of the required software are packages available via standard repositories and hence we can simply execute\nsudo yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel policycoreutils-python unzip zip  One has a choice of database clients. MariaDB is directly supported by Centos 7 and is simplest to install. This is done via\nsudo yum -y install mariadb  One could deploy the MySQL database software as the alternative.\nTo do this you need to install the MySQL Community repository files then install the client. Instructions for installation of the MySQL Community repository files can be found here or on the MySQL Site (external link). Once you have installed the MySQL repository files, install the client via\nsudo yum -y install mysql-community-client  Note that additional software will be required for other integration components (e.g. Apache httpd/mod_jk). This is described in the Web Service Integration section of this document.\nNote also, that Standalone or Forwarding Stroom Proxy deployments do NOT need a database client deployed.\nEntropy Issues in Virtual environments Both the Stroom Application and Stroom Proxy currently run on Tomcat (Version 7) which relies on the Java SecureRandom class to provide random values for any generated session identifiers as well as other components. In some circumstances the Java runtime can be delayed if the entropy source that is used to initialise SecureRandom is short of entropy. The delay is caused by the Java runtime waiting on the blocking entropy souce /dev/random to have sufficient entropy. This quite often occurs in virtual environments were there are few sources that can contribute to a system’s entropy.\nTo view the current available entropy on a Linux system, run the command\ncat /proc/sys/kernel/random/entropy_avail  A reasonable value would be over 2000 and a poor value would be below a few hundred.\nIf you are deploying Stroom onto systems with low available entropy, the start time for the Stroom Proxy can be as high as 5 minutes and for the Application as high as 15 minutes.\nOne software based solution would be to install the haveged (external link) service that attempts to provide an easy-to-use, unpredictable random number generator based upon an adaptation of the HAVEGE algorithm. To install execute\nyum -y install haveged systemctl enable haveged systemctl start haveged  For background reading in this matter, see this reference (external link) or this reference (external link).\nStorage Scenario For the purpose of this Installation HOWTO, the following sets up the storage hierarchy for a two node processing cluster. To share our permanent data we will use NFS. Accept that the NFS deployment described here is very simple, and in a production deployment, a lot more security controls should be used. Further,\nOur hierarchy is\n Node: stroomp00.strmdev00.org /stroomdata/stroom-data-p00\t- location to store Stroom application data files (events, etc.) for this node /stroomdata/stroom-index-p00\t- location to store Stroom application index files /stroomdata/stroom-working-p00\t- location to store Stroom application working files (e.g. temporary files, output, etc.) for this node /stroomdata/stroom-working-p00/proxy\t- location for Stroom proxy to store inbound data files Node: stroomp01.strmdev00.org /stroomdata/stroom-data-p01\t- location to store Stroom application data files (events, etc.) for this node /stroomdata/stroom-index-p01\t- location to store Stroom application index files /stroomdata/stroom-working-p01\t- location to store Stroom application working files (e.g. temporary files, output, etc.) for this node /stroomdata/stroom-working-p01/proxy\t- location for Stroom proxy to store inbound data files  Creation of Storage Hierarchy So, we first create processing user on all nodes as per\nsudo useradd --system stroomuser  And the relevant commands to create the above hierarchy would be\n Node: stroomp00.strmdev00.org  sudo mkdir -p /stroomdata/stroom-data-p00 /stroomdata/stroom-index-p00 /stroomdata/stroom-working-p00 /stroomdata/stroom-working-p00/proxy sudo mkdir -p /stroomdata/stroom-data-p01 # So that this node can mount stroomp01's data directory sudo chown -R stroomuser:stroomuser /stroomdata sudo chmod -R 750 /stroomdata   Node: stroomp01.strmdev00.org  sudo mkdir -p /stroomdata/stroom-data-p01 /stroomdata/stroom-index-p01 /stroomdata/stroom-working-p01 /stroomdata/stroom-working-p01/proxy sudo mkdir -p /stroomdata/stroom-data-p00 # So that this node can mount stroomp00's data directory sudo chown -R stroomuser:stroomuser /stroomdata sudo chmod -R 750 /stroomdata  Deployment of NFS to share Stroom Storage We will use NFS to cross mount the permanent data directories. That is\n node stroomp00.strmdev00.org will mount stroomp01.strmdev00.org:/stroomdata/stroom-data-p01 and, node stroomp01.strmdev00.org will mount stroomp00.strmdev00.org:/stroomdata/stroom-data-p00.  The HOWTO guide to deploy and configure NFS for our Scenario is here\nStroom Installation Pre-installation setup Before installing either the Stroom Proxy or Stroom Application, we need establish various files and scripts within the Stroom Processing user’s home directory to support the Stroom services and their persistence. This is setup is described here.\nStroom Proxy Installation Instructions for installation of the Stroom Proxy can be found here.\nStroom Application Installation Instructions for installation of the Stroom application can be found here.\nWeb Service Integration One typically ‘fronts’ either a Stroom Proxy or Stroom Application with a secure web service such as Apache’s Httpd or NGINX. In our scenario, we will use SSL to secure the web service and further, we will use Apache’s Httpd.\nWe first need to create certificates for use by the web service. The following provides instructions for this. The created certificates can then be used when configuration the web service.\nThis HOWTO is designed to deploy Apache’s httpd web service as a front end (https) (to the user) and Apache’s mod_jk as the interface between Apache and the Stroom tomcat applications. The instructions to configure this can be found here.\nOther Web service capability can be used, for example, NGINX (external link).\nInstallation Validation We will now check that the installation and web services integration has worked.\nSanity firewall check To ensure you have the firewall correctly set up, the following command\nsudo firewall-cmd --reload sudo firewall-cmd --zone=public --list-all  should result in\npublic (active) target: default icmp-block-inversion: no interfaces: enp0s3 sources: services: dhcpv6-client http https nfs ssh ports: 8009/tcp 9080/tcp 8080/tcp 9009/tcp protocols: masquerade: no forward-ports: sourceports: icmp-blocks: rich rules:  Test Posting of data to the Stroom service You can test the data posting service with the command\ncurl -k --data-binary @/etc/group \"https://stroomp.strmdev00.org/stroom/datafeed\" -H \"Feed:TEST-FEED-V1_0\" -H \"System:EXAMPLE_SYSTEM\" -H \"Environment:EXAMPLE_ENVIRONMENT\"  which WILL result in an error as we have not configured the Stroom Application as yet. The error should look like\n\u003chtml\u003e\u003chead\u003e\u003ctitle\u003eApache Tomcat/7.0.53 - Error report\u003c/title\u003e\u003cstyle\u003e\u003c!--H1 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:22px;} H2 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:16px;} H3 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:14px;} BODY {font-family:Tahoma,Arial,sans-serif;color:black;background-color:white;} B {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;} P {font-family:Tahoma,Arial,sans-serif;background:white;color:black;font-size:12px;}A {color : black;}A.name {color : black;}HR {color : #525D76;}--\u003e\u003c/style\u003e \u003c/head\u003e\u003cbody\u003e\u003ch1\u003eHTTP Status 406 - Stroom Status 110 - Feed is not set to receive data - \u003c/h1\u003e\u003cHR size=\"1\" noshade=\"noshade\"\u003e\u003cp\u003e\u003cb\u003etype\u003c/b\u003e Status report\u003c/p\u003e\u003cp\u003e\u003cb\u003emessage\u003c/b\u003e \u003cu\u003eStroom Status 110 - Feed is not set to receive data - \u003c/u\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003edescription\u003c/b\u003e \u003cu\u003eThe resource identified by this request is only capable of generating responses with characteristics not acceptable according to the request \"accept\" headers.\u003c/u\u003e\u003c/p\u003e\u003cHR size=\"1\" noshade=\"noshade\"\u003e\u003ch3\u003eApache Tomcat/7.0.53\u003c/h3\u003e\u003c/body\u003e\u003c/html\u003e  If you view the Stroom proxy log, ~/stroom-proxy/instance/logs/stroom.log, on both processing nodes, you will see on one node, the datafeed.DataFeedRequestHandler events running under, in this case, the ajp-apr-9009-exec-1 thread indicating the failure\n... 2017-01-03T03:35:47.366Z WARN [ajp-apr-9009-exec-1] datafeed.DataFeedRequestHandler (DataFeedRequestHandler.java:131) - \"handleException()\",\"Environment=EXAMPLE_ENVIRONMENT\",\"Expect=100-continue\",\"Feed=TEST-FEED-V1_0\",\"GUID=39960cf9-e50b-4ae8-a5f2-449ee670d2eb\",\"ReceivedTime=2017-01-03T03:35:46.915Z\",\"RemoteAddress=192.168.2.220\",\"RemoteHost=192.168.2.220\",\"System=EXAMPLE_SYSTEM\",\"accept=*/*\",\"content-length=1051\",\"content-type=application/x-www-form-urlencoded\",\"host=stroomp.strmdev00.org\",\"user-agent=curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.21 Basic ECC zlib/1.2.3 libidn/1.18 libssh2/1.4.2\",\"Stroom Status 110 - Feed is not set to receive data\" 2017-01-03T03:35:47.367Z ERROR [ajp-apr-9009-exec-1] zip.StroomStreamException (StroomStreamException.java:131) - sendErrorResponse() - 406 Stroom Status 110 - Feed is not set to receive data - 2017-01-03T03:35:47.368Z INFO [ajp-apr-9009-exec-1] datafeed.DataFeedRequestHandler$1 (DataFeedRequestHandler.java:104) - \"doPost() - Took 478 ms to process (concurrentRequestCount=1) 406\",\"Environment=EXAMPLE_ENVIRONMENT\",\"Expect=100-continue\",\"Feed=TEST-FEED-V1_0\",\"GUID=39960cf9-e50b-4ae8-a5f2-449ee670d2eb\",\"ReceivedTime=2017-01-03T03:35:46.915Z\",\"RemoteAddress=192.168.2.220\",\"RemoteHost=192.168.2.220\",\"System=EXAMPLE_SYSTEM\",\"accept=*/*\",\"content-length=1051\",\"content-type=application/x-www-form-urlencoded\",\"host=stroomp.strmdev00.org\",\"user-agent=curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.21 Basic ECC zlib/1.2.3 libidn/1.18 libssh2/1.4.2\" ...  Further, if you execute the data posting command (curl) multiple times, you will see the loadbalancer working in that, the above WARN/ERROR/INFO logs will swap between the proxy services (i.e. first error will be in stroomp00.strmdev00.org’s proxy log file, then second on stroomp01.strmdev00.org’s proxy log file, then back to stroomp00.strmdev00.org and so on).\nStroom Application Configuration Although we have installed our multi node Stroom cluster, we now need to configure it. We do this via the user interface (UI).\nLogging into the Stroom UI for the first time To log into the UI of your newly installed Stroom instance, present the base URL to your Chrome (external link) browser. In this deployment, you should enter the URLS http://stroomp.strmdev00.org, or https://stroomp.strmdev00.org or https://stroomp.strmdev00.org/stroom, noting the first URLs should automatically direct you to the last URL.\nIf you have personal certificates loaded in your Chrome browser, you may be asked which certificate to use to authenticate yourself to stroomp.strmdev00.org:443. As Stroom has not been configured to use user certificates, the choice is not relevant, just choose one and continue.\nAdditionally, if you are using self-signed certificates, your browser will generate an alert as per Self Signed Certificate Initial Warning    To proceed you need to select the ADVANCED hyperlink to see\nSelf Signed Certificate Advanced Warning    If you select the Proceed to stroomp.strmdev00.org (unsafe) hyper-link you will be presented with the standard Stroom UI login page.\nStroom UI Login Page    This page has two panels - About Stroom and Login.\nIn the About Stroom panel we see an introductory description of Stroom in the top left and deployment details in the bottom left of the panel. The deployment details provide\n Build Version: - the build version of the Stroom application deployed Build Date: - the date the version was built Up Date: - the install date Node Name: - the node within the Stroom cluster you have connected to  Login with Stroom default Administrative User Each new Stroom deployment automatically creates the administrative user admin and this user’s password is initially set to admin. We will login as this user which also validates that the database and UI is working correctly in that you can login and the password is admin.\nCreate an Attributed User to perform configuration We should configure Stroom using an attributed user account. That is, we should create a user, in our case it will be burn (the author) and once created, we login with that account then perform the initial configuration activities. You don’t have to do this, but it is sound security practice.\nOnce you have created the user you should log out of the admin account and log back in as our user burn.\nConfigure the Volumes for our Stroom deployment Before we can store data within Stroom we need to configure the volumes we have allocated in our Storage hierarchy. The Volume Maintenance HOWTO shows how to do this.\nConfigure the Nodes for our Stroom deployment In a Stroom cluster, nodes are expected to communicate with each other on port 8080 over http. Our installation in a multi node environment ensures the firewall will allow this but we also need to configure the nodes. This is achieved via the Stroom UI where we set a Cluster URL for each node. The following Node Configuration HOWTO demonstrates how do set the Cluster URL.\nData Stream Processing To enable Stroom to process data, it’s Data Processors need to be enabled. There are NOT enabled by default on installation. The following section in our Stroom Tasks HowTo shows how to do this.\nTesting our Stroom Application and Proxy Installation To complete the installation process we will test that we can send and ingest data.\nAdd a Test Feed In order for Stroom to be able to handle various data sources, be they Apache HTTPD web access logs, MicroSoft Windows Event logs or Squid Proxy logs, Stroom must be told what the data is when it is received. This is achieved using Event Feeds. Each feed has a unique name within the system.\nTo test our installation can accept and ingest data, we will create a test Event feed. The ‘name’ of the feed will be TEST-FEED-V1_0. Note that in a production environment is is best that a well defined nomenclature is used for feed ‘names’. For our testing purposes TEST-FEED-V1_0 is sufficient.\nSending Test Data NOTE: Before testing our new feed, we should restart both our Stroom application services so that any volume changes are propagated. This can be achieved by simply running\nsudo -i -u stroomuser bin/StopServices.sh bin/StartServices.sh  on both nodes. It is suggested you first log out of Stroom, if you are currently logged in and you should monitor the Stroom application logs to ensure it has successfully restarted. Remember to use the T and Tp bash aliases we set up.\nFor this test, we will send the contents of /etc/group to our test feed. We will also send the file from the cluster’s database machine. The command to send this file is\ncurl -k --data-binary @/etc/group \"https://stroomp.strmdev00.org/stroom/datafeed\" -H \"Feed:TEST-FEED-V1_0\" -H \"System:EXAMPLE_SYSTEM\" -H \"Environment:EXAMPLE_ENVIRONMENT\"  We will test a number of features as part of our installation test. These are\n simple post of data simple post of data to validate load balancing is working simple post to direct feed interface simple post to direct feed interface to validate load balancing is working identify that the Stroom Proxy Aggregation is working correctly  As part of our testing will check the presence of the inbound data, as files, within the proxy storage area. Now as the proxy storage area is also the location from which the Stroom application automatically aggregates then ingests the data stored by the proxy, we can either turn off the Proxy Aggregation task, or attempt to perform our tests noting that proxy aggregation occurs every 10 minutes by default. For simplicity, we will turn off the Proxy Aggregation task.\nWe can now perform out tests. Follow the steps in the Data Posting Tests section of the Testing Stroom Installation HOWTO\nForwarding Stroom Proxy Deployment In this deployment will install a Stroom Forwarding Proxy which is designed to aggregate data posted to it for managed forwarding to a central Stroom processing system. This scenario is assuming we are installing on the fully patch Centos 7.3 host, stroomfp0.strmdev00.org. Further it assumes we have installed, configured and tested the destination Stroom system we will be forwarding to.\nWe will first deploy the Stroom Proxy then configure it as a Forwarding Proxy then integrate a web service to run ‘in-front’ of Proxy.\nPrerequisite Software Installation for Forwarding Proxy Certain software packages are required for the Stroom Proxy to run.\nThe core software list is\n java-1.8.0-openjdk java-1.8.0-openjdk-devel policycoreutils-python unzip zip  Most of the required software are packages available via standard repositories and hence we can simply execute\nsudo yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel policycoreutils-python unzip zip  Note that additional software will be required for other integration components (e.g. Apache httpd/mod_jk). This is described in the Web Service Integration for Forwarding Proxy section of this document.\nForwarding Proxy Storage Since we are a proxy that stores data sent to it and forwards it each minute we have only one directory.\n /stroomdata/stroom-working-fp0/proxy - location for Stroom proxy to store inbound data files prior to forwarding  You will note that these HOWTOs use a consistent storage nomenclature for simplicity of documentations.\nCreation of Storage for Forwarding Proxy We create the processing user, as per\nsudo useradd --system stroomuser  then create the storage hierarchy with the commands\nsudo mkdir -p /stroomdata/stroom-working-fp0/proxy sudo chown -R stroomuser:stroomuser /stroomdata sudo chmod -R 750 /stroomdata  Stroom Forwarding Proxy Installation Pre-installation setup Before installing the Stroom Forwarding Proxy, we need establish various files and scripts within the Stroom Processing user’s home directory to support the Stroom services and their persistence. This is setup is described here. Although this setup HOWTO is orientated towards a complete Stroom Proxy and Application installation, it does provide all the processing user setup requirements for a Stroom Proxy as well.\nStroom Forwarding Proxy Installation Instructions for installation of the Stroom Proxy can be found here, noting you should follow the steps for configuring the proxy as a Forwarding proxy.\nWeb Service Integration for Forwarding Proxy One typically ‘fronts’ a Stroom Proxy with a secure web service such as Apache’s Httpd or NGINX. In our scenario, we will use SSL to secure the web service and further, we will use Apache’s Httpd.\nWe first need to create certificates for use by the web service. The SSL Certificate Generation HOWTO provides instructions for this. The created certificates can then be used when configuration the web service. NOTE also, that for a forwarding proxy we will need to establish Key and Trust stores as well. This is also documented in the SSL Certificate Generation HOWTO here\nThis HOWTO is designed to deploy Apache’s httpd web service as a front end (https) (to the user) and Apache’s mod_jk as the interface between Apache and the Stroom tomcat applications. The instructions to configure this can be found here. Please take note of where a Stroom Proxy configuration item is different to that of a Stroom Application processing node.\nOther Web service capability can be used, for example, NGINX (external link).\nTesting our Forwarding Proxy Installation To complete the installation process we will test that we can send data to the forwarding proxy and that it forwards the files it receives to the central Stroom processing system. As stated earlier, it is assumed we have installed, configured and tested the destination central Stroom processing system and thus we will have a test Feed already established - TEST-FEED-V1_0.\nSending Test Data For this test, we will send the contents of /etc/group to our test feed - TEST-FEED-V1_0. It doesn’t matter from which host we send the file from. The command to send file is\ncurl -k --data-binary @/etc/group \"https://stroomfp0.strmdev00.org/stroom/datafeed\" -H \"Feed:TEST-FEED-V1_0\" -H \"System:EXAMPLE_SYSTEM\" -H \"Environment:EXAMPLE_ENVIRONMENT\"  Before testing, it is recommended you set up to monitor the Stroom proxy logs on the central server as well as on the Forwarding Proxy server.\nFollow the steps in the Forwarding Proxy Data Posting Tests section of the Testing Stroom Installation HOWTO\nStandalone Stroom Proxy Deployment In this deployment will install a Stroom Standalone Proxy which is designed to accept and store data posted to it for manual forwarding to a central Stroom processing system. This scenario is assuming we are installing on the fully patch Centos 7.3 host, stroomsap0.strmdev00.org.\nWe will first deploy the Stroom Proxy then configure it as a Standalone Proxy then integrate a web service to run ‘in-front’ of Proxy.\nPrerequisite Software Installation for Forwarding Proxy Certain software packages are required for the Stroom Proxy to run.\nThe core software list is\n java-1.8.0-openjdk java-1.8.0-openjdk-devel policycoreutils-python unzip zip  Most of the required software are packages available via standard repositories and hence we can simply execute\nsudo yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel policycoreutils-python unzip zip  Note that additional software will be required for other integration components (e.g. Apache httpd/mod_jk). This is described in the Web Service Integration for Standalone Proxy section of this document.\nStandalone Proxy Storage Since we are a proxy that stores data sent to it we have only one directory.\n /stroomdata/stroom-working-sap0/proxy - location for Stroom proxy to store inbound data files  You will note that these HOWTOs use a consistent storage nomenclature for simplicity of documentations.\nCreation of Storage for Standalone Proxy We create the processing user, as per\nsudo useradd --system stroomuser  then create the storage hierarchy with the commands\nsudo mkdir -p /stroomdata/stroom-working-sap0/proxy sudo chown -R stroomuser:stroomuser /stroomdata sudo chmod -R 750 /stroomdata  Stroom Standalone Proxy Installation Pre-installation setup Before installing the Stroom Standalone Proxy, we need establish various files and scripts within the Stroom Processing user’s home directory to support the Stroom services and their persistence. This is setup is described here. Although this setup HOWTO is orientated towards a complete Stroom Proxy and Application installation, it does provide all the processing user setup requirements for a Stroom Proxy as well.\nStroom Standalone Proxy Installation Instructions for installation of the Stroom Proxy can be found here, noting you should follow the steps for configuring the proxy as a Store_NoDB proxy.\nWeb Service Integration for Standalone Proxy One typically ‘fronts’ a Stroom Proxy with a secure web service such as Apache’s Httpd or NGINX. In our scenario, we will use SSL to secure the web service and further, we will use Apache’s Httpd.\nWe first need to create certificates for use by the web service. The SSL Certificate Generation HOWTO provides instructions for this. The created certificates can then be used when configuration the web service. There is no need for Trust or Key stores.\nThis HOWTO is designed to deploy Apache’s httpd web service as a front end (https) (to the user) and Apache’s mod_jk as the interface between Apache and the Stroom tomcat applications. The instructions to configure this can be found here. Please take note of where a Stroom Proxy configuration item is different to that of a Stroom Application processing node.\nOther Web service capability can be used, for example, NGINX (external link).\nTesting our Standalone Proxy Installation To complete the installation process we will test that we can send data to the standalone proxy and it stores it.\nSending Test Data For this test, we will send the contents of /etc/group to our test feed - TEST-FEED-V1_0. It doesn’t matter from which host we send the file from. The command to send file is\ncurl -k --data-binary @/etc/group \"https://stroomsap0.strmdev00.org/stroom/datafeed\" -H \"Feed:TEST-FEED-V1_0\" -H \"System:EXAMPLE_SYSTEM\" -H \"Environment:EXAMPLE_ENVIRONMENT\"  Before testing, it is recommended you set up to monitor the Standalone Proxy logs.\nFollow the steps in the Standalone Proxy Data Posting Tests section of the Testing Stroom Installation HOWTO\nAddition of a Node to a Stroom Cluster Deployment In this deployment we will deploy both the Stroom Proxy and Stroom Application software to a new processing node we wish to add to our cluster. Once we have deploy and configured the Stroom software, we will then integrate a web service to run ‘in-front’ of our Stroom software, and then perform the initial configuration of to add this node via the user interface. The node we will add is stroomp02.strmdev00.org.\nGrant access to the database for this node Connect to the Stroom database as the administrative (root) user, via the command\nsudo mysql --user=root -p  and at the MariaDB [(none)]\u003e or mysql\u003e  prompt enter\ngrant all privileges on stroom.* to stroomuser@stroomp02.strmdev00.org identified by 'Stroompassword1@'; quit;  Prerequisite Software Installation Certain software packages are required for either the Stroom Proxy or Stroom Application to run.\nThe core software list is\n java-1.8.0-openjdk java-1.8.0-openjdk-devel policycoreutils-python unzip zip mariadb or mysql client  Most of the required software are packages available via standard repositories and hence we can simply execute\nsudo yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel policycoreutils-python unzip zip sudo yum -y install mariadb  In the above instance, the database client choice is MariaDB as it is directly supported by Centos 7. One could deploy the MySQL database software as the alternative. If you have chosen a different database for the already deployed Stroom Cluster then you should use that one. See earlier in this document on how to install the MySQL Community client.\nNote that additional software will be required for other integration components (e.g. Apache httpd/mod_jk). This is described in the Web Service Integration section of this document.\nStorage Scenario To maintain our Storage Scenario them, the scenario for this node is\n Node: stroomp02.strmdev00.org /stroomdata/stroom-data-p02 - location to store Stroom application data files (events, etc.) for this node /stroomdata/stroom-index-p02 - location to store Stroom application index files /stroomdata/stroom-working-p02 - location to store Stroom application working files (e.g. tmp, output, etc.) for this node /stroomdata/stroom-working-p02/proxy - location for Stroom proxy to store inbound data files  Creation of Storage Hierarchy So, we first create processing user on our new node as per\nsudo useradd --system stroomuser  then create the storage via\nsudo mkdir -p /stroomdata/stroom-data-p02 /stroomdata/stroom-index-p02 /stroomdata/stroom-working-p02 /stroomdata/stroom-working-p02/proxy sudo mkdir -p /stroomdata/stroom-data-p00 # So that this node can mount stroomp00's data directory sudo mkdir -p /stroomdata/stroom-data-p01 # So that this node can mount stroomp01's data directory sudo chown -R stroomuser:stroomuser /stroomdata sudo chmod -R 750 /stroomdata  As we need to share this new nodes permanent data directories to the existing nodes in the Cluster, we need to create mount point directories on our existing nodes in addition to deploying NFS.\nSo we execute on\n Node: stroomp00.strmdev00.org  sudo mkdir -p /stroomdata/stroom-data-p02 sudo chmod 750 /stroomdata/stroom-data-p02 sudo chown stroomuser:stroomuser /stroomdata/stroom-data-p02  and on\n Node: stroomp01.strmdev00.org  sudo mkdir -p /stroomdata/stroom-data-p02 sudo chmod 750 /stroomdata/stroom-data-p02 sudo chown stroomuser:stroomuser /stroomdata/stroom-data-p02  Deployment of NFS to share Stroom Storage We will use NFS to cross mount the permanent data directories. That is\n node stroomp00.strmdev00.org will mount  stroomp01.strmdev00.org:/stroomdata/stroom-data-p01 and, stroomp02.strmdev00.org:/stroomdata/stroom-data-p02 and,   node stroomp01.strmdev00.org will mount  stroomp00.strmdev00.org:/stroomdata/stroom-data-p00 and stroomp02.strmdev00.org:/stroomdata/stroom-data-p02   node stroomp02.strmdev00.org will mount  stroomp00.strmdev00.org:/stroomdata/stroom-data-p00 and stroomp01.strmdev00.org:/stroomdata/stroom-data-p01    The HOWTO guide to deploy and configure NFS for our Scenario is here.\nStroom Installation Pre-installation setup Before installing either the Stroom Proxy or Stroom Application, we need establish various files and scripts within the Stroom Processing user’s home directory to support the Stroom services and their persistence. This is setup is described here. Note you should remember to set the N bash variable when generating the Environment Variable files to 02.\nStroom Proxy Installation Instructions for installation of the Stroom Proxy can be found here. Note you will be deploying a Store proxy and during the setup execution ensure you enter the appropriate values for NODE (‘stroomp02’) and REPO_DIR ('/stroomdata/stroom-working-p02/proxy'). All other values will be the same.\nStroom Application Installation Instructions for installation of the Stroom application can be found here. When executing the setup script ensure you enter the appropriate values for TEMP_DIR ('/stroomdata/stroom-working-p02') and NODE (‘stroomp02’). All other values will be the same. Note also that you will not have to wait for the ‘first’ node to initialise the Stroom database as this would have already been done when you first deployed your Stroom Cluster.\nWeb Service Integration One typically ‘fronts’ either a Stroom Proxy or Stroom Application with a secure web service such as Apache’s Httpd or NGINX. In our scenario, we will use SSL to secure the web service and further, we will use Apache’s Httpd.\nAs we are a cluster, we use the same certificate as the other nodes. Thus we need to gain the certificate package from an existing node.\nSo, on stroomp00.strmdev00.org, we replicate the directory ~stroomuser/stroom-jks to our new node. That is, tar it up, copy the tar file to stroomp02 and untar it. We can make use of the other node’s mounted file system.\nsudo -i -u stroomuser cd ~stroomuser tar cf stroom-jks.tar stroom-jks mv stroom-jks.tar /stroomdata/stroom-data-p02  then on our new node (stroomp02.strmdev00.org) we extract the data.\nsudo -i -u stroomuser cd ~stroomuser tar xf /stroomdata/stroom-data-p02/stroom-jks.tar \u0026\u0026 rm -f /stroomdata/stroom-data-p02/stroom-jks.tar  Now ensure protection, ownership and SELinux context for these files by running\nchmod 700 ~stroomuser/stroom-jks/private ~stroomuser/stroom-jks chown -R stroomuser:stroomuser ~stroomuser/stroom-jks chcon -R --reference /etc/pki ~stroomuser/stroom-jks  This HOWTO is designed to deploy Apache’s httpd web service as a front end (https) (to the user) and Apache’s mod_jk as the interface between Apache and the Stroom tomcat applications. The instructions to configure this can be found here. You should pay particular attention to the section on the Apache Mod_JK configuration as you MUST regenerate the Mod_JK workers.properties file on the existing cluster nodes as well as generating it on our new node.\nOther Web service capability can be used, for example, NGINX (external link).\nNote that once you have integrated the web services for our new node, you will need to restart the Apache systemd process on the existing two nodes that that the new Mod_JK configuration has taken place.\nInstallation Validation We will now check that the installation and web services integration has worked. We do this with a simple firewall check and later perform complete integration tests.\nSanity firewall check To ensure you have the firewall correctly set up, the following command\nsudo firewall-cmd --reload sudo firewall-cmd --zone=public --list-all  should result in\npublic (active) target: default icmp-block-inversion: no interfaces: enp0s3 sources: services: dhcpv6-client http https nfs ssh ports: 8009/tcp 9080/tcp 8080/tcp 9009/tcp protocols: masquerade: no forward-ports: sourceports: icmp-blocks: rich rules:  Stroom Application Configuration - New Node We will need to configure this new node’s Volumes, set it’s Cluster URL and enable it’s Stream Processors. We do this by logging into the Stroom User Interface (UI) with an account with Administrator privileges. It is recommended you use a attributed user for this activity. Once you have logged in you can configure this new node.\nConfigure the Volumes for our Stroom deployment Before we can store data on this new Stroom node we need to configure it’s volumes we have allocated in our Storage hierarchy. The section on adding new volumes in the Volume Maintenance HOWTO shows how to do this.\nConfigure the Nodes for our Stroom deployment In a Stroom cluster, nodes are expected to communicate with each other on port 8080 over http. Our installation in a multi node environment ensures the firewall will allow this but we also need to configure the new node. This is achieved via the Stroom UI where we set a Cluster URL for our node. The section on Configuring a new node in the Node Configuration HOWTO demonstrates how do set the Cluster URL.\nData Stream Processing To enable Stroom to process data, it’s Data Processors need to be enabled. There are NOT enabled by default on installation. The following section in our Stroom Tasks HowTo shows how to do this.\nTesting our New Node Installation To complete the installation process we will test that our new node has successfully integrated into our cluster.\nFirst we need to ensure we have restarted the Apache Httpd service (httpd.service) on the original nodes so that the new workers.properties configuration files take effect.\nWe now test the node integration by running the tests we use to validate a Multi Node Stroom Cluster Deployment found here noting we should monitor all three nodes proxy and application log files. Basically we are looking to see that this new node participates in the load balancing for the stroomp.strmdev00.org cluster.\n","categories":"","description":"This HOWTO is provided to assist users in setting up a number of different Stroom environments based on Centos 7.3 infrastructure.\n","excerpt":"This HOWTO is provided to assist users in setting up a number of …","ref":"/stroom-docs/hugo-docsy/docs/howtos/install/installhowto/","tags":["installation"],"title":"Installation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/installation/","tags":"","title":"installation"},{"body":"Assumptions  the user has reasonable RHEL/Centos System administration skills installation is on a fully patched minimal Centos 7.3 instance. the Stroom stroom database has been created and resides on the host stroomdb0.strmdev00.org listening on port 3307. the Stroom stroom database user is stroomuser with a password of Stroompassword1@. the Stroom statistics database has been created and resides on the host stroomdb0.strmdev00.org listening on port 3308. the Stroom statistics database user is stroomuser with a password of Stroompassword2@. the application user stroomuser has been created the user is or has deployed the two node Stroom cluster described here the user has set up the Stroom processing user as described here the prerequisite software has been installed when a screen capture is documented, data entry is identified by the data surrounded by ‘\u003c’ ‘\u003e’ . This excludes enter/return presses.  Confirm Prerequisite Software Installation The following command will ensure the prerequisite software has been deployed\nsudo yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel policycoreutils-python unzip zip sudo yum -y install mariadb or sudo yum -y install mysql-community-client  Test Database connectivity We need to test access to the Stroom databases on stroomdb0.strmdev00.org. We do this using the client mysql utility. We note that we must enter the stroomuser user’s password set up in the creation of the database earlier (Stroompassword1@) when connecting to the stroom database and we must enter the stroomstats user’s password (Stroompassword2@) when connecting to the statistics database.\nWe first test we can connect to the stroom database and then set the default database to be stroom.\n[burn@stroomp00 ~]$ mysql --user=stroomuser --host=stroomdb0.strmdev00.org --port=3307 --password Enter password: \u003c__ Stroompassword1@ __\u003e Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 2 Server version: 5.5.52-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MariaDB [(none)]\u003e use stroom; Database changed MariaDB [stroom]\u003e exit Bye [burn@stroomp00 ~]$  In the case of a MySQL Community deployment you will see\n[burn@stroomp00 ~]$ mysql --user=stroomuser --host=stroomdb0.strmdev00.org --port=3307 --password Enter password: \u003c__ Stroompassword1@ __\u003e Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 9 Server version: 5.7.18 MySQL Community Server (GPL) Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql\u003e use stroom; Database changed mysql\u003e quit Bye [burn@stroomp00 ~]$  We next test connecting to the statistics database and verify we can set the default database to be statistics.\n[burn@stroomp00 ~]$ mysql --user=stroomstats --host=stroomdb0.strmdev00.org --port=3308 --password Enter password: \u003c__ Stroompassword2@ __\u003e Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 2 Server version: 5.5.52-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MariaDB [(none)]\u003e use statistics; Database changed MariaDB [stroom]\u003e exit Bye [burn@stroomp00 ~]$  In the case of a MySQL Community deployment you will see\n[burn@stroomp00 ~]$ mysql --user=stroomstats --host=stroomdb0.strmdev00.org --port=3308 --password Enter password: \u003c__ Stroompassword2@ __\u003e Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 9 Server version: 5.7.18 MySQL Community Server (GPL) Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql\u003e use statistics; Database changed mysql\u003e quit Bye [burn@stroomp00 ~]$  If there are any errors, correct them.\nGet the Software The following will gain the identified, in this case release 5.0-beta.18, Stroom Application software release from github, then deploy it. You should regularly monitor the site for newer releases.\nsudo -i -u stroomuser App=5.0-beta.18 wget https://github.com/gchq/stroom/releases/download/v${App}/stroom-app-distribution-${App}-bin.zip unzip stroom-app-distribution-${App}-bin.zip chmod 750 stroom-app  Configure the Software We install the application via\nstroom-app/bin/setup.sh  during which one is prompted for a number of configuration settings. Use the following\nTEMP_DIR should be set to '/stroomdata/stroom-working-p00' or '/stroomdata/stroom-working-p01' etc depending on the node we are installing on NODE to be the hostname (not FQDN) of your host (i.e. 'stroomp00' or 'stroomp01' in our multi node scenario) RACK can be ignored, just press return PORT_PREFIX should use the default, just press return JDBC_CLASSNAME should use the default, just press return JDBC_URL to 'jdbc:mysql://stroomdb0.strmdev00.org:3307/stroom?useUnicode=yes\u0026characterEncoding=UTF-8' DB_USERNAME should be our processing user, 'stroomuser' DB_PASSWORD should be the one we set when creating the stroom database, that is 'Stroompassword1@' JPA_DIALECT should use the default, just press return JAVA_OPTS can use the defaults, but ensure you have sufficient memory, either change or accept the default STROOM_STATISTICS_SQL_JDBC_CLASSNAME should use the default, just press return STROOM_STATISTICS_SQL_JDBC_URL to 'jdbc:mysql://stroomdb0.strmdev00.org:3308/statistics?useUnicode=yes\u0026characterEncoding=UTF-8' STROOM_STATISTICS_SQL_DB_USERNAME should be our processing user, 'stroomstats' STROOM_STATISTICS_SQL_DB_PASSWORD should be the one we set when creating the stroom database, that is 'Stroompassword2@' STATS_ENGINES should use the default, just press return CONTENT_PACK_IMPORT_ENABLED should use the default, just press return CREATE_DEFAULT_VOLUME_ON_START should use the default, just press return  At this point, the script will configure the application. There should be no errors, but review the output. If you made an error then just re-run the script.\nYou will note that TEMP_DIR is the same directory we used for our STROOM_TMP environment variable when we set up the processing user scripts. Note that if you are deploying a single node environment, where the database is also running on your Stroom node, then the JDBC_URL setting can be the default.\nStart the Application service Now we start the application. In the case of multi node Stroom deployment, we start the Stroom application on the first node in the cluster, then wait until it has initialised the database commenced it’s Lifecycle task. You will need to monitor the log file to see it’s completed initialisation.\nSo as the stroomuser start the application with the command\nstroom-app/bin/start.sh  Now monitor stroom-app/instance/logs for any errors. Initially you will see the log files localhost_access_log.YYYY-MM-DD.txt and catalina.out. Check them for errors and correct (or post a question). The log4j warnings in catalina.out can be ignored. Eventually the log file stroom-app/instance/logs/stroom.log will appear. Again check it for errors and then wait for the application to be initialised. That is, wait for the Lifecycle service thread to start. This is indicated by the message\nINFO [Thread-11] lifecycle.LifecycleServiceImpl (LifecycleServiceImpl.java:166) - Started Stroom Lifecycle service  The directory stroom-app/instance/logs/events will also appear with an empty file with the nomenclature events_YYYY-MM-DDThh:mm:ss.msecZ. This is the directory for storing Stroom’s application event logs. We will return to this directory and it’s content in a later HOWTO.\nIf you have a multi node configuration, then once the database has initialised, start the application service on all other nodes. Again with\nstroom-app/bin/start.sh  and then monitor the files in its stroom-app/instance/logs for any errors. Note that in multi node configurations, you will see server.UpdateClusterStateTaskHandler messages in the log file of the form\nWARN [Stroom P2 #9 - GenericServerTask] server.UpdateClusterStateTaskHandler (UpdateClusterStateTaskHandler.java:150) - discover() - unable to contact stroomp00 - No cluster call URL has been set for node: stroomp00  This is ok as we will establish the cluster URL’s later.\nMulti Node Firewall Provision In the case of a multi node Stroom deployment, you will need to open certain ports to allow Tomcat to communicate to all nodes participating in the cluster. Execute the following on all nodes. Note you will need to drop out of the stroomuser shell prior to execution.\nexit; # To drop out of the stroomuser shell sudo firewall-cmd --zone=public --add-port=8080/tcp --permanent sudo firewall-cmd --zone=public --add-port=9080/tcp --permanent sudo firewall-cmd --zone=public --add-port=8009/tcp --permanent sudo firewall-cmd --zone=public --add-port=9009/tcp --permanent sudo firewall-cmd --reload sudo firewall-cmd --zone=public --list-all  In a production environment you would improve the above firewall settings - to perhaps limit the communication to just the Stroom processing nodes.\n","categories":"","description":"This HOWTO describes the installation and initial configuration of the Stroom Application.\n","excerpt":"This HOWTO describes the installation and initial configuration of the …","ref":"/stroom-docs/hugo-docsy/docs/howtos/install/installapplicationhowto/","tags":["installation"],"title":"Installation of Stroom Application"},{"body":"Assumptions The following assumptions are used in this document.\n the user has reasonable RHEL/Centos System administration skills. installation is on a fully patched minimal Centos 7.3 instance. the Stroom database has been created and resides on the host stroomdb0.strmdev00.org listening on port 3307. the Stroom database user is stroomuser with a password of Stroompassword1@. the application user stroomuser has been created. the user is or has deployed the two node Stroom cluster described here. the user has set up the Stroom processing user as described here. the prerequisite software has been installed. when a screen capture is documented, data entry is identified by the data surrounded by ‘\u003c’ ‘\u003e’ . This excludes enter/return presses.  Confirm Prerequisite Software Installation The following command will ensure the prerequisite software has been deployed\nsudo yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel policycoreutils-python unzip zip sudo yum -y install mariadb or sudo yum -y install mysql-community-client  Note that we do NOT need the database client software for a Forwarding or Standalone proxy.\nGet the Software The following will gain the identified, in this case release 5.1-beta.10, Stroom Application software release from github, then deploy it. You should regularly monitor the site for newer releases.\nsudo -i -u stroomuser Prx=v5.1-beta.10 wget https://github.com/gchq/stroom-proxy/releases/download/${Prx}/stroom-proxy-distribution-${Prx}.zip unzip stroom-proxy-distribution-${Prx}.zip  Configure the Software There are three different types of Stroom Proxy\n Store  A store proxy accepts batches of events, as files. It will validate the batch with the database then store the batches as files in a configured directory.\n Store_NoDB  A store_nodb proxy accepts batches of events, as files. It has no connectivity to the database, so it assumes all batches are valid, so it stores the batches as files in a configured directory.\n Forwarding  A forwarding proxy accepts batches of events, as files. It has indirect connectivity to the database via the destination proxy, so it validates the batches then stores the batches as files in a configured directory until they are periodically forwarded to the configured destination Stroom proxy.\nWe will demonstrate the installation of each.\nStore Proxy Configuration In our Store Proxy description below, we will use the multi node deployment scenario. That is we are deploying the Store proxy on multiple Stroom nodes (stroomp00, stroomp01) and we have configured our storage as per the Storage Scenario which means the directories to install the inbound batches of data are /stroomdata/stroom-working-p00/proxy and /stroomdata/stroom-working-p01/proxy depending on the node.\nTo install a Store proxy, we run\nstroom-proxy/bin/setup.sh store  during which one is prompted for a number of configuration settings. Use the following\nNODE to be the hostname (not FQDN) of your host (i.e. 'stroomp00' or 'stroomp01' depending on the node we are installing on) PORT_PREFIX should use the default, just press return REPO_DIR should be set to '/stroomdata/stroom-working-p00/proxy' or '/stroomdata/stroom-working-p01/proxy' depending on the node we are installing on REPO_FORMAT can be left as the default, just press return JDBC_CLASSNAME should use the default, just press return JDBC_URL should be set to 'jdbc:mysql://stroomdb0.strmdev00.org:3307/stroom' DB_USERNAME should be our processing user, 'stroomuser' DB_PASSWORD should be the one we set when creating the stroom database, that is 'Stroompassword1@' JAVA_OPTS can use the defaults, but ensure you have sufficient memory, either change or accept the default  At this point, the script will configure the proxy. There should be no errors, but review the output. If you make a mistake in the above, just re-run the script.\nNOTE: The selection of the REPO_DIR above and the setting of the STROOM_TMP environment variable earlier ensure that not only inbound files are placed in the REPO_DIR location but the Stroom Application itself will access the same directory when it aggregates inbound data for ingest in it’s proxy aggregation threads.\nForwarding Proxy Configuration In our Forwarding Proxy description below, we will deploy on a host named stroomfp0 and it will store the files in /stroomdata/stroom-working-fp0/proxy. Remember, we are being consistent with our Storage hierarchy to make documentation and scripting simpler. Our destination host to periodically forward the files to will be stroomp.strmdev00.org (the CNAME for stroomp00.strmdev00.org).\nTo install a Forwarding proxy, we run\nstroom-proxy/bin/setup.sh forward  during which one is prompted for a number of configuration settings. Use the following\nNODE to be the hostname (not FQDN) of your host (i.e. 'stroomfp0' in our example) PORT_PREFIX should use the default, just press return REPO_DIR should be set to '/stroomdata/stroom-working-fp0/proxy' which we created earlier. REPO_FORMAT can be left as the default, just press return FORWARD_SERVER should be set to our stroom server. (i.e. 'stroomp.strmdev00.org' in our example) JAVA_OPTS can use the defaults, but ensure you have sufficient memory, either change or accept the default  At this point, the script will configure the proxy. There should be no errors, but review the output.\nStore No Database Proxy Configuration In our Store_NoDB Proxy description below, we will deploy on a host named stroomsap0 and it will store the files in /stroomdata/stroom-working-sap0/proxy. Remember, we are being consistent with our Storage hierarchy to make documentation and scripting simpler.\nTo install a Store_NoDB proxy, we run\nstroom-proxy/bin/setup.sh store_nodb  during which one is prompted for a number of configuration settings. Use the following\nNODE to be the hostname (not FQDN) of your host (i.e. 'stroomsap0' in our example) PORT_PREFIX should use the default, just press return REPO_DIR should be set to '/stroomdata/stroom-working-sap0/proxy' which we created earlier. REPO_FORMAT can be left as the default, just press return JAVA_OPTS can use the defaults, but ensure you have sufficient memory, either change or accept the default  At this point, the script will configure the proxy. There should be no errors, but review the output.\nApache/Mod_JK change For all proxy deployments, if we are using Apache’s mod_jk then we need to ensure the proxy’s AJP connector specifies a 64K packetSize. View the file stroom-proxy/instance/conf/server.xml to ensure the Connector element for the AJP protocol has a packetSize attribute of 65536. For example,\ngrep AJP stroom-proxy/instance/conf/server.xml  shows\n\u003cConnector port=\"9009\" protocol=\"AJP/1.3\" connectionTimeout=\"20000\" redirectPort=\"8443\" maxThreads=\"200\" packetSize=\"65536\" /\u003e  This check is required for earlier releases of the Stroom Proxy. Releases since v5.1-beta.4 have set the AJP packetSize.\nStart the Proxy Service We can now manually start our proxy service. Do so as the stroomuser with the command\nstroom-proxy/bin/start.sh  Now monitor the directory stroom-proxy/instance/logs for any errors. Initially you will see the log files localhost_access_log.YYYY-MM-DD.txt and catalina.out. Check them for errors and correct (or pose a question to this arena). The context path and unknown version warnings in catalina.out can be ignored.\nEventually (about 60 seconds) the log file stroom-proxy/instance/logs/stroom.log will appear. Again check it for errors. The proxy will have completely started when you see the messages\nINFO [localhost-startStop-1] spring.StroomBeanLifeCycleReloadableContextBeanProcessor (StroomBeanLifeCycleReloadableContextBeanProcessor.java:109) - ** proxyContext 0 START COMPLETE **  and\nINFO [localhost-startStop-1] spring.StroomBeanLifeCycleReloadableContextBeanProcessor (StroomBeanLifeCycleReloadableContextBeanProcessor.java:109) - ** webContext 0 START COMPLETE **  If you leave it for a while you will eventually see cyclic (10 minute cycle) messages of the form\nINFO [Repository Reader Thread 1] repo.ProxyRepositoryReader (ProxyRepositoryReader.java:170) - run() - Cron Match at YYYY-MM-DD ...  If a proxy takes too long to start, you should read the section on Entropy Issues.\nProxy Repository Format A Stroom Proxy stores inbound files in a hierarchical file system whose root is supplied during the proxy setup (REPO_DIR) and as files arrive they are given a repository id that is a one-up number starting at one (1). The files are stored in a specific repository format. The default template is ${pathId}/${id} and this pattern will produce the following output files under REPO_DIR for the given repository id\n   Repository Id FilePath     1 000.zip   100 100.zip   1000 001/001000.zip   10000 010/010000.zip   100000 100/100000.zip    Since version v5.1-beta.4, this template can be specified during proxy setup via the entry to the Stroom Proxy Repository Format prompt\n... @@REPO_FORMAT@@ : Stroom Proxy Repository Format [${pathId}/${id}] \u003e ...  The template uses replacement variables to form the file path. As indicated above, the default template is ${pathId}/${id} where ${pathId} is the automatically generated directory for a given repository id and ${id} is the repository id.\nOther replacement variables can be used to in the template including http header meta data parameters (e.g. ‘${feed}') and time based parameters (e.g. ‘${year}'). Replacement variables that cannot be resolved will be output as ‘_’. You must ensure that all templates include the ‘${id}’ replacement variable at the start of the file name, failure to do this will result in an invalid repository.\nAvailable time based parameters are based on the file’s time of processing and are zero filled (excluding ms).\n   Parameter Description     year four digit year   month two digit month   day two digit day   hour two digit hour   minute two digit minute   second two digit second   millis three digit milliseconds value   ms milliseconds since Epoch value    Proxy Repository Template Examples For each of the following templates applied to a Store NoDB Proxy, the resultant proxy directory tree is shown after three posts were sent to the test feed TEST-FEED-V1_0 and two posts to the test feed FEED-NOVALUE-V9_0\nExample A - The default - ${pathId}/${id} [stroomuser@stroomsap0 ~]$ find /stroomdata/stroom-working-sap0/proxy/ /stroomdata/stroom-working-sap0/proxy/ /stroomdata/stroom-working-sap0/proxy/001.zip /stroomdata/stroom-working-sap0/proxy/002.zip /stroomdata/stroom-working-sap0/proxy/003.zip /stroomdata/stroom-working-sap0/proxy/004.zip /stroomdata/stroom-working-sap0/proxy/005.zip [stroomuser@stroomsap0 ~]$  Example B - A feed orientated structure - ${feed}/${year}/${month}/${day}/${pathId}/${id} [stroomuser@stroomsap0 ~]$ find /stroomdata/stroom-working-sap0/proxy/ /stroomdata/stroom-working-sap0/proxy/ /stroomdata/stroom-working-sap0/proxy/TEST-FEED-V1_0 /stroomdata/stroom-working-sap0/proxy/TEST-FEED-V1_0/2017 /stroomdata/stroom-working-sap0/proxy/TEST-FEED-V1_0/2017/07 /stroomdata/stroom-working-sap0/proxy/TEST-FEED-V1_0/2017/07/23 /stroomdata/stroom-working-sap0/proxy/TEST-FEED-V1_0/2017/07/23/001.zip /stroomdata/stroom-working-sap0/proxy/TEST-FEED-V1_0/2017/07/23/002.zip /stroomdata/stroom-working-sap0/proxy/TEST-FEED-V1_0/2017/07/23/003.zip /stroomdata/stroom-working-sap0/proxy/FEED-NOVALUE-V9_0 /stroomdata/stroom-working-sap0/proxy/FEED-NOVALUE-V9_0/2017 /stroomdata/stroom-working-sap0/proxy/FEED-NOVALUE-V9_0/2017/07 /stroomdata/stroom-working-sap0/proxy/FEED-NOVALUE-V9_0/2017/07/23 /stroomdata/stroom-working-sap0/proxy/FEED-NOVALUE-V9_0/2017/07/23/004.zip /stroomdata/stroom-working-sap0/proxy/FEED-NOVALUE-V9_0/2017/07/23/005.zip [stroomuser@stroomsap0 ~]$  Example C - A date orientated structure - ${year}/${month}/${day}/${pathId}/${id} [stroomuser@stroomsap0 ~]$ find /stroomdata/stroom-working-sap0/proxy/ /stroomdata/stroom-working-sap0/proxy/ /stroomdata/stroom-working-sap0/proxy/2017 /stroomdata/stroom-working-sap0/proxy/2017/07 /stroomdata/stroom-working-sap0/proxy/2017/07/23 /stroomdata/stroom-working-sap0/proxy/2017/07/23/001.zip /stroomdata/stroom-working-sap0/proxy/2017/07/23/002.zip /stroomdata/stroom-working-sap0/proxy/2017/07/23/003.zip /stroomdata/stroom-working-sap0/proxy/2017/07/23/004.zip /stroomdata/stroom-working-sap0/proxy/2017/07/23/005.zip [stroomuser@stroomsap0 ~]$  Example D - A feed orientated structure, but with a bad parameter - ${feed}/${badparam}/${day}/${pathId}/${id} [stroomuser@stroomsap0 ~]$ find /stroomdata/stroom-working-sap0/proxy/ /stroomdata/stroom-working-sap0/proxy/ /stroomdata/stroom-working-sap0/proxy/TEST-FEED-V1_0 /stroomdata/stroom-working-sap0/proxy/TEST-FEED-V1_0/_ /stroomdata/stroom-working-sap0/proxy/TEST-FEED-V1_0/_/23 /stroomdata/stroom-working-sap0/proxy/TEST-FEED-V1_0/_/23/001.zip /stroomdata/stroom-working-sap0/proxy/TEST-FEED-V1_0/_/23/002.zip /stroomdata/stroom-working-sap0/proxy/TEST-FEED-V1_0/_/23/003.zip /stroomdata/stroom-working-sap0/proxy/FEED-NOVALUE-V9_0 /stroomdata/stroom-working-sap0/proxy/FEED-NOVALUE-V9_0/_ /stroomdata/stroom-working-sap0/proxy/FEED-NOVALUE-V9_0/_/23 /stroomdata/stroom-working-sap0/proxy/FEED-NOVALUE-V9_0/_/23/004.zip /stroomdata/stroom-working-sap0/proxy/FEED-NOVALUE-V9_0/_/23/005.zip [stroomuser@stroomsap0 ~]$  and one would also see a warning for each post in the proxy’s log file of the form\nWARN [ajp-apr-9009-exec-4] repo.StroomFileNameUtil (StroomFileNameUtil.java:133) - Unused variables found: [badparam]  ","categories":"","description":"This HOWTO describes the installation and configuration of the Stroom Proxy software.\n","excerpt":"This HOWTO describes the installation and configuration of the Stroom …","ref":"/stroom-docs/hugo-docsy/docs/howtos/install/installproxyhowto/","tags":["installation","proxy"],"title":"Installation of Stroom Proxy"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/nfs/","tags":"","title":"NFS"},{"body":"Assumptions The following assumptions are used in this document.\n the user has reasonable RHEL/Centos System administration skills installations are on Centos 7.3 minimal systems (fully patched) the user is or has deployed the example two node Stroom cluster storage hierarchy described here the configuration of this NFS is NOT secure. It is highly recommended to improve it’s security in a production environment. This could include improved firewall configuration to limit NFS access, NFS4 with Kerberos etc.  Installation of NFS software We install NFS on each node, via\nsudo yum -y install nfs-utils  and enable the relevant services, via\nsudo systemctl enable rpcbind sudo systemctl enable nfs-server sudo systemctl enable nfs-lock sudo systemctl enable nfs-idmap sudo systemctl start rpcbind sudo systemctl start nfs-server sudo systemctl start nfs-lock sudo systemctl start nfs-idmap  Configuration of NFS exports We now export the node’s /stroomdata directory (in case you want to share the working directories) by configuring /etc/exports. For simplicity sake, we will allow all nodes with the hostname nomenclature of stroomp*.strmdev00.org to mount the /stroomdata directory. This means the same configuration applies to all nodes.\n# Share Stroom data directory /stroomdata\tstroomp*.strmdev00.org(rw,sync,no_root_squash)  This can be achieved with the following on both nodes\nsudo su -c \"printf '# Share Stroom data directory\\n' \u003e\u003e /etc/exports\" sudo su -c \"printf '/stroomdata\\tstroomp*.strmdev00.org(rw,sync,no_root_squash)\\n' \u003e\u003e /etc/exports\"  On both nodes restart the NFS service to ensure the above export takes effect via\nsudo systemctl restart nfs-server  So that our nodes can offer their filesystems, we need to enable NFS access on the firewall. This is done via\nsudo firewall-cmd --zone=public --add-service=nfs --permanent sudo firewall-cmd --reload sudo firewall-cmd --zone=public --list-all  Test Mounting You should do test mounts on each node.\n Node: stroomp00.strmdev00.org  sudo mount -t nfs4 stroomp01.strmdev00.org:/stroomdata/stroom-data-p01 /stroomdata/stroom-data-p01   Node: stroomp01.strmdev00.org  sudo mount -t nfs4 stroomp00.strmdev00.org:/stroomdata/stroom-data-p00 /stroomdata/stroom-data-p00  If you are concerned you can’t see the mount with a df try a df --type=nfs4 -a or a sudo df. Irrespective, once the mounting works, make the mounts permanent by adding the following to each node’s /etc/fstab file.\n Node: stroomp00.strmdev00.org  stroomp01.strmdev00.org:/stroomdata/stroom-data-p01 /stroomdata/stroom-data-p01 nfs4 soft,bg  achieved with\nsudo su -c \"printf 'stroomp01.strmdev00.org:/stroomdata/stroom-data-p01 /stroomdata/stroom-data-p01 nfs4 soft,bg\\n' \u003e\u003e /etc/fstab\"   Node: stroomp01.strmdev00.org  stroomp00.strmdev00.org:/stroomdata/stroom-data-p00 /stroomdata/stroom-data-p00 nfs4 soft,bg  achieved with\nsudo su -c \"printf 'stroomp00.strmdev00.org:/stroomdata/stroom-data-p00 /stroomdata/stroom-data-p00 nfs4 soft,bg\\n' \u003e\u003e /etc/fstab\"  At this point reboot all processing nodes to ensure the directories mount automatically. You may need to give the nodes a minute to do this.\nAddition of another Node If one needs to add another node to the cluster, lets say, stroomp02.strmdev00.org, on which /stroomdata follows the same storage hierarchy as the existing nodes and all nodes have added mount points (directories) for this new node, you would take the following steps in order.\n  Node: stroomp02.strmdev00.org\n Install NFS software as above Configure the exports file as per    sudo su -c \"printf '# Share Stroom data directory\\n' \u003e\u003e /etc/exports\" sudo su -c \"printf '/stroomdata\\tstroomp*.strmdev00.org(rw,sync,no_root_squash)\\n' \u003e\u003e /etc/exports\"   Restart the NFS service and make the firewall enable NFS access as per  sudo systemctl restart nfs-server sudo firewall-cmd --zone=public --add-service=nfs --permanent sudo firewall-cmd --reload sudo firewall-cmd --zone=public --list-all   Test mount the existing node file systems  sudo mount -t nfs4 stroomp00.strmdev00.org:/stroomdata/stroom-data-p00 /stroomdata/stroom-data-p00 sudo mount -t nfs4 stroomp01.strmdev00.org:/stroomdata/stroom-data-p01 /stroomdata/stroom-data-p01   Once the test mounts work, we make them permanent by adding the following to the /etc/fstab file.  stroomp00.strmdev00.org:/home/stroomdata/stroom-data-p00 /home/stroomdata/stroom-data-p00 nfs4 soft,bg stroomp01.strmdev00.org:/home/stroomdata/stroom-data-p01 /home/stroomdata/stroom-data-p01 nfs4 soft,bg  achieved with\nsudo su -c \"printf 'stroomp00.strmdev00.org:/stroomdata/stroom-data-p00 /stroomdata/stroom-data-p00 nfs4 soft,bg\\n' \u003e\u003e /etc/fstab\" sudo su -c \"printf 'stroomp01.strmdev00.org:/stroomdata/stroom-data-p01 /stroomdata/stroom-data-p01 nfs4 soft,bg\\n' \u003e\u003e /etc/fstab\"    Node: stroomp00.strmdev00.org and stroomp01.strmdev00.org\n Test mount the new node’s filesystem as per    sudo mount -t nfs4 stroomp02.strmdev00.org:/stroomdata/stroom-data-p02 /stroomdata/stroom-data-p02   Once the test mount works, make the mount permanent by adding the following to the /etc/fstab file  stroomp02.strmdev00.org:/stroomdata/stroom-data-p02 /stroomdata/stroom-data-p02 nfs4 soft,bg  achieved with\nsudo su -c \"printf 'stroomp02.strmdev00.org:/stroomdata/stroom-data-p02 /stroomdata/stroom-data-p02 nfs4 soft,bg\\n' \u003e\u003e /etc/fstab\"  ","categories":"","description":"The following is a HOWTO to assist users in the installation and set up of NFS to support the sharing of directories in a two node Stroom cluster or add a new node to an existing cluster.\n","excerpt":"The following is a HOWTO to assist users in the installation and set …","ref":"/stroom-docs/hugo-docsy/docs/howtos/install/installnfshowto/","tags":["NFS","installation"],"title":"NFS Installation and Configuration"},{"body":"In a Stroom cluster, Nodes are expected to communicate with each other on port 8080 over http. To facilitate this, we need to set each node’s Cluster URL and the following demonstrates this process.\nAssumptions  an account with the Administrator Application Permission is currently logged in. we have a multi node Stroom cluster with two nodes, stroomp00 and stroomp01 appropriate firewall configurations have been made in the scenario of adding a new node to our multi node deployment, the node added will be stroomp02  Configure Two Nodes To configure the nodes, move to the Monitoring item of the Main Menu and select it to bring up the Monitoring sub-menu.\nStroom UI Monitoring sub-menu    then move down and select the Nodes sub-item to be presented with the Nodes configuration tab as seen below.\nStroom UI Node Management - management tab    To set stroomp00’s Cluster URL, move the it’s line in the display and select it. It will be highlighted.\nStroom UI Node Management - select first node    Then move the cursor to the Edit Node icon in the top left of the Nodes tab and select it. On selection the Edit Node configuration window will be displayed and into the Cluster URL: entry box, enter the first node’s URL of http://stroomp00.strmdev00.org:8080/stroom/clustercall.rpc\nStroom UI Node Management - set clustercall url for first node    then press the Stroom UI OkButton    at which we see the Cluster URL has been set for the first node as per\nStroom UI Node Management - set clustercall url on first node    We next select the second node\nStroom UI Node Management - select second node    then move the cursor to the Edit Node icon in the top left of the Nodes tab and select it. On selection the Edit Node configuration window will be displayed and into the Cluster URL: entry box, enter the second node’s URL of http://stroomp01.strmdev00.org:8080/stroom/clustercall.rpc\nStroom UI Node Management - set clustercall url for second node    then press the Stroom UI OkButton    At this we will see both nodes have the Cluster URLs set.\n Stroom UI Node Management - both nodes setup    .\nYou may need to press the Refresh icon found at top left of Nodes configuration tab, until both nodes show healthy pings.\n Stroom UI Node Management - both nodes ping    .\nIf you do not get ping results for each node, then they are not configured correctly. In that situation, review all log files and processes that you have performed.\nOnce you have set the Cluster URLs of each node you should also set the master assignment priority for each node to be different to all of the others. In the image above both have been assigned equal priority - 1. We will change stroomp00 to have a different priority - 3. You should note that the node with the highest priority gains the Master node status.\n Stroom UI Node Management - set node priorities    .\nConfigure New Node When one expands a Multi Node Stroom cluster deployment, after the installation of the Stroom Proxy and Application software and services on the new node, one has to configure the new node’s Cluster URL.\nTo configure the new node, move to the Monitoring item of the Main Menu and select it to bring up the Monitoring sub-menu.\nStroom UI Monitoring sub-menu    then move down and select the Nodes sub-item to be presented with the Nodes configuration tab as seen below.\nStroom UI New Node Management - management tab    To set stroomp02’s Cluster URL, move the it’s line in the display and select it. It will be highlighted.\nStroom UI Node Management - select new node    Then move the cursor to the Edit Node icon in the top left of the Nodes tab and select it. On selection the Edit Node configuration window will be displayed and into the Cluster URL: entry box, enter the first node’s URL of http://stroomp02.strmdev00.org:8080/stroom/clustercall.rpc\nStroom UI New Node Management - set clustercall url for new node    then press the Stroom UI OkButton    button at which we see the Cluster URL has been set for the first node as per\nStroom UI New Node Management - set clustercall url on new node    You need to press the Refresh icon found at top left of Nodes configuration tab, until the new node shows a healthy ping.\n Stroom UI New Node Management - all nodes ping    .\nIf you do not get a ping results for the new node, then it is not configured correctly. In that situation, review all log files and processes that you have performed.\nOnce you have set the Cluster URL you should also set the master assignment priority for each node to be different to all of the others. In the image above both stroomp01 and the new node, stroomp02, have been assigned equal priority - 1. We will change stroomp01 to have a different priority - 2. You should note that the node with the highest priority maintains the Master node status.\n Stroom UI New Node Management - set node priorities    .\n","categories":"","description":"Configuring Stroom cluster URLs\n","excerpt":"Configuring Stroom cluster URLs\n","ref":"/stroom-docs/hugo-docsy/docs/howtos/install/installnodeshowto/","tags":["cluster","installation"],"title":"Node Cluster URL Setup"},{"body":"Assumptions  the user has reasonable RHEL/Centos System administration skills installation is on a fully patched minimal Centos 7.3 instance. the application user stroomuser has been created the user is deploying for either the example two node Stroom cluster whose storage is described here a simple Forwarding or Standalone Proxy adding a node to an existing Stroom cluster  Set up the Stroom processing user’s environment To automate the running of a Stroom Proxy or Application service under out Stroom processing user, stroomuser, there are a number of configuration files and scripts we need to deploy.\nWe first become the stroomuser\nsudo -i -u stroomuser  Environment Variable files When either a Stroom Proxy or Application starts, it needs predefined environment variables. We set these up in the stroomuser home directory. We need two files for this. The first is for the Stroom processes themselves and the second is for the Stroom systemd service we deploy. The difference is that for the Stroom processes, we need to export the environment variables where as the Stroom systemd service file just needs to read them.\nThe JAVA_HOME and PATH variables are to support Java running the Tomcat instances. The STROOM_TMP variable is set to a working area for the Stroom Application to use. The application accesses this environment variable internally via the ${stroom_tmp} context variable. Note that we only need the STROOM_TMP variable for Stroom Application deployments, so one could remove it from the files for a Forwarding or Standalone proxy deployment.\nWith respect to the working area, we will make use of the Storage Scenario we have defined and hence use the directory /stroomdata/stroom-working-p_nn_ where nn is the hostname node number (i.e 00 for host stroomp00, 01 for host stroomp01, etc).\nSo, for the first node, 00, we run\nN=00 F=~/env.sh printf '# Environment variables for Stroom services\\n' \u003e ${F} printf 'export JAVA_HOME=/usr/lib/jvm/java-1.8.0\\n' \u003e\u003e ${F} printf 'export PATH=${JAVA_HOME}/bin:${PATH}\\n' \u003e\u003e ${F} printf 'export STROOM_TMP=/stroomdata/stroom-working-p%s\\n' ${N} \u003e\u003e ${F} chmod 640 ${F} F=~/env_service.sh printf '# Environment variables for Stroom services, executed out of systemd service\\n' \u003e ${F} printf 'JAVA_HOME=/usr/lib/jvm/java-1.8.0\\n' \u003e\u003e ${F} printf 'PATH=${JAVA_HOME}/bin:${PATH}\\n' \u003e\u003e ${F} printf 'STROOM_TMP=/stroomdata/stroom-working-p%s\\n' ${N} \u003e\u003e ${F} chmod 640 ${F}  then we can change the N variable on each successive node and run the above.\nAlternately, for a Stroom Forwarding or Standalone proxy, the following would be sufficient\nF=~/env.sh printf '# Environment variables for Stroom services\\n' \u003e ${F} printf 'export JAVA_HOME=/usr/lib/jvm/java-1.8.0\\n' \u003e\u003e ${F} printf 'export PATH=${JAVA_HOME}/bin:${PATH}\\n' \u003e\u003e ${F} chmod 640 ${F} F=~/env_service.sh printf '# Environment variables for Stroom services, executed out of systemd service\\n' \u003e ${F} printf 'JAVA_HOME=/usr/lib/jvm/java-1.8.0\\n' \u003e\u003e ${F} printf 'PATH=${JAVA_HOME}/bin:${PATH}\\n' \u003e\u003e ${F} chmod 640 ${F}  And we integrate the environment into our bash instantiation script as well as setting up useful bash functions. This is the same for all nodes. Note that the T and Tp aliases are always installed whether they are of use of not. IE a Standalone or Forwarding Stroom Proxy could make no use of the T shell alias.\nF=~/.bashrc printf '. ~/env.sh\\n\\n' \u003e\u003e ${F} printf '# Simple functions to support Stroom\\n' \u003e\u003e ${F} printf '# T - continually monitor (tail) the Stroom application log\\n' \u003e\u003e ${F} printf '# Tp - continually monitor (tail) the Stroom proxy log\\n' \u003e\u003e ${F} printf 'function T {\\n tail --follow=name ~/stroom-app/instance/logs/stroom.log\\n}\\n' \u003e\u003e ${F} printf 'function Tp {\\n tail --follow=name ~/stroom-proxy/instance/logs/stroom.log\\n}\\n' \u003e\u003e ${F}  And test it has set up correctly\n. ./.bashrc which java  which should return /usr/lib/jvm/java-1.8.0/bin/java\nEstablish Simple Start/Stop Scripts We create some simple start/stop scripts that start, or stop, all the available Stroom services. At this point, it’s just the Stroom application and proxy.\nif [ ! -d ~/bin ]; then mkdir ~/bin; fi F=~/bin/StartServices.sh printf '#!/bin/bash\\n' \u003e ${F} printf '# Start all Stroom services\\n' \u003e\u003e ${F} printf '# Set list of services\\n' \u003e\u003e ${F} printf 'Services=\"stroom-proxy stroom-app\"\\n' \u003e\u003e ${F} printf 'for service in ${Services}; do\\n' \u003e\u003e ${F} printf ' if [ -f ${service}/bin/start.sh ]; then\\n' \u003e\u003e ${F} printf ' bash ${service}/bin/start.sh\\n' \u003e\u003e ${F} printf ' fi\\n' \u003e\u003e ${F} printf 'done\\n' \u003e\u003e ${F} chmod 750 ${F} F=~/bin/StopServices.sh printf '#!/bin/bash\\n' \u003e ${F} printf '# Stop all Stroom services\\n' \u003e\u003e ${F} printf '# Set list of services\\n' \u003e\u003e ${F} printf 'Services=\"stroom-proxy stroom-app\"\\n' \u003e\u003e ${F} printf 'for service in ${Services}; do\\n' \u003e\u003e ${F} printf ' if [ -f ${service}/bin/stop.sh ]; then\\n' \u003e\u003e ${F} printf ' bash ${service}/bin/stop.sh\\n' \u003e\u003e ${F} printf ' fi\\n' \u003e\u003e ${F} printf 'done\\n' \u003e\u003e ${F} chmod 750 ${F}  Although one can modify the above for Stroom Forwarding or Standalone Proxy deployments, there is no issue if you use the same scripts.\nEstablish and Deploy Systemd services Processing or Proxy node For a standard Stroom Processing or Proxy nodes, we can use the following service script. (Noting this is done as root)\nsudo bash F=/etc/systemd/system/stroom-services.service printf '# Install in /etc/systemd/system\\n' \u003e ${F} printf '# Enable via systemctl enable stroom-services.service\\n\\n' \u003e\u003e ${F} printf '[Unit]\\n' \u003e\u003e ${F} printf '# Who we are\\n' \u003e\u003e ${F} printf 'Description=Stroom Service\\n' \u003e\u003e ${F} printf '# We want the network and httpd up before us\\n' \u003e\u003e ${F} printf 'Requires=network-online.target httpd.service\\n' \u003e\u003e ${F} printf 'After= httpd.service network-online.target\\n\\n' \u003e\u003e ${F} printf '[Service]\\n' \u003e\u003e ${F} printf '# Source our environment file so the Stroom service start/stop scripts work\\n' \u003e\u003e ${F} printf 'EnvironmentFile=/home/stroomuser/env_service.sh\\n' \u003e\u003e ${F} printf 'Type=oneshot\\n' \u003e\u003e ${F} printf 'ExecStart=/bin/su --login stroomuser /home/stroomuser/bin/StartServices.sh\\n' \u003e\u003e ${F} printf 'ExecStop=/bin/su --login stroomuser /home/stroomuser/bin/StopServices.sh\\n' \u003e\u003e ${F} printf 'RemainAfterExit=yes\\n\\n' \u003e\u003e ${F} printf '[Install]\\n' \u003e\u003e ${F} printf 'WantedBy=multi-user.target\\n' \u003e\u003e ${F} chmod 640 ${F}  Single Node Scenario with local database Should you only have a deployment where the database is on a processing node, use the following service script. The only difference is the Stroom dependency on the database. The database dependency below is for the MariaDB database. If you had installed the MySQL Community database, then replace mariadb.service with mysqld.service. (Noting this is done as root)\nsudo bash F=/etc/systemd/system/stroom-services.service printf '# Install in /etc/systemd/system\\n' \u003e ${F} printf '# Enable via systemctl enable stroom-services.service\\n\\n' \u003e\u003e ${F} printf '[Unit]\\n' \u003e\u003e ${F} printf '# Who we are\\n' \u003e\u003e ${F} printf 'Description=Stroom Service\\n' \u003e\u003e ${F} printf '# We want the network, httpd and Database up before us\\n' \u003e\u003e ${F} printf 'Requires=network-online.target httpd.service mariadb.service\\n' \u003e\u003e ${F} printf 'After=mariadb.service httpd.service network-online.target\\n\\n' \u003e\u003e ${F} printf '[Service]\\n' \u003e\u003e ${F} printf '# Source our environment file so the Stroom service start/stop scripts work\\n' \u003e\u003e ${F} printf 'EnvironmentFile=/home/stroomuser/env_service.sh\\n' \u003e\u003e ${F} printf 'Type=oneshot\\n' \u003e\u003e ${F} printf 'ExecStart=/bin/su --login stroomuser /home/stroomuser/bin/StartServices.sh\\n' \u003e\u003e ${F} printf 'ExecStop=/bin/su --login stroomuser /home/stroomuser/bin/StopServices.sh\\n' \u003e\u003e ${F} printf 'RemainAfterExit=yes\\n\\n' \u003e\u003e ${F} printf '[Install]\\n' \u003e\u003e ${F} printf 'WantedBy=multi-user.target\\n' \u003e\u003e ${F} chmod 640 ${F}  Enable the service Now we enable the Stroom service, but we DO NOT start it as we will manually start the Stroom services as part of the installation process.\nsystemctl enable stroom-services.service  ","categories":"","description":"This HOWTO demonstrates how to set up various files and scripts that the Stroom processing user requires.\n","excerpt":"This HOWTO demonstrates how to set up various files and scripts that …","ref":"/stroom-docs/hugo-docsy/docs/howtos/install/installprocessingusersetuphowto/","tags":["installation"],"title":"Processing User setup"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/docs/howtos/referencefeeds/","tags":["reference-data"],"title":"Reference Feeds"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/reference-data/","tags":"","title":"reference-data"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/docs/howtos/search/","tags":["search"],"title":"Search"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/search/","tags":"","title":"search"},{"body":"  Create an API Key for yourself, this will allow the API to authenticate as you and run the query with your privileges.\n  Create a Dashboard that extracts the data you are interested in. You should create a Query and Table.\n  Download the JSON for your Query. Press the download icon in the Query Pane to generate a file containing the JSON. Save the JSON to a file named query.json.\n  Use curl to send the query to Stroom.\nAPI_KEY='\u003cput your API Key here' URI=stroom.host/api/searchable/v2/search curl -s --request POST ${URL} -o response.out -H \"Authorization:Bearer ${API_KEY}\" -H \"Content-Type: application/json\" --data-binary @query.json    The query response should be in a file named response.out.\n  Optional step: reformat the response to csv using jq.\ncat response.out | jq '.results[0].rows[].values | @csv'    ","categories":"","description":"Stroom v6 introduced an API that allows a user to perform queries against Stroom resources such as indices and statistics. This is a guide to show how to perform a Stroom Query directly from bash using Stroom v7.\n","excerpt":"Stroom v6 introduced an API that allows a user to perform queries …","ref":"/stroom-docs/hugo-docsy/docs/howtos/search/searchfrombash/","tags":"","title":"Search API"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/solr/","tags":"","title":"solr"},{"body":"Assumptions  You are familiar with Lucene indexing within Stroom You have some data to index  Points to note  A Solr core is the home for exactly one Stroom index. Cores must initially be created in Solr. It is good practice to name your Solr core the same as your Stroom Index.  Method   Start a docker container for a single solr node.\ndocker run -d -p 8983:8983 --name my_solr solr    Check your Solr node. Point your browser at http://yourSolrHost:8983\n  Create a core in Solr using the CLI.\ndocker exec -it my_solr solr create_core -c test_index    Create a SolrIndex in Stroom New Solr Index      Update settings for your new Solr Index in Stroom then press “Test Connection”. If successful then press Save. Note the “Solr URL” field is a reference to the newly created Solr core. Solr Index Settings      Add some Index fields. e.g.EventTime, UserId\n  Retention is different in Solr, you must specify an expression that matches data that can be deleted. Solr Retention      Your Solr Index can now be used as per a Stroom Lucene Index. However, your Indexing pipeline must use a SolrIndexingFilter instead of an IndexingFilter.\n  ","categories":"","description":"This document will show how to use Solr from within Stroom. A single Solr node will be used running in a docker container.\n","excerpt":"This document will show how to use Solr from within Stroom. A single …","ref":"/stroom-docs/hugo-docsy/docs/howtos/search/solr/","tags":["search","solr"],"title":"Solr integration"},{"body":"Assumptions The following assumptions are used in this document.\n the user has reasonable RHEL/Centos System administration skills installations are on Centos 7.3 minimal systems (fully patched) either a Stroom Proxy or Stroom Application has already been deployed processing node names are ‘stroomp00.strmdev00.org’ and ‘stroomp01.strmdev00.org’ the first node, ‘stroomp00.strmdev00.org’ also has a CNAME ‘stroomp.strmdev00.org’ in the scenario of a Stroom Forwarding Proxy, the node name is ‘stroomfp0.strmdev00.org’ in the scenario of a Stroom Standalone Proxy, the node name is ‘stroomsap0.strmdev00.org’ stroom runs as user ‘stroomuser’ the use of self signed certificates is appropriate for test systems, but users should consider appropriate CA infrastructure in production environments in this document, when a screen capture is documented, data entry is identified by the data surrounded by ‘\u003c’ ‘\u003e’ . This excludes enter/return presses.  Create certificates The first step is to establish a self signed certificate for our Stroom service. If you have a certificate server, then certainly gain an appropriately signed certificate. For this HOWTO, we will stay with a self signed solution and hence no certificate authorities are involved. If you are deploying a cluster, then you will only have one certificate for all nodes. We achieve this by setting up an alias for the first node in the cluster and then use that alias for addressing the cluster. That is, we have set up a CNAME, stroomp.strmdev00.org for stroomp00.strmdev00.org. This means within the web service we deploy, the ServerName will be stroomp.strmdev00.org on each node. Since it’s one certificate we only need to set it up on one node then deploy the certificate key files to other nodes.\nAs the certificates will be stored in the stroomuser's home directory, we become the stroom user\nsudo -i -u stroomuser  Use host variable To make things simpler in the following bash extracts, we establish the bash variable H to be used in filename generation. The variable name is set to the name of the host (or cluster alias) your are deploying the certificates on. In our multi node HOWTO example we are using, we would use the host CNAME stroomp. Thus we execute\nexport H=stroomp  Note in our the Stroom Forwarding Proxy HOWTO we would use the name stroomfp0. In the case of our Standalone Proxy we would use stroomsap0.\nWe set up a directory to house our certificates via\ncd ~stroomuser rm -rf stroom-jks mkdir -p stroom-jks stroom-jks/public stroom-jks/private cd stroom-jks  Create a server key for Stroom service (enter a password when prompted for both initial and verification prompts)\nopenssl genrsa -des3 -out private/$H.key 2048  as per\nGenerating RSA private key, 2048 bit long modulus .................................................................+++ ...............................................+++ e is 65537 (0x10001) Enter pass phrase for private/stroomp.key: \u003c__ENTER_SERVER_KEY_PASSWORD__\u003e Verifying - Enter pass phrase for private/stroomp.key: \u003c__ENTER_SERVER_KEY_PASSWORD__\u003e  Create a signing request. The two important prompts are the password and Common Name. All the rest can use the defaults offered. The requested password is for the server key and you should use the host (or cluster alias) your are deploying the certificates on for the Common Name. In the output below we will assume a multi node cluster certificate is being generated, so will use stroomp.strmdev00.org.\nopenssl req -sha256 -new -key private/$H.key -out $H.csr  as per\nEnter pass phrase for private/stroomp.key: \u003c__ENTER_SERVER_KEY_PASSWORD__\u003e You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [XX]: State or Province Name (full name) []: Locality Name (eg, city) [Default City]: Organization Name (eg, company) [Default Company Ltd]: Organizational Unit Name (eg, section) []: Common Name (eg, your name or your server's hostname) []:\u003c__ stroomp.strmdev00.org __\u003e Email Address []: Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []: An optional company name []:  We now self sign the certificate (again enter the server key password)\nopenssl x509 -req -sha256 -days 720 -in $H.csr -signkey private/$H.key -out public/$H.crt  as per\nSignature ok subject=/C=XX/L=Default City/O=Default Company Ltd/CN=stroomp.strmdev00.org Getting Private key Enter pass phrase for private/stroomp.key: \u003c__ENTER_SERVER_KEY_PASSWORD__\u003e  and noting the subject will change depending on the host name used when generating the signing request.\nCreate insecure version of private key for Apache autoboot (you will again need to enter the server key password)\nopenssl rsa -in private/$H.key -out private/$H.key.insecure  as per\nEnter pass phrase for private/stroomp.key: \u003c__ENTER_SERVER_KEY_PASSWORD__\u003e writing RSA key  and then move the insecure keys as appropriate\nmv private/$H.key private/$H.key.secure chmod 600 private/$H.key.secure mv private/$H.key.insecure private/$H.key  We have now completed the creation of our certificates and keys.\nReplication of Keys Directory to other nodes If you are deploying a multi node Stroom cluster, then you would replicate the directory ~stroomuser/stroom-jks to each node in the cluster. That is, tar it up, copy the tar file to the other node(s) then untar it. We can make use of the other node’s mounted file system for this process. That is one could execute the commands on the first node, where we created the certificates\ncd ~stroomuser tar cf stroom-jks.tar stroom-jks mv stroom-jks.tar /stroomdata/stroom-data-p01  then on the another node, say stroomp01.strmdev00.org, as the stroomuser we extract the data.\nsudo -i -u stroomuser cd ~stroomuser tar xf /stroomdata/stroom-data-p01/stroom-jks.tar \u0026\u0026 rm -f /stroomdata/stroom-data-p01/stroom-jks.tar  Protection, Ownership and SELinux Context Now ensure protection, ownership and SELinux context for these key files on ALL nodes via\nchmod 700 ~stroomuser/stroom-jks/private ~stroomuser/stroom-jks chown -R stroomuser:stroomuser ~stroomuser/stroom-jks chcon -R --reference /etc/pki ~stroomuser/stroom-jks  Stroom Proxy to Proxy Key and Trust Stores In order for a Stroom Forwarding Proxy to communicate to a central Stroom proxy over https, the JVM running the forwarding proxy needs relevant keystores set up.\nOne would set up a Stroom’s forwarding proxy SSL certificate as per above, with the change that the hostname would be different. That is, in the initial setup, we would set the hostname variable H to be the hostname of the forwarding proxy. Lets say it is stroomfp0 thus we would set\nexport H=stroomfp0  and then proceed as above.\nNote that you also need the public key of the central Stroom server you will be connecting to. For the following, we will assume the central Stroom proxy is the stroomp.strmdev00.org server and it’s public key is stored in the file stroomp.crt. We will store this file on the forwarding proxy in ~stroomuser/stroom-jks/public/stroomp.crt.\nSo once you have created the forwarding proxy server’s SSL keys and have deployed the central proxy’s public key, we next need to convert the proxy server’s SSL keys into DER format. This is done by executing the following.\ncd ~stroomuser/stroom-jks export H=stroomfp0 export S=stroomp rm -f ${H}_k.jks ${S}_t.jks H_k=${H} S_k=${S} # Convert public key openssl x509 -in public/$H.crt -inform PERM -out public/$H.crt.der -outform DER  When you convert the local server’s private key, you will be prompted for the server key password.\n# Convert the local server's Private key openssl pkcs8 -topk8 -nocrypt -in private/$H.key.secure -inform PEM -out private/$H.key.der -outform DER  as per\nEnter pass phrase for private/stroomfp0.key.secure: \u003c__ENTER_SERVER_KEY_PASSWORD__\u003e  We now import these keys into our Key Store. As part of the Stroom Proxy release, an Import Keystore application has been provisioned. We identify where it’s found with the command\nfind ~stroomuser/*proxy -name 'stroom*util*.jar' -print | head -1  which should return /home/stroomuser/stroom-proxy/lib/stroom-proxy-util-v5.1-beta.10.jar or similar depending on the release version. To make execution simpler, we set this as a shell variable as per\nStroom_UTIL_JAR=`find ~/*proxy -name 'stroom*util*.jar' -print | head -1`  We now create the keystore and import the proxy’s server key\njava -cp ${Stroom_UTIL_JAR} stroom.util.cert.ImportKey keystore=${H}_k.jks keypass=$H alias=$H keyfile=private/$H.key.der certfile=public/$H.crt.der  as per\nOne certificate, no chain  We now import the destination server’s public key\nkeytool -import -noprompt -alias ${S} -file public/${S}.crt -keystore ${S}_k.jks -storepass ${S}  as per\nCertificate was added to keystore  We now add the key and trust store location and password arguments to our Stroom proxy environment files.\nPWD=`pwd` echo \"export JAVA_OPTS=\\\"-Djavax.net.ssl.trustStore=${PWD}/${S}_k.jks -Djavax.net.ssl.trustStorePassword=${S} -Djavax.net.ssl.keyStore=${PWD}/${H}_k.jks -Djavax.net.ssl.keyStorePassword=${H}\\\"\" \u003e\u003e ~/env.sh echo \"JAVA_OPTS=\\\"-Djavax.net.ssl.trustStore=${PWD}/${S}_k.jks -Djavax.net.ssl.trustStorePassword=${S} -Djavax.net.ssl.keyStore=${PWD}/${H}_k.jks -Djavax.net.ssl.keyStorePassword=${H}\\\"\" \u003e\u003e ~/env_service.sh  At this point you should restart the proxy service. Using the commands\ncd ~stroomuser source ./env.sh stroom-proxy/bin/stop.sh stroom-proxy/bin/start.sh  then check the logs to ensure it started correctly.\n","categories":"","description":"A HOWTO to assist users in setting up various SSL Certificates to support a Web interface to Stroom.\n","excerpt":"A HOWTO to assist users in setting up various SSL Certificates to …","ref":"/stroom-docs/hugo-docsy/docs/howtos/install/installcertificateshowto/","tags":["certificates","installation"],"title":"SSL Certificate Generation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/testing/","tags":"","title":"testing"},{"body":"Assumptions  Stroom Single or Multi Node Cluster Testing the Multi Node Stroom Cluster (Proxy and Application) has been deployed a Test Feed, TEST-FEED-V1_0 has been added Proxy aggregation has been turned off on all Stroom Store Proxies the Stroom Proxy Repository Format (REPO_FORMAT) chosen was the default - ${pathId}/${id Stroom Forwarding Proxy Testing the Multi Node Stroom Cluster (Proxy and Application) has been deployed the Stroom Forwarding Proxy has been deployed a Test Feed, TEST-FEED-V1_0 has been added the Stroom Proxy Repository Format (REPO_FORMAT) chosen was the default - ${pathId}/${id Stroom Standalone Proxy Testing the Stroom Standalone Proxy has been deployed the Stroom Proxy Repository Format (REPO_FORMAT) chosen was the default - ${pathId}/${id  Stroom Single or Multi Node Cluster Testing Data Post Tests Simple Post tests These tests are to ensure the Stroom Store proxy and it’s connection to the database is working along with the Apache mod_jk loadbalancer. We will send a file to the load balanced stroomp.strmdev00.org node (really stroomp00.strmdev00.org) and each time we send the file, it’s receipt should be managed by alternate proxy nodes. As a number of elements can effect load balancing, it is not always guaranteed to alternate every time but for the most part it will.\nPerform the following\n Log onto the Stroom database node (stroomdb0.strmdev00.org) as any user. Log onto both Stroom nodes and become the stroomuser and monitor each node’s Stroom proxy service using the Tp bash macro. That is, on each node, run  sudo -i -u stroomuser Tp  You will note events of the form from stroomp00.strmdev00.org:\n... 2017-01-14T06:22:26.672Z INFO [ProxyProperties refresh thread 0] datafeed.ProxyHandlerFactory$1 (ProxyHandlerFactory.java:96) - refreshThread() - Started 2017-01-14T06:30:00.993Z INFO [Repository Reader Thread 1] handler.ProxyRepositoryReader (ProxyRepositoryReader.java:143) - run() - Cron Match at 2017-01-14T06:30:00.993Z 2017-01-14T06:40:00.245Z INFO [Repository Reader Thread 1] handler.ProxyRepositoryReader (ProxyRepositoryReader.java:143) - run() - Cron Match at 2017-01-14T06:40:00.245Z  and from stroomp01.strmdev00.org:\n... 2017-01-14T06:22:26.828Z INFO [ProxyProperties refresh thread 0] datafeed.ProxyHandlerFactory$1 (ProxyHandlerFactory.java:96) - refreshThread() - Started 2017-01-14T06:30:00.066Z INFO [Repository Reader Thread 1] handler.ProxyRepositoryReader (ProxyRepositoryReader.java:143) - run() - Cron Match at 2017-01-14T06:30:00.066Z 2017-01-14T06:40:00.318Z INFO [Repository Reader Thread 1] handler.ProxyRepositoryReader (ProxyRepositoryReader.java:143) - run() - Cron Match at 2017-01-14T06:40:00.318Z   On the Stroom database node, execute the command  curl -k --data-binary @/etc/group \"https://stroomp.strmdev00.org/stroom/datafeed\" -H \"Feed:TEST-FEED-V1_0\" -H \"System:EXAMPLE_SYSTEM\" -H \"Environment:EXAMPLE_ENVIRONMENT\"  If you are monitoring the proxy log of stroomp00.strmdev00.org you would see two new logs indicating the successful arrival of the file\n2017-01-14T06:46:06.411Z INFO [ajp-apr-9009-exec-1] handler.LogRequestHandler (LogRequestHandler.java:37) - log() - guid=54dc0da2-f35c-4dc2-8a98-448415ffc76b,feed=TEST-FEED-V1_0,system=EXAMPLE_SYSTEM,environment=EXAMPLE_ENVIRONMENT,remotehost=192.168.2.144,remoteaddress=192.168.2.144 2017-01-14T06:46:06.449Z INFO [ajp-apr-9009-exec-1] datafeed.DataFeedRequestHandler$1 (DataFeedRequestHandler.java:104) - \"doPost() - Took 571 ms to process (concurrentRequestCount=1) 200\",\"Environment=EXAMPLE_ENVIRONMENT\",\"Feed=TEST-FEED-V1_0\",\"GUID=54dc0da2-f35c-4dc2-8a98-448415ffc76b\",\"ReceivedTime=2017-01-14T06:46:05.883Z\",\"RemoteAddress=192.168.2.144\",\"RemoteHost=192.168.2.144\",\"System=EXAMPLE_SYSTEM\",\"accept=*/*\",\"content-length=527\",\"content-type=application/x-www-form-urlencoded\",\"host=stroomp.strmdev00.org\",\"user-agent=curl/7.29.0\"   On the Stroom database node, again execute the command  curl -k --data-binary @/etc/group \"https://stroomp.strmdev00.org/stroom/datafeed\" -H \"Feed:TEST-FEED-V1_0\" -H \"System:EXAMPLE_SYSTEM\" -H \"Environment:EXAMPLE_ENVIRONMENT\"  If you are monitoring the proxy log of stroomp01.strmdev00.org you should see a new log. As foreshadowed, we didn’t as the time delay resulted in the first node getting the file. That is stroomp00.strmdev00.org log file gained the two entries\n2017-01-14T06:47:26.642Z INFO [ajp-apr-9009-exec-2] handler.LogRequestHandler (LogRequestHandler.java:37) - log() - guid=941d2904-734f-4764-9ccf-4124b94a56f6,feed=TEST-FEED-V1_0,system=EXAMPLE_SYSTEM,environment=EXAMPLE_ENVIRONMENT,remotehost=192.168.2.144,remoteaddress=192.168.2.144 2017-01-14T06:47:26.645Z INFO [ajp-apr-9009-exec-2] datafeed.DataFeedRequestHandler$1 (DataFeedRequestHandler.java:104) - \"doPost() - Took 174 ms to process (concurrentRequestCount=1) 200\",\"Environment=EXAMPLE_ENVIRONMENT\",\"Feed=TEST-FEED-V1_0\",\"GUID=941d2904-734f-4764-9ccf-4124b94a56f6\",\"ReceivedTime=2017-01-14T06:47:26.470Z\",\"RemoteAddress=192.168.2.144\",\"RemoteHost=192.168.2.144\",\"System=EXAMPLE_SYSTEM\",\"accept=*/*\",\"content-length=527\",\"content-type=application/x-www-form-urlencoded\",\"host=stroomp.strmdev00.org\",\"user-agent=curl/7.29.0\"   Again on the database node, execute the command and this time we see that node stroomp01.strmdev00.org received the file as per  2017-01-14T06:47:30.782Z INFO [ajp-apr-9009-exec-1] handler.LogRequestHandler (LogRequestHandler.java:37) - log() - guid=2cef6e23-b0e6-4d75-8374-cca7caf66e15,feed=TEST-FEED-V1_0,system=EXAMPLE_SYSTEM,environment=EXAMPLE_ENVIRONMENT,remotehost=192.168.2.144,remoteaddress=192.168.2.144 2017-01-14T06:47:30.816Z INFO [ajp-apr-9009-exec-1] datafeed.DataFeedRequestHandler$1 (DataFeedRequestHandler.java:104) - \"doPost() - Took 593 ms to process (concurrentRequestCount=1) 200\",\"Environment=EXAMPLE_ENVIRONMENT\",\"Feed=TEST-FEED-V1_0\",\"GUID=2cef6e23-b0e6-4d75-8374-cca7caf66e15\",\"ReceivedTime=2017-01-14T06:47:30.238Z\",\"RemoteAddress=192.168.2.144\",\"RemoteHost=192.168.2.144\",\"System=EXAMPLE_SYSTEM\",\"accept=*/*\",\"content-length=527\",\"content-type=application/x-www-form-urlencoded\",\"host=stroomp.strmdev00.org\",\"user-agent=curl/7.29.0\"   Running the curl post command in quick succession shows the loadbalancer working … four executions result in seeing our pair of logs appearing on alternate proxies.  stroomp00:\n2017-01-14T06:52:09.815Z INFO [ajp-apr-9009-exec-3] handler.LogRequestHandler (LogRequestHandler.java:37) - log() - guid=bf0bc38c-3533-4d5c-9ddf-5d30c0302787,feed=TEST-FEED-V1_0,system=EXAMPLE_SYSTEM,environment=EXAMPLE_ENVIRONMENT,remotehost=192.168.2.144,remoteaddress=192.168.2.144 2017-01-14T06:52:09.817Z INFO [ajp-apr-9009-exec-3] datafeed.DataFeedRequestHandler$1 (DataFeedRequestHandler.java:104) - \"doPost() - Took 262 ms to process (concurrentRequestCount=1) 200\",\"Environment=EXAMPLE_ENVIRONMENT\",\"Feed=TEST-FEED-V1_0\",\"GUID=bf0bc38c-3533-4d5c-9ddf-5d30c0302787\",\"ReceivedTime=2017-01-14T06:52:09.555Z\",\"RemoteAddress=192.168.2.144\",\"RemoteHost=192.168.2.144\",\"System=EXAMPLE_SYSTEM\",\"accept=*/*\",\"content-length=527\",\"content-type=application/x-www-form-urlencoded\",\"host=stroomp.strmdev00.org\",\"user-agent=curl/7.29.0\"  stroomp01:\n2017-01-14T06:52:11.139Z INFO [ajp-apr-9009-exec-2] handler.LogRequestHandler (LogRequestHandler.java:37) - log() - guid=1088fdd8-6869-489f-8baf-948891363734,feed=TEST-FEED-V1_0,system=EXAMPLE_SYSTEM,environment=EXAMPLE_ENVIRONMENT,remotehost=192.168.2.144,remoteaddress=192.168.2.144 2017-01-14T06:52:11.150Z INFO [ajp-apr-9009-exec-2] datafeed.DataFeedRequestHandler$1 (DataFeedRequestHandler.java:104) - \"doPost() - Took 289 ms to process (concurrentRequestCount=1) 200\",\"Environment=EXAMPLE_ENVIRONMENT\",\"Feed=TEST-FEED-V1_0\",\"GUID=1088fdd8-6869-489f-8baf-948891363734\",\"ReceivedTime=2017-01-14T06:52:10.861Z\",\"RemoteAddress=192.168.2.144\",\"RemoteHost=192.168.2.144\",\"System=EXAMPLE_SYSTEM\",\"accept=*/*\",\"content-length=527\",\"content-type=application/x-www-form-urlencoded\",\"host=stroomp.strmdev00.org\",\"user-agent=curl/7.29.0\"  stroomp00:\n2017-01-14T06:52:12.284Z INFO [ajp-apr-9009-exec-4] handler.LogRequestHandler (LogRequestHandler.java:37) - log() - guid=def94a4a-cf78-4c4d-9261-343663f7f79a,feed=TEST-FEED-V1_0,system=EXAMPLE_SYSTEM,environment=EXAMPLE_ENVIRONMENT,remotehost=192.168.2.144,remoteaddress=192.168.2.144 2017-01-14T06:52:12.289Z INFO [ajp-apr-9009-exec-4] datafeed.DataFeedRequestHandler$1 (DataFeedRequestHandler.java:104) - \"doPost() - Took 5.0 ms to process (concurrentRequestCount=1) 200\",\"Environment=EXAMPLE_ENVIRONMENT\",\"Feed=TEST-FEED-V1_0\",\"GUID=def94a4a-cf78-4c4d-9261-343663f7f79a\",\"ReceivedTime=2017-01-14T06:52:12.284Z\",\"RemoteAddress=192.168.2.144\",\"RemoteHost=192.168.2.144\",\"System=EXAMPLE_SYSTEM\",\"accept=*/*\",\"content-length=527\",\"content-type=application/x-www-form-urlencoded\",\"host=stroomp.strmdev00.org\",\"user-agent=curl/7.29.0\"  stroomp01:\n2017-01-14T06:52:13.374Z INFO [ajp-apr-9009-exec-3] handler.LogRequestHandler (LogRequestHandler.java:37) - log() - guid=55dda4c9-2c76-43c8-9b48-dcdb3a1f459b,feed=TEST-FEED-V1_0,system=EXAMPLE_SYSTEM,environment=EXAMPLE_ENVIRONMENT,remotehost=192.168.2.144,remoteaddress=192.168.2.144 2017-01-14T06:52:13.378Z INFO [ajp-apr-9009-exec-3] datafeed.DataFeedRequestHandler$1 (DataFeedRequestHandler.java:104) - \"doPost() - Took 3.0 ms to process (concurrentRequestCount=1) 200\",\"Environment=EXAMPLE_ENVIRONMENT\",\"Feed=TEST-FEED-V1_0\",\"GUID=55dda4c9-2c76-43c8-9b48-dcdb3a1f459b\",\"ReceivedTime=2017-01-14T06:52:13.374Z\",\"RemoteAddress=192.168.2.144\",\"RemoteHost=192.168.2.144\",\"System=EXAMPLE_SYSTEM\",\"accept=*/*\",\"content-length=527\",\"content-type=application/x-www-form-urlencoded\",\"host=stroomp.strmdev00.org\",\"user-agent=curl/7.29.0\"  At this point we will see what the proxies have received.\n On each node run the command  ls -l /stroomdata/stroom-working*/proxy  On stroomp00 we see\n[stroomuser@stroomp00 ~]$ ls -l /stroomdata/stroom-working*/proxy total 16 -rw-rw-r--. 1 stroomuser stroomuser 785 Jan 14 17:46 001.zip -rw-rw-r--. 1 stroomuser stroomuser 783 Jan 14 17:47 002.zip -rw-rw-r--. 1 stroomuser stroomuser 784 Jan 14 17:52 003.zip -rw-rw-r--. 1 stroomuser stroomuser 783 Jan 14 17:52 004.zip [stroomuser@stroomp00 ~]$  and on stroomp01 we see\n[stroomuser@stroomp01 ~]$ ls -l /stroomdata/stroom-working*/proxy total 12 -rw-rw-r--. 1 stroomuser stroomuser 785 Jan 14 17:47 001.zip -rw-rw-r--. 1 stroomuser stroomuser 783 Jan 14 17:52 002.zip -rw-rw-r--. 1 stroomuser stroomuser 784 Jan 14 17:52 003.zip [stroomuser@stroomp01 ~]$  which corresponds to the seven posts of data and the associated events in the proxy logs. To see the contents of one of these files we execute on either node, the command\nunzip -c /stroomdata/stroom-working*/proxy/001.zip  to see\nArchive: /stroomdata/stroom-working-p00/proxy/001.zip inflating: 001.dat root:x:0: bin:x:1: daemon:x:2: sys:x:3: adm:x:4: tty:x:5: disk:x:6: lp:x:7: mem:x:8: kmem:x:9: wheel:x:10:burn cdrom:x:11: mail:x:12:postfix man:x:15: dialout:x:18: floppy:x:19: games:x:20: tape:x:30: video:x:39: ftp:x:50: lock:x:54: audio:x:63: nobody:x:99: users:x:100: utmp:x:22: utempter:x:35: input:x:999: systemd-journal:x:190: systemd-bus-proxy:x:998: systemd-network:x:192: dbus:x:81: polkitd:x:997: ssh_keys:x:996: dip:x:40: tss:x:59: sshd:x:74: postdrop:x:90: postfix:x:89: chrony:x:995: burn:x:1000:burn mysql:x:27: inflating: 001.meta content-type:application/x-www-form-urlencoded Environment:EXAMPLE_ENVIRONMENT Feed:TEST-FEED-V1_0 GUID:54dc0da2-f35c-4dc2-8a98-448415ffc76b host:stroomp.strmdev00.org ReceivedTime:2017-01-14T06:46:05.883Z RemoteAddress:192.168.2.144 RemoteHost:192.168.2.144 StreamSize:527 System:EXAMPLE_SYSTEM user-agent:curl/7.29.0 [stroomuser@stroomp00 ~]$  Checking the /etc/group file on stroomdb0.strmdev00.org confirms the above contents. For the present, ignore the metadata file present in the zip archive.\nIf you execute the same command on the other files, all that changes is the value of the ReceivedTime: attribute in the .meta file.\nFor those curious about the file size differences, this is a function of the compression process within the proxy. Using stroomp01’s files and extracting them manually and renaming them results in the six files\n[stroomuser@stroomp01 xx]$ ls -l total 24 -rw-rw-r--. 1 stroomuser stroomuser 527 Jan 14 17:47 A_001.dat -rw-rw-r--. 1 stroomuser stroomuser 321 Jan 14 17:47 A_001.meta -rw-rw-r--. 1 stroomuser stroomuser 527 Jan 14 17:52 B_001.dat -rw-rw-r--. 1 stroomuser stroomuser 321 Jan 14 17:52 B_001.meta -rw-rw-r--. 1 stroomuser stroomuser 527 Jan 14 17:52 C_001.dat -rw-rw-r--. 1 stroomuser stroomuser 321 Jan 14 17:52 C_001.meta [stroomuser@stroomp01 xx]$ cmp A_001.dat B_001.dat [stroomuser@stroomp01 xx]$ cmp B_001.dat C_001.dat [stroomuser@stroomp01 xx]$  We have effectively tested the receipt of our data and the load balancing of the Apache mod_jk installation.\nSimple Direct Post tests In this test we will use the direct feed interface of the Stroom application, rather than sending data via the proxy. One would normally use this interface for time sensitive data which shouldn’t aggregate in a proxy waiting for the Stroom application to collect it. In this situation we use the command\ncurl -k --data-binary @/etc/group \"https://stroomp.strmdev00.org/stroom/datafeed/direct\" -H \"Feed:TEST-FEED-V1_0\" -H \"System:EXAMPLE_SYSTEM\" -H \"Environment:EXAMPLE_ENVIRONMENT\"  To prepare for this test, we monitor the Stroom application log using the T bash alias on each node. So on each node run the command\nsudo -i -u stroomuser T  On each node you should see LifecyleTask events, for example,\n2017-01-14T07:42:08.281Z INFO [Stroom P2 #7 - LifecycleTask] spring.StroomBeanMethodExecutable (StroomBeanMethodExecutable.java:47) - Executing nodeStatusExecutor.exec 2017-01-14T07:42:18.284Z INFO [Stroom P2 #2 - LifecycleTask] spring.StroomBeanMethodExecutable (StroomBeanMethodExecutable.java:47) - Executing SQLStatisticEventStore.evict 2017-01-14T07:42:18.284Z INFO [Stroom P2 #10 - LifecycleTask] spring.StroomBeanMethodExecutable (StroomBeanMethodExecutable.java:47) - Executing activeQueriesManager.evictExpiredElements 2017-01-14T07:42:18.285Z INFO [Stroom P2 #7 - LifecycleTask] spring.StroomBeanMethodExecutable (StroomBeanMethodExecutable.java:47) - Executing distributedTaskFetcher.execute  To perform the test, on the database node, run the posting command a number of times in rapid succession. This will result in server.DataFeedServiceImpl events in both log files. The Stroom application log is quite busy, you may have to look for these logs.\nIn the following we needed to execute the posting command three times before seeing the data arrive on both nodes. Looking at the arrival times, the file turned up on the second node twice before appearing on the first node. strooomp00:\n2017-01-14T07:43:09.394Z INFO [ajp-apr-8009-exec-6] server.DataFeedServiceImpl (DataFeedServiceImpl.java:133) - handleRequest response 200 - 0 - OK  and on stroomp01:\n2017-01-14T07:43:05.614Z INFO [ajp-apr-8009-exec-1] server.DataFeedServiceImpl (DataFeedServiceImpl.java:133) - handleRequest response 200 - 0 - OK 2017-01-14T07:43:06.821Z INFO [ajp-apr-8009-exec-2] server.DataFeedServiceImpl (DataFeedServiceImpl.java:133) - handleRequest response 200 - 0 - OK  To confirm this data arrived, we need to view the Data pane of our TEST-FEED-V1_0 tab. To do this, log onto the Stroom UI then move the cursor to the TEST-FEED-V1_0 entry in the Explorer tab and select the item with a left click\nStroom UI Test Feed - Open Feed    and double click on the entry to see our TEST-FEED-V1_0 tab.\n Stroom UI Test Feed - Opened Feed    and it is noted that we are viewing the Feed’s attributes as we can see the Setting hyper-link highlighted. As we want to see the Data we have received for this feed, move the cursor to the Data hyper-link and select it to see Stroom UI Test Feed - Opened Feed view Data    .\nThese three entries correspond to the three posts we performed.\nWe have successfully tested direct posting to a Stroom feed and that the Apache mod_jk loadbalancer also works for this posting method.\nTest Proxy Aggregation is Working To test that the Proxy Aggregation is working, we need to enable on each node.\nBy enabling the Proxy Aggregation process, both nodes immediately performed the task as indicated by each node’s Stroom application logs as per stroomp00:\n2017-01-14T07:58:58.752Z INFO [Stroom P2 #3 - LifecycleTask] server.ProxyAggregationExecutor (ProxyAggregationExecutor.java:138) - exec() - started 2017-01-14T07:58:58.937Z INFO [Stroom P2 #2 - GenericServerTask] server.ProxyAggregationExecutor$2 (ProxyAggregationExecutor.java:203) - processFeedFiles() - Started TEST-FEED-V1_0 (4 Files) 2017-01-14T07:58:59.045Z INFO [Stroom P2 #2 - GenericServerTask] server.ProxyAggregationExecutor$2 (ProxyAggregationExecutor.java:265) - processFeedFiles() - Completed TEST-FEED-V1_0 in 108 ms 2017-01-14T07:58:59.101Z INFO [Stroom P2 #3 - LifecycleTask] server.ProxyAggregationExecutor (ProxyAggregationExecutor.java:152) - exec() - completedin 349 ms  and stroomp01:\n2017-01-14T07:59:16.687Z INFO [Stroom P2 #10 - LifecycleTask] server.ProxyAggregationExecutor (ProxyAggregationExecutor.java:138) - exec() - started 2017-01-14T07:59:16.799Z INFO [Stroom P2 #5 - GenericServerTask] server.ProxyAggregationExecutor$2 (ProxyAggregationExecutor.java:203) - processFeedFiles() - Started TEST-FEED-V1_0 (3 Files) 2017-01-14T07:59:16.909Z INFO [Stroom P2 #5 - GenericServerTask] server.ProxyAggregationExecutor$2 (ProxyAggregationExecutor.java:265) - processFeedFiles() - Completed TEST-FEED-V1_0 in 110 ms 2017-01-14T07:59:16.997Z INFO [Stroom P2 #10 - LifecycleTask] server.ProxyAggregationExecutor (ProxyAggregationExecutor.java:152) - exec() - completed in 310 ms  And on refreshing the top pane of the TEST-FEED-V1_0 tab we see that two more batches of data have arrived.\n Stroom UI Test Feed - Proxy Aggregated data arrival    .\nThis demonstrates that Proxy Aggregation is working.\nStroom Forwarding Proxy Testing Data Post Tests Simple Post tests This test is to ensure the Stroom Forwarding proxy and it’s connection to the central Stroom Processing system is working.\nWe will send a file to our Forwarding proxy (stroomfp0.strmdev00.org) and monitor this nodes' proxy log files as well as all the destination nodes proxy log files. The reason for monitoring all the destination system’s proxy log files is that the destination system is probably load balancing and hence the forwarded file may turn up on any of the destination nodes.\nPerform the following\n Log onto any host where you will perform the curl post Monitor all proxy log files Log onto the Forwarding Proxy node and become the stroomuser and monitor the Stroom proxy service using the Tp bash macro. Log onto the destination Stroom nodes and become the stroomuser and monitor each node’s Stroom proxy service using the Tp bash macro. That is, on each node, run  sudo -i -u stroomuser Tp   On the ‘posting’ node, run the command  curl -k --data-binary @/etc/group \"https://stroomfp0.strmdev00.org/stroom/datafeed\" -H \"Feed:TEST-FEED-V1_0\" -H \"System:EXAMPLE_SYSTEM\" -H \"Environment:EXAMPLE_ENVIRONMENT\"  In the Stroom Forwarding proxy log, ~/stroom-proxy/instance/logs/stroom.log, you will see the arrival of the file as per the datafeed.DataFeedRequestHandler$1 event running under, in this case, the ajp-apr-9009-exec-1 thread.\n... 2017-01-01T23:17:00.240Z INFO [Repository Reader Thread 1] handler.ProxyRepositoryReader (ProxyRepositoryReader.java:143) - run() - Cron Match at 2017-01-01T23:17:00.240Z 2017-01-01T23:18:00.275Z INFO [Repository Reader Thread 1] handler.ProxyRepositoryReader (ProxyRepositoryReader.java:143) - run() - Cron Match at 2017-01-01T23:18:00.275Z 2017-01-01T23:18:12.367Z INFO [ajp-apr-9009-exec-1] datafeed.DataFeedRequestHandler$1 (DataFeedRequestHandler.java:104) - \"doPost() - Took 782 ms to process (concurrentRequestCount=1) 200\",\"Environment=EXAMPLE_ENVIRONMENT\",\"Expect=100-continue\",\"Feed=TEST-FEED-V1_0\",\"GUID=9601198e-98db-4cae-8b71-9404722ef1f9\",\"ReceivedTime=2017-01-01T23:18:11.588Z\",\"RemoteAddress=192.168.2.220\",\"RemoteHost=192.168.2.220\",\"System=EXAMPLE_SYSTEM\",\"accept=*/*\",\"content-length=1051\",\"content-type=application/x-www-form-urlencoded\",\"host=stroomfp0.strmdev00.org\",\"user-agent=curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.21 Basic ECC zlib/1.2.3 libidn/1.18 libssh2/1.4.2\"  And then at the next periodic interval (60 second intervals) this file will be forwarded to the main stroom proxy server stroomp.strmdev00.org as shown by the handler.ForwardRequestHandler events running under the pool-10-thread-2 thread.\n2017-01-01T23:19:00.304Z INFO [Repository Reader Thread 1] handler.ProxyRepositoryReader (ProxyRepositoryReader.java:143) - run() - Cron Match at 2017-01-01T23:19:00.304Z 2017-01-01T23:19:00.586Z INFO [pool-10-thread-2] handler.ForwardRequestHandler (ForwardRequestHandler.java:109) - handleHeader() - https://stroomp00.strmdev00.org/stroom/datafeed Sending request {ReceivedPath=stroomfp0.strmdev00.org, Feed=TEST-FEED-V1_0, Compression=ZIP} 2017-01-01T23:19:00.990Z INFO [pool-10-thread-2] handler.ForwardRequestHandler (ForwardRequestHandler.java:89) - handleFooter() - b5722ead-714b-411b-a09f-901fb8b20389 took 403 ms to forward 1.4 kB response 200 - {ReceivedPath=stroomfp0.strmdev00.org, Feed=TEST-FEED-V1_0, GUID=b5722ead-714b-411b-a09f-901fb8b20389, Compression=ZIP} 2017-01-01T23:20:00.064Z INFO [Repository Reader Thread 1] handler.ProxyRepositoryReader (ProxyRepositoryReader.java:143) - run() - Cron Match at 2017-01-01T23:20:00.064Z ...  On one of the central processing nodes, when the file is send by the Forwarding Proxy, you will see the file’s arrival as per the datafeed.DataFeedRequestHandler$1 event in the ajp-apr-9009-exec-3 thread.\n... 2017-01-01T23:00:00.236Z INFO [Repository Reader Thread 1] handler.ProxyRepositoryReader (ProxyRepositoryReader.java:143) - run() - Cron Match at 2017-01-01T23:00:00.236Z 2017-01-01T23:10:00.473Z INFO [Repository Reader Thread 1] handler.ProxyRepositoryReader (ProxyRepositoryReader.java:143) - run() - Cron Match at 2017-01-01T23:10:00.473Z 2017-01-01T23:19:00.787Z INFO [ajp-apr-9009-exec-3] handler.LogRequestHandler (LogRequestHandler.java:37) - log() - guid=b5722ead-714b-411b-a09f-901fb8b20389,feed=TEST-FEED-V1_0,system=null,environment=null,remotehost=null,remoteaddress=null 2017-01-01T23:19:00.981Z INFO [ajp-apr-9009-exec-3] datafeed.DataFeedRequestHandler$1 (DataFeedRequestHandler.java:104) - \"doPost() - Took 196 ms to process (concurrentRequestCount=1) 200\",\"Cache-Control=no-cache\",\"Compression=ZIP\",\"Feed=TEST-FEED-V1_0\",\"GUID=b5722ead-714b-411b-a09f-901fb8b20389\",\"ReceivedPath=stroomfp0.strmdev00.org\",\"Transfer-Encoding=chunked\",\"accept=text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2\",\"connection=keep-alive\",\"content-type=application/audit\",\"host=stroomp00.strmdev00.org\",\"pragma=no-cache\",\"user-agent=Java/1.8.0_111\" 2017-01-01T23:20:00.771Z INFO [Repository Reader Thread 1] handler.ProxyRepositoryReader (ProxyRepositoryReader.java:143) - run() - Cron Match at 2017-01-01T23:20:00.771Z ...  Stroom Standalone Proxy Testing Data Post Tests Simple Post tests This test is to ensure the Stroom Store NODB or Standalone proxy is working.\nWe will send a file to our Standalone proxy (stroomsap0.strmdev00.org) and monitor this nodes' proxy log files as well the directory the received files are meant to be stored in.\nPerform the following\n Log onto any host where you will perform the curl post Log onto the Standalone Proxy node and become the stroomuser and monitor the Stroom proxy service using the Tp bash macro. That is run  sudo -i -u stroomuser Tp   On the ‘posting’ node, run the command  curl -k --data-binary @/etc/group \"https://stroomsap0.strmdev00.org/stroom/datafeed\" -H \"Feed:TEST-FEED-V1_0\" -H \"System:EXAMPLE_SYSTEM\" -H \"Environment:EXAMPLE_ENVIRONMENT\"  In the stroom proxy log, ~/stroom-proxy/instance/logs/stroom.log, you will see the arrival of the file via both the handler.LogRequestHandler and datafeed.DataFeedRequestHandler$1 events running under, in this case, the ajp-apr-9009-exec-1 thread.\n... 2017-01-02T02:10:00.325Z INFO [Repository Reader Thread 1] handler.ProxyRepositoryReader (ProxyRepositoryReader.java:143) - run() - Cron Match at 2017-01-02T02:10:00.325Z 2017-01-02T02:11:34.501Z INFO [ajp-apr-9009-exec-1] handler.LogRequestHandler (LogRequestHandler.java:37) - log() - guid=ebd11215-7d4c-4be6-a524-358015e2ac38,feed=TEST-FEED-V1_0,system=EXAMPLE_SYSTEM,environment=EXAMPLE_ENVIRONMENT,remotehost=192.168.2.220,remoteaddress=192.168.2.220 2017-01-02T02:11:34.528Z INFO [ajp-apr-9009-exec-1] datafeed.DataFeedRequestHandler$1 (DataFeedRequestHandler.java:104) - \"doPost() - Took 33 ms to process (concurrentRequestCount=1) 200\",\"Environment=EXAMPLE_ENVIRONMENT\",\"Expect=100-continue\",\"Feed=TEST-FEED-V1_0\",\"GUID=ebd11215-7d4c-4be6-a524-358015e2ac38\",\"ReceivedTime=2017-01-02T02:11:34.501Z\",\"RemoteAddress=192.168.2.220\",\"RemoteHost=192.168.2.220\",\"System=EXAMPLE_SYSTEM\",\"accept=*/*\",\"content-length=1051\",\"content-type=application/x-www-form-urlencoded\",\"host=stroomsap0.strmdev00.org\",\"user-agent=curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.21 Basic ECC zlib/1.2.3 libidn/1.18 libssh2/1.4.2\" ...  Further, if you check the proxy’s storage directory, you will see the file 001.zip. The file names number upwards from 001.\nls -l /stroomdata/stroom-working-sap0/proxy  shows\n[stroomuser@stroomsap0 ~]$ ls -l /stroomdata/stroom-working-sap0/proxy total 4 -rw-rw-r--. 1 stroomuser stroomuser 1107 Jan 2 13:11 001.zip [stroomuser@stroomsap0 ~]$  On viewing the contents of this file we see both a .dat and .meta file.\n[stroomuser@stroomsap0 ~]$ (cd /stroomdata/stroom-working-sap0/proxy; unzip 001.zip) Archive: 001.zip inflating: 001.dat inflating: 001.meta [stroomuser@stroomsap0 ~]$  The .dat file holds the content of the file we posted - /etc/group.\n[stroomuser@stroomsap0 ~]$ (cd /stroomdata/stroom-working-sap0/proxy; head -5 001.dat) root:x:0: bin:x:1:bin,daemon daemon:x:2:bin,daemon sys:x:3:bin,adm adm:x:4:adm,daemon [stroomuser@stroomsap0 ~]$  The .meta file is generated by the proxy and holds information about the posted file\n[stroomuser@stroomsap0 ~]$ (cd /stroomdata/stroom-working-sap0/proxy; cat 001.meta) content-type:application/x-www-form-urlencoded Environment:EXAMPLE_ENVIRONMENT Feed:TEST-FEED-V1_0 GUID:ebd11215-7d4c-4be6-a524-358015e2ac38 host:stroomsap0.strmdev00.org ReceivedTime:2017-01-02T02:11:34.501Z RemoteAddress:192.168.2.220 RemoteHost:192.168.2.220 StreamSize:1051 System:EXAMPLE_SYSTEM user-agent:curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.21 Basic ECC zlib/1.2.3 libidn/1.18 libssh2/1.4.2 [stroomuser@stroomsap0 ~]$ (cd /stroomdata/stroom-working-sap0/proxy; rm 001.meta 001.dat) [stroomuser@stroomsap0 ~]$  ","categories":"","description":"This HOWTO will demonstrate various ways to test that your Stroom installation has been successful.\n","excerpt":"This HOWTO will demonstrate various ways to test that your Stroom …","ref":"/stroom-docs/hugo-docsy/docs/howtos/install/installtestinghowto/","tags":["testing","installation"],"title":"Testing Stroom Installation"},{"body":"Stroom stores data in volumes. These are the logical link to the Storage hierarchy we setup on the operating system. This HOWTO will demonstrate how one first sets up volumes and also how to add additional volumes if one expanded an existing Stroom cluster.\nAssumptions  an account with the Administrator Application Permission is currently logged in. we will add volumes as per the Multi Node Stroom deployment Storage hierarchy  Configure the Volumes We need to configure the volumes for Stroom. The follow demonstrates adding the volumes for two nodes, but demonstrates the process for a single node deployment as well the volume maintenance needed when expanding a Multi Node Cluster when adding in a new node.\nTo configure the volumes, move to the Tools item of the Main Menu and select it to bring up the Tools sub-menu. Stroom UI Tools sub-menu    then move down and select the Volumes sub-item to be presented with the Volumes configuration window as seen below. Stroom UI Volumes - configuration window    The attributes we see for each volume are\n Node - the processing node the volume resides on (this is just the node name entered when configuration the Stroom application) Path - the path to the volume Volume Type - The type of volume Public - to indicate that all nodes would access this volume Private - to indicate that only the local node will access this volume Stream Status Active - to store data within the volume Inactive - to NOT store data within the volume Closed - had stored data within the volume, but now no more data can be stored Index Status Active - to store index data within the volume Inactive - to NOT store index data within the volume Closed - had stored index data within the volume, but now no more index data can be stored Usage Date - the date and time the volume was last used Limit - the maximum amount of data the system will store on the volume Used - the amount of data in use on the volume Free - the amount of available storage on the volume Use% - the usage percentage  If you are setting up Stroom for the first time and you had accepted the default for the CREATE_DEFAULT_VOLUME_ON_START configuration option (true) when configuring the Stroom service application, you will see two default volumes have already been created. Had you set this option to false then the window would be empty.\nAdd Volumes Now from our two node Stroom Cluster example, our storage hierarchy was\n Node: stroomp00.strmdev00.org /stroomdata/stroom-data-p00 - location to store Stroom application data files (events, etc.) for this node /stroomdata/stroom-index-p00 - location to store Stroom application index files /stroomdata/stroom-working-p00 - location to store Stroom application working files (e.g. temporary files, output, etc.) for this node /stroomdata/stroom-working-p00/proxy - location for Stroom proxy to store inbound data files Node: stroomp01.strmdev00.org /stroomdata/stroom-data-p01 - location to store Stroom application data files (events, etc.) for this node /stroomdata/stroom-index-p01 - location to store Stroom application index files /stroomdata/stroom-working-p01 - location to store Stroom application working files (e.g. temporary files, output, etc.) for this node /stroomdata/stroom-working-p01/proxy - location for Stroom proxy to store inbound data files  From this we need to create four volumes. On stroomp00.strmdev00.org we create\n /stroomdata/stroom-data-p00 - location to store Stroom application data files (events, etc.) for this node /stroomdata/stroom-index-p00 - location to store Stroom application index files  and on stroomp01.strmdev00.org we create\n /stroomdata/stroom-data-p01 - location to store Stroom application data files (events, etc.) for this node /stroomdata/stroom-index-p01 - location to store Stroom application index files  So the first step to configure a volume is to move the cursor to the New icon in the top left of the Volumes window and select it. This will bring up the Add Volume configuration window\nStroom UI Add Volume - Volume configuration window    As you can see, the entry box titles reflect the attributes of a volume. So we will add the first nodes data volume\n /stroomdata/stroom-data-p00 - location to store Stroom application data files (events, etc.) for this node for node stroomp00.  If you move the the Node drop down entry box and select it you will be presented with a choice of available nodes - in this case stroomp00 and stroomp01 as we have a two node cluster with these node names.\nStroom UI Add Volume - select node    By selecting the node stroomp00 we see\nStroom UI Add Volume - selected node    To configure the rest of the attributes for this volume, we:\n enter the Path to our first node’s data volume select a Volume Type of Public as this is a data volume we want all nodes to access select a Stream Status of Active to indicate we want to store data on it select an Index Status of Inactive as we do NOT want index data stored on it set a Limit of 12GB for allowed storage  Stroom UI Add Volume - adding first data volume    and on selection of the Stroom UI OkButton    we see the changes in the Volumes configuration window\nStroom UI Add Volume - added first data volume    We next add the first node’s index volume, as per\nStroom UI Add Volume - adding first index volume    And after adding the second node’s volumes we are finally presented with our configured volumes\nStroom UI Add Volume - all volumes added    Delete Default Volumes We now need to deal with our default volumes. We want to delete them.\nStroom UI Delete Default - display default    So we move the cursor to the first volume’s line (stroomp00 /home/stroomuser/stroom-app/volumes/defaultindexVolume …) and select the line then move the cursor to the Delete icon in the top left of the Volumes window and select it. On selection you will be given a confirmation request\nStroom UI Delete Default - confirm deletion    at which we press the Stroom UI OkButton    button to see the first default volume has been deleted\nStroom UI Delete Default - first volume deleted    and after we select then delete the second default volume( stroomp00 /home/stroomuser/stroom-app/volumes/defaultStreamVolume …), we are left with\nStroom UI Delete Default - all deleted    At this one can close the Volumes configuration window by pressing the Stroom UI CloseButton    button.\nNOTE: At the time of writing there is an issue regarding volumes\nStroom Github Issue 84 - https://github.com/gchq/stroom/issues/84 Due to Issue 84, if we delete volumes in a multi node environment, the deletion is not propagated to all other nodes in a cluster. Thus if we attempted to use the volumes we would get a database error. The current workaround is to restart all the Stroom applications which will cause a reload of all volume information. This MUST be done before sending any data to your multi-node Stroom cluster.\nAdding new Volumes When one expands a Multi Node Stroom cluster deployment, after the installation of the Stroom Proxy and Application software and services on the new node, one has to configure the new volumes that are on the new node. The following demonstrates this assuming we are adding\n the new node is stroomp02 the storage hierarchy for this node is /stroomdata/stroom-data-p02 - location to store Stroom application data files (events, etc.) for this node /stroomdata/stroom-index-p02 - location to store Stroom application index files /stroomdata/stroom-working-p02 - location to store Stroom application working files (e.g. tmp, output, etc.) for this node /stroomdata/stroom-working-p02/proxy - location for Stroom proxy to store inbound data files  From this we need to create two volumes on stroomp02\n /stroomdata/stroom-data-p02 - location to store Stroom application data files (events, etc.) for this node /stroomdata/stroom-index-p02 - location to store Stroom application index files  To configure the volumes, move to the Tools item of the Main Menu and select it to bring up the Tools sub-menu. Stroom UI Tools sub-menu    then move down and select the Volumes sub-item to be presented with the Volumes configuration window as. We then move the cursor to the New icon in the top left of the Volumes window and select it. This will bring up the Add Volume configuration window where we select our volume’s node stroomp02.\nStroom UI Volumes - New Node configuration window start data volume    We select this node and then configure the rest of the attributes for this data volume\nStroom UI Volumes - New Node configuration window data volume    then press the Stroom UI OkButton    button.\nWe then add another volume for the index volume for this node with attributes as per\nStroom UI Volumes - New Node configuration window index volume added    And on pressing the Stroom UI OkButton    button we see our two new volumes for this node have been added.\nStroom UI Volumes - New Node configuration window volumes added    At this one can close the Volumes configuration window by pressing the Stroom UI CloseButton    button.\n","categories":"","description":"How to maintain Stroom's data and index volumes.\n","excerpt":"How to maintain Stroom's data and index volumes.\n","ref":"/stroom-docs/hugo-docsy/docs/howtos/install/installvolumeshowto/","tags":["volumes","installation"],"title":"Volume Maintenance"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/docs/howtos/administration/","tags":"","title":"Administration"},{"body":"In this example, the logs are in a well-defined, line based, text format so we will use a Data Splitter parser to transform the logs into simple record-based XML and then a XSLT translation to normalise them into the Event schema.\nA separate document will describe the method of automating the storage of normalised events for this feed. Further, we will not Decorate these events. Again, Event Decoration is described in another document.\nAuthor: John Doe\nLast Updated: 7 Mar 2020\nRecommended Additional Documentation: HOWTO - Enabling Processors for a Pipeline\nEvent Log Source For this example, we will use logs from an Apache HTTPD Web server. In fact, the web server in front of Stroom.\nTo get the optimal information from the Apache HTTPD access logs, we define our log format based on an extension of the BlackBox format. The format is described and defined below. This is an extract from a httpd configuration file (/etc/httpd/conf/httpd.conf)\n# Stroom - Black Box Auditing configuration # # %a - Client IP address (not hostname (%h) to ensure ip address only) # When logging the remote host, it is important to log the client IP address, not the # hostname. We do this with the '%a' directive. Even if HostnameLookups are turned on, # using '%a' will only record the IP address. For the purposes of BlackBox formats, # reversed DNS should not be trusted # %{REMOTE_PORT}e - Client source port # Logging the client source TCP port can provide some useful network data and can help # one associate a single client with multiple requests. # If two clients from the same IP address make simultaneous connections, the 'common log' # file format cannot distinguish between those clients. Otherwise, if the client uses # keep-alives, then every hit made from a single TCP session will be associated by the same # client port number. # The port information can indicate how many connections our server is handling at once, # which may help in tuning server TCP/OP settings. It will also identify which client ports # are legitimate requests if the administrator is examining a possible SYN-attack against a # server. # Note we are using the REMOTE_PORT environment variable. Environment variables only come # into play when mod_cgi or mod_cgid is handling the request. # %X - Connection status (use %c for Apache 1.3) # The connection status directive tells us detailed information about the client connection. # It returns one of three flags: # x if the client aborted the connection before completion, # + if the client has indicated that it will use keep-alives (and request additional URLS), # - if the connection will be closed after the event # Keep-Alive is a HTTP 1.1. directive that informs a web server that a client can request multiple # files during the same connection. This way a client doesn't need to go through the overhead # of re-establishing a TCP connection to retrieve a new file. # %t - time - or [%{%d/%b/%Y:%T}t.%{msec_frac}t %{%z}t] for Apache 2.4 # The %t directive records the time that the request started. # NOTE: When deployed on an Apache 2.4, or better, environment, you should use # strftime format in order to get microsecond resolution. # %l - remote logname # %u - username [in quotes] # The remote user (from auth; This may be bogus if the return status (%s) is 401 # for non-ssl services) # For SSL services, user names need to be delivered as DNs to deliver PKI user details # in full. To pass through PKI certificate properties in the correct form you need to # add the following directives to your Apache configuration: # SSLUserName SSL_CLIENT_S_DN # SSLOptions +StdEnvVars # If you cannot, then use %{SSL_CLIENT_S_DN}x in place of %u and use blackboxSSLUser # LogFormat nickname # %r - first line of text sent by web client [in quotes] # This is the first line of text send by the web client, which includes the request # method, the full URL, and the HTTP protocol. # %s - status code before any redirection # This is the status code of the original request. # %\u003es - status code after any redirection has taken place # This is the final status code of the request, after any internal redirections may # have taken place. # %D - time in microseconds to handle the request # This is the number of microseconds the server took to handle the request in microseconds # %I - incoming bytes # This is the bytes received, include request and headers. It cannot, by definition be zero. # %O - outgoing bytes # This is the size in bytes of the outgoing data, including HTTP headers. It cannot, by # definition be zero. # %B - outgoing content bytes # This is the size in bytes of the outgoing data, EXCLUDING HTTP headers. Unlike %b, which # records '-' for zero bytes transferred, %B will record '0'. # %{Referer}i - Referrer HTTP Request Header [in quotes] # This is typically the URL of the page that made the request. If linked from # e-mail or direct entry this value will be empty. Note, this can be spoofed # or turned off # %{User-Agent}i - User agent HTTP Request Header [in quotes] # This is the identifying information the client (browser) reports about itself. # It can be spoofed or turned off # %V - the server name according to the UseCannonicalName setting # This identifies the virtual host in a multi host webservice # %p - the canonical port of the server servicing the request # Define a variation of the Black Box logs # # Note, you only need to use the 'blackboxSSLUser' nickname if you cannot set the # following directives for any SSL configurations # SSLUserName SSL_CLIENT_S_DN # SSLOptions +StdEnvVars # You will also note the variation for no logio module. The logio module supports # the %I and %O formatting directive # \u003cIfModule mod_logio.c\u003e LogFormat \"%a/%{REMOTE_PORT}e %X %t %l \\\"../../\"%r\\\" %s/%\u003es %D %I/%O/%B \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %V/%p\" blackboxUser LogFormat \"%a/%{REMOTE_PORT}e %X %t %l \\\"%{SSL_CLIENT_S_DN../../\"%r\\\" %s/%\u003es %D %I/%O/%B \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %V/%p\" blackboxSSLUser \u003c/IfModule\u003e \u003cIfModule !mod_logio.c\u003e LogFormat \"%a/%{REMOTE_PORT}e %X %t %l \\\"../../\"%r\\\" %s/%\u003es %D 0/0/%B \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %V/$p\" blackboxUser LogFormat \"%a/%{REMOTE_PORT}e %X %t %l \\\"%{SSL_CLIENT_S_DN../../\"%r\\\" %s/%\u003es %D 0/0/%B \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %V/$p\" blackboxSSLUser \u003c/IfModule\u003e  A copy of this configuration can be found here.\nAs Stroom can use PKI for login, you can configure Stroom’s Apache to make use of the blackboxSSLUser log format. A sample set of logs in this format appear below.\n192.168.4.220/61801 - [18/Jan/2020:12:39:04 -0800] - \"/C=USA/ST=CA/L=Los Angeles/O=Default Company Ltd/CN=Burn Frank (burn)\" \"POST /accounting/ui/dispatch.rpc HTTP/1.1\" 200/200 21221 2289/415/14 \"https://host01.company4.org/accounting/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36\" host01.company4.org/443 192.168.4.220/61854 - [18/Jan/2020:12:40:04 -0800] - \"/C=USA/ST=CA/L=Los Angeles/O=Default Company Ltd/CN=Burn Frank (burn)\" \"POST /accounting/ui/dispatch.rpc HTTP/1.1\" 200/200 7889 2289/415/14 \"https://host01.company4.org/accounting/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36\" host01.company4.org/443 192.168.4.220/61909 - [18/Jan/2020:12:41:04 -0800] - \"/C=USA/ST=CA/L=Los Angeles/O=Default Company Ltd/CN=Burn Frank (burn)\" \"POST /accounting/ui/dispatch.rpc HTTP/1.1\" 200/200 6901 2389/3796/14 \"https://host01.company4.org/accounting/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36\" host01.company4.org/443 192.168.4.220/61962 - [18/Jan/2020:12:42:04 -0800] - \"/C=USA/ST=CA/L=Los Angeles/O=Default Company Ltd/CN=Burn Frank (burn)\" \"POST /accounting/ui/dispatch.rpc HTTP/1.1\" 200/200 11219 2289/415/14 \"https://host01.company4.org/accounting/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36\" host01.company4.org/443 192.168.3.117/62015 - [18/Jan/2020:12:43:04 -1000] - \"/C=AUS/ST=NSW/L=Sydney/O=Default Company Ltd/CN=Max Bergman (maxb)\" \"POST /accounting/ui/dispatch.rpc HTTP/1.1\" 200/200 4265 2289/415/14 \"https://stroomnode01.strmdev01.org/accounting/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36\" stroomnode01.strmdev01.org/443 192.168.3.117/62092 - [18/Jan/2020:12:44:04 -1000] - \"/C=AUS/ST=NSW/L=Sydney/O=Default Company Ltd/CN=Max Bergman (maxb)\" \"POST /accounting/ui/dispatch.rpc HTTP/1.1\" 200/200 9791 2289/415/14 \"https://stroomnode01.strmdev01.org/accounting/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36\" stroomnode01.strmdev01.org/443 192.168.3.117/62147 - [18/Jan/2020:12:44:04 -1000] - \"/C=AUS/ST=NSW/L=Sydney/O=Default Company Ltd/CN=Max Bergman (maxb)\" \"POST /accounting/ui/dispatch.rpc HTTP/1.1\" 200/200 11509 2289/415/14 \"https://stroomnode01.strmdev01.org/accounting/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36\" stroomnode01.strmdev01.org/443 192.168.3.117/62202 - [18/Jan/2020:12:44:04 -1000] - \"/C=AUS/ST=NSW/L=Sydney/O=Default Company Ltd/CN=Max Bergman (maxb)\" \"POST /accounting/ui/dispatch.rpc HTTP/1.1\" 200/200 4627 2389/3796/14 \"https://stroomnode01.strmdev01.org/accounting/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36\" stroomnode01.strmdev01.org/443 192.168.3.117/62294 - [18/Jan/2020:12:44:04 -1000] - \"/C=AUS/ST=NSW/L=Sydney/O=Default Company Ltd/CN=Max Bergman (maxb)\" \"POST /accounting/ui/dispatch.rpc HTTP/1.1\" 200/200 12367 2289/415/14 \"https://stroomnode01.strmdev01.org/accounting/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36\" stroomnode01.strmdev01.org/443 192.168.3.117/62349 - [18/Jan/2020:12:44:04 -1000] - \"/C=AUS/ST=NSW/L=Sydney/O=Default Company Ltd/CN=Max Bergman (maxb)\" \"POST /accounting/ui/dispatch.rpc HTTP/1.1\" 200/200 12765 2289/415/14 \"https://stroomnode01.strmdev01.org/accounting/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36\" stroomnode01.strmdev01.org/443 192.168.2.245/62429 - [18/Jan/2020:12:50:04 +0200] - \"/C=GBR/ST=GLOUCESTERSHIRE/L=Bristol/O=Default Company Ltd/CN=Kostas Kosta (kk)\" \"POST /accounting/ui/dispatch.rpc HTTP/1.1\" 200/200 12245 2289/415/14 \"https://stroomnode00.strmdev01.org/accounting/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36\" stroomnode00.strmdev01.org/443 192.168.2.245/62495 - [18/Jan/2020:12:51:04 +0200] - \"/C=GBR/ST=GLOUCESTERSHIRE/L=Bristol/O=Default Company Ltd/CN=Kostas Kosta (kk)\" \"POST /accounting/ui/dispatch.rpc HTTP/1.1\" 200/200 4327 2289/415/14 \"https://stroomnode00.strmdev01.org/accounting/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36\" stroomnode00.strmdev01.org/443 192.168.2.245/62549 - [18/Jan/2020:12:52:04 +0200] - \"/C=GBR/ST=GLOUCESTERSHIRE/L=Bristol/O=Default Company Ltd/CN=Kostas Kosta (kk)\" \"POST /accounting/ui/dispatch.rpc HTTP/1.1\" 200/200 7148 2289/415/14 \"https://stroomnode00.strmdev01.org/accounting/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36\" stroomnode00.strmdev01.org/443 192.168.2.245/62626 - [18/Jan/2020:12:52:04 +0200] - \"/C=GBR/ST=GLOUCESTERSHIRE/L=Bristol/O=Default Company Ltd/CN=Kostas Kosta (kk)\" \"POST /accounting/ui/dispatch.rpc HTTP/1.1\" 200/200 11386 2289/415/14 \"https://stroomnode00.strmdev01.org/accounting/\" \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36\" stroomnode00.strmdev01.org/443  A copy of this sample data source can be found here. Save a copy of this data to your local environment for use later in this HOWTO. Save this file as a text document with ANSI encoding.\nCreate the Feed and its Pipeline To reflect the source of these Accounting Logs, we will name our feed and its pipeline Apache-SSLBlackBox-V2.0-EVENTS and it will be stored in the system group Apache HTTPD under the main system group - Event Sources.\nCreate System Group To create the system group Apache HTTPD, navigate to the Event Sources/Infrastructure/WebServer system group within the Explorer pane (if this system group structure does not already exist in your Stroom instance then refer to the HOWTO Stroom Explorer Management for guidance). Left click to highlight the WebServer system group then right click to bring up the object context menu. Navigate to the New icon, then the Folder icon to reveal the New Folder selection window.\nNavigate Explorer    In the New Folder window enter Apache HTTPD into the Name: text entry box.\nCreate System Group    The click on OK at which point you will be presented with the Apache HTTPD system group configuration tab. Also note, the WebServer system group within the Explorer pane has automatically expanded to display the Apache HTTPD system group.\nCreate System Group tab    Close the Apache HTTPD system group configuration tab by clicking on the close item icon on the right-hand side of the tab closeItem    . We now need to create, in order\n the Feed, the Text Parser, the Translation and finally, the Pipeline.  Create Feed Within the Explorer pane, and having selected the Apache HTTPD group, right click to bring up object context menu. Navigate to New, Feed\nApache Create Feed    Select the Feed icon feedItem    , when the New Feed selection window comes up, ensure the Apache HTTPD system group is selected or navigate to it. Then enter the name of the feed, Apache-SSLBlackBox-V2.0-EVENTS, into the Name: text entry box the press OK.\nIt should be noted that the default Stroom FeedName pattern will not accept this name. One needs to modify the stroom.feedNamePattern stroom property to change the default pattern to ^[a-zA-Z0-9_-\\.]{4,}$. See the HOWTO on System Properties document to see how to make this change.\nNew Feed dialog    At this point you will be presented with the new feed’s configuration tab and the feed’s Explorer object will automatically appear in the Explorer pane within the Apache HTTPD system group.\nSelect the Settings tab on the feed’s configuration tab. Enter an appropriate description into the Description: text entry box, for instance:\n“Apache HTTPD events for BlackBox Version 2.0. These events are from a Secure service (https).”\nIn the Classification: text entry box, enter a Classification of the data that the event feed will contain - that is the classification or sensitivity of the accounting log’s content itself.\nAs this is not a Reference Feed, leave the Reference Feed: check box unchecked.\nWe leave the Feed Status: at Receive.\nWe leave the Stream Type: as Raw Events as this we will be sending batches (streams) of raw event logs.\nWe leave the Data Encoding: as UTF-8 as the raw logs are in this form.\nWe leave the Context Encoding: as UTF-8 as there no context events for this feed.\nWe leave the Retention Period: at Forever as we do not want to delete the raw logs.\nThis results in\nNew Feed tab    Save the feed by clicking on the Save    icon.\nCreate Text Converter Within the Explorer pane, and having selected the Apache HTTPD system group, right click to bring up object context menu, then navigate to the Text Converter Item    Text Converter item\nSelect Text Converter    and left click to select. When the New Text Converter\nNew Text Converter    selection window comes up enter the name of the feed, Apache-SSLBlackBox-V2.0-EVENTS, into the Name: text entry box then press OK. At this point you will be presented with the new text converter’s configuration tab.\nText Converter configuration tab    Enter an appropriate description into the Description: text entry box, for instance\n“Apache HTTPD events for BlackBox Version 2.0 - text converter. See Conversion for complete documentation.”\nSet the Converter Type: to be Data Splitter from drop down menu.\nText Converter configuration settings    Save the text converter by clicking on the Save    icon.\nCreate XSLT Translation Within the Explorer pane, and having selected the Apache HTTPD system group, right click to bring up object context menu, then navigate to the New icon to reveal the New sub-context menu. Next, navigate to the xsltItem    item and left click to select.\nNew XSLT    When the New XSLT selection window comes up,\nNew XSLT    enter the name of the feed, Apache-SSLBlackBox-V2.0-EVENTS, into the Name: text entry box then press OK. At this point you will be presented with the new XSLT’s configuration tab.\nNew XSLT tab    Enter an appropriate description into the Description: text entry box, for instance\n“Apache HTTPD events for BlackBox Version 2.0 - translation. See Translation for complete documentation.”\nNew XSLT settings    Save the XSLT by clicking on the Save    icon.\nCreate Pipeline In the process of creating this pipeline we have assumed that the Template Pipeline content pack has been loaded, so that we can Inherit a pipeline structure from this content pack and configure it to support this specific feed.\nWithin the Explorer pane, and having selected the Apache HTTPD system group, right click to bring up object context menu, then the New sub-context menu. Navigate to the Pipeline    Pipeline and left click to select. When the New Pipeline\nNew Pipeline    selection window comes up, navigate to, then select the Apache HTTPD system group and then enter the name of the pipeline, Apache-SSLBlackBox-V2.0-EVENTS into the Name: text entry box then press OK. At this you will be presented with the new pipeline’s configuration tab\nNew Pipeline tab    As usual, enter an appropriate Description:\n“Apache HTTPD events for BlackBox Version 2.0 - pipeline. This pipeline uses the standard event pipeline to store the events in the Event Store.”\nNew Pipeline settings    Save the pipeline by clicking on the Save    icon.\nWe now need to select the structure this pipeline will use. We need to move from the Settings sub-item on the pipeline configuration tab to the Structure sub-item. This is done by clicking on the Structure link, at which we see\nNew Pipeline Structure    Next we will choose an Event Data pipeline. This is done by inheriting it from a defined set of Template Pipelines. To do this, click on the menu selection icon to the right of the Inherit From: text display box.\nWhen the Choose item\nNew Pipeline inherited from    selection window appears, select from the Template Pipelines system group. In this instance, as our input data is text, we select (left click) the\n Pipeline    Event Data (Text) pipeline\nNew Pipeline inherited selection    then press OK. At this we see the inherited pipeline structure of\nNew Pipeline inherited structure    For the purpose of this HOWTO, we are only interested in two of the eleven (11) elements in this pipeline\n the Text Converter labelled dsParser the XSLT Translation labelled translationFilter  We now need to associate our Text Converter and Translation with the pipeline so that we can pass raw events (logs) through our pipeline in order to save them in the Event Store.\nTo associate the Text Converter, select the Text Converter icon, to display.\nNew Pipeline associate textconverter    Now identify to the Property pane (the middle pane of the pipeline configuration tab), then and double click on the textConverter Property Name to display the Edit Property selection window that allows you to edit the given property\nNew Pipeline textconverter association    We leave the Property Source: as Inherit but we need to change the Property Value: from None to be our newly created Apache-SSLBlackBox-V2.0-EVENTS Text Converter.\nTo do this, position the cursor over the menu selection Menu    icon to the right of the Value: text display box and click to select. Navigate to the Apache HTTPD system group then select the Apache-SSLBlackBox-V2.0-EVENTS text Converter\nNew Pipeline textconverter association    then press OK. At this we will see the Property Value set\nNew Pipeline textconverter association    Again press OK to finish editing this property and we see that the textConverter Property has been set to Apache-SSLBlackBox-V2.0-EVENTS\nNew Pipeline textconverter association    We perform the same actions to associate the translation.\nFirst, we select the translation Filter’s Translation Filter    icon and then within translation Filter’s Property pane we double click on the xslt Property Name to bring up the Property Editor. As before, bring up the Choose item selection window, navigate to the Apache HTTPD system group and select the Apache-SSLBlackBox-V2.0-EVENTS xslt Translation.\nNew Pipeline Translation association    We leave the remaining properties in the translation Filter’s Property pane at their default values. The result is the assignment of our translation to the xslt Property.\nNew Pipeline Translation association    For the moment, we will not associate a decoration filter.\nSave the pipeline by clicking on its Save    icon.\nManually load Raw Event test data Having established the pipeline, we can now start authoring our text converter and translation. The first step is to load some Raw Event test data. Previously in the Event Log Source of this HOWTO you saved a copy of the file sampleApacheBlackBox.log to your local environment. It contains only a few events as the content is consistently formatted. We could feed the test data by posting the file to Stroom’s accounting/datafeed url, but for this example we will manually load the file. Once developed, raw data is posted to the web service.\nSelect the ApacheHHTPDFeed    configuration tab and select the Data sub-item to display\nData Loading    This window is divided into three panes.\nThe top pane displays the Stream Table, which is a table of the latest streams that belong to the feed (clearly it’s empty).\nData Loading - Stream Table    Note that a Raw Event stream is made up of data from a single file of data or aggregation of multiple data files and also meta-data associated with the data file(s). For example, file names, file size, etc.\nThe middle pane displays a Specific feed and any linked streams. To display a Specific feed, you select it from the Stream Table above.\nData Loading - Specific Stream    The bottom pane displays the selected stream’s data or meta-data.\nData Loading - Data/Metadata    Note the Upload icon Upload    in the top left of the Stream table pane. On clicking the Upload icon, we are presented with the data Upload selection window.\nData Loading - Upload Data    As stated earlier, raw event data is normally posted as a file to the Stroom web server. As part of this posting action, a set of well-defined HTTP extra headers are sent as part of the post. These headers, in the form of key value pairs, provide additional context associated with the system sending the logs. These standard headers become Stroom feed attributes available to the Stroom translation. Common attributes are\n System - the name of the System providing the logs Environment - the environment of the system (Production, Quality Assurance, Reference, Development) Feed - the feedname itself MyHost - the fully qualified domain name of the system sending the logs MyIPaddress - the IP address of the system sending the logs MyNameServer - the name server the system resolves names through  Since our translation will want these feed attributes, we will set them in the Meta Data text entry box of the Upload selection window. Note we can skip Feed as this will automatically be assigned correctly as part of the upload action (setting it to Apache-SSLBlackBox-V2.0-EVENTS obviously). Our Meta Data: will have\n System:LinuxWebServer Environment:Production MyHost:stroomnode00.strmdev01.org MyIPaddress:192.168.2.245 MyNameServer:192.168.2.254  We select a Stream Type: of Raw Events as this data is for an Event Feed. As this is not a Reference Feed we ignore the Effective: entry box (a date/time selector).\nUpload Data    We now click the Choose File button, then navigate to the location of the raw log file you downloaded earlier, sampleApacheBlackBox.log\nUpload Data    then click Open to return to the Upload selection window where we can then press OK to perform the upload.\nUpload Data    An Alert dialog window is presented Alert    which should be closed.\nThe stream we have just loaded will now be displayed in the Streams Table pane. Note that the Specific Stream and Data/Meta-data panes are still blank.\nData Loading - Streams Table    If we select the stream by clicking anywhere along its line, the stream is highlighted and the Specific Stream and Data/Meta-data_ panes now display data.\nData Loading - Streams Table    The Specific Stream pane only displays the Raw Event stream and the Data/Meta-data pane displays the content of the log file just uploaded (the Data link). If we were to click on the Meta link at the top of the Data/Meta-data pane, the log data is replaced by this stream’s meta-data.\nData Loading - Meta-data    Note that, in addition to the feed attributes we set, the upload process added additional feed attributes of\n Feed - the feed name ReceivedTime - the time the feed was received by Stroom RemoteFile - the name of the file loaded StreamSize - the size, in bytes, of the loaded data within the stream user-agent - the user agent used to present the stream to Stroom - in this case, the Stroom user Interface  We now have data that will allow us to develop our text converter and translation.\nStep data through Pipeline - Source We now need to step our data through the pipeline.\nTo do this, set the check-box on the Specific Stream pane and we note that the previously grayed out action icons Specific Stream Action Icons activated    Select Stream to Step    We now want to step our data through the first element of the pipeline, the Text Converter. We enter Stepping Mode by pressing the stepping button Enter Stepping Mode    found at the bottom right corner of the Data/Meta-data pane.\nWe will then be requested to choose a pipeline to step with, at which, you should navigate to the Apache-SSLBlackBox-V2.0-EVENTS pipeline as per\nSelect pipeline to Step    then press OK.\nAt this point, we enter the pipeline Stepping tab\npipeline Stepping tab - Source    which, initially displays the Raw Event data from our stream. This is the Source display for the Event Pipeline.\nStep data through Pipeline - Text Converter We click on the dsParser    icon to enter the Text Converter stepping window.\npipeline Stepping tab - Text Converter    This stepping tab is divided into three sub-panes. The top one is the Text Converter editor and it will allow you to edit the text conversion. The bottom left window displays the input to the Text Converter. The bottom right window displays the output from the Text Converter for the given input.\nWe also note an error indicator - that of an error in the editor pane as indicated by the black back-grounded x and rectangular black boxes to the right of the editor’s scroll bar.\npipeline Stepping tab - Error    In essence, this means that we have no text converter to pass the Raw Event data through.\nTo correct this, we will author our text converter using the Data Splitter language. Normally this is done incrementally to more easily develop the parser. The minimum text converter contains\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.1.xsd\" version=\"3.0\"\u003e \u003csplit delimiter=\"\\n\"\u003e \u003cgroup\u003e \u003cregex pattern=\"^(.*)$\"\u003e \u003cdata name=\"rest\" value=\"$1\" /\u003e \u003c/regex\u003e \u003c/group\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  If we now press the Step First Step first    icon the error will disappear and the stepping window will show.\npipeline Stepping tab - Text Converter Simple A    As we can see, the first line of our Raw Event is displayed in the input pane and the output window holds the converted XML output where we just have a single data element with a name attribute of rest and a value attribute of the complete raw event as our regular expression matched the entire line.\nThe next incremental step in the parser, would be to parse out additional data elements. For example, in this next iteration we extract the client ip address, the client port and hold the rest of the Event in the rest data element.\nWith the text converter containing\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.1.xsd\" version=\"3.0\"\u003e \u003csplit delimiter=\"\\n\"\u003e \u003cgroup\u003e \u003cregex pattern=\"^([^/]+)/([^ ]+) (.*)$\"\u003e \u003cdata name=\"clientip\" value=\"$1\" /\u003e \u003cdata name=\"clientport\" value=\"$2\" /\u003e \u003cdata name=\"rest\" value=\"$3\" /\u003e \u003c/regex\u003e \u003c/group\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  and a click on the Refresh Current Step Step Refresh    icon we will see the output pane contain\nText Converter Simple B    We continue this incremental parsing until we have our complete parser.\nThe following is our complete Text Converter which generates xml records as defined by the Stroom records v3.0 schema.\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.1.xsd\" version=\"3.0\"\u003e \u003c!-- CLASSIFICATION: UNCLASSIFIED --\u003e \u003c!-- Release History: Release 20131001, 1 Oct 2013 - Initial release General Notes: This data splitter takes audit events for the Stroom variant of the Black Box Apache Auditing. Event Format: The following is extracted from the Configuration settings for the Stroom variant of the Black Box Apache Auditing format. # STROOM - Black Box Auditing configuration # # %a - Client IP address (not hostname (%h) to ensure ip address only) # When logging the remote host, it is important to log the client IP address, not the # hostname. We do this with the '%a' directive. Even if HostnameLookups are turned on, # using '%a' will only record the IP address. For the purposes of BlackBox formats, # reversed DNS should not be trusted # %{REMOTE_PORT}e - Client source port # Logging the client source TCP port can provide some useful network data and can help # one associate a single client with multiple requests. # If two clients from the same IP address make simultaneous connections, the 'common log' # file format cannot distinguish between those clients. Otherwise, if the client uses # keep-alives, then every hit made from a single TCP session will be associated by the same # client port number. # The port information can indicate how many connections our server is handling at once, # which may help in tuning server TCP/OP settings. It will also identify which client ports # are legitimate requests if the administrator is examining a possible SYN-attack against a # server. # Note we are using the REMOTE_PORT environment variable. Environment variables only come # into play when mod_cgi or mod_cgid is handling the request. # %X - Connection status (use %c for Apache 1.3) # The connection status directive tells us detailed information about the client connection. # It returns one of three flags: # x if the client aborted the connection before completion, # + if the client has indicated that it will use keep-alives (and request additional URLS), # - if the connection will be closed after the event # Keep-Alive is a HTTP 1.1. directive that informs a web server that a client can request multiple # files during the same connection. This way a client doesn't need to go through the overhead # of re-establishing a TCP connection to retrieve a new file. # %t - time - or [%{%d/%b/%Y:%T}t.%{msec_frac}t %{%z}t] for Apache 2.4 # The %t directive records the time that the request started. # NOTE: When deployed on an Apache 2.4, or better, environment, you should use # strftime format in order to get microsecond resolution. # %l - remote logname # # %u - username [in quotes] # The remote user (from auth; This may be bogus if the return status (%s) is 401 # for non-ssl services) # For SSL services, user names need to be delivered as DNs to deliver PKI user details # in full. To pass through PKI certificate properties in the correct form you need to # add the following directives to your Apache configuration: # SSLUserName SSL_CLIENT_S_DN # SSLOptions +StdEnvVars # If you cannot, then use %{SSL_CLIENT_S_DN}x in place of %u and use blackboxSSLUser # LogFormat nickname # %r - first line of text sent by web client [in quotes] # This is the first line of text send by the web client, which includes the request # method, the full URL, and the HTTP protocol. # %s - status code before any redirection # This is the status code of the original request. # %\u003es - status code after any redirection has taken place # This is the final status code of the request, after any internal redirections may # have taken place. # %D - time in microseconds to handle the request # This is the number of microseconds the server took to handle the request in microseconds # %I - incoming bytes # This is the bytes received, include request and headers. It cannot, by definition be zero. # %O - outgoing bytes # This is the size in bytes of the outgoing data, including HTTP headers. It cannot, by # definition be zero. # %B - outgoing content bytes # This is the size in bytes of the outgoing data, EXCLUDING HTTP headers. Unlike %b, which # records '-' for zero bytes transferred, %B will record '0'. # %{Referer}i - Referrer HTTP Request Header [in quotes] # This is typically the URL of the page that made the request. If linked from # e-mail or direct entry this value will be empty. Note, this can be spoofed # or turned off # %{User-Agent}i - User agent HTTP Request Header [in quotes] # This is the identifying information the client (browser) reports about itself. # It can be spoofed or turned off # %V - the server name according to the UseCannonicalName setting # This identifies the virtual host in a multi host webservice # %p - the canonical port of the server servicing the request # Define a variation of the Black Box logs # # Note, you only need to use the 'blackboxSSLUser' nickname if you cannot set the # following directives for any SSL configurations # SSLUserName SSL_CLIENT_S_DN # SSLOptions +StdEnvVars # You will also note the variation for no logio module. The logio module supports # the %I and %O formatting directive # \u003cIfModule mod_logio.c\u003e LogFormat \"%a/%{REMOTE_PORT}e %X %t %l \\\"%u\\\" \\\"%r\\\" %s/%\u003es %D I/%O/%B \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %V/%p\" blackboxUser LogFormat \"%a/%{REMOTE_PORT}e %X %t %l \\\"%{SSL_CLIENT_S_DN}x\\\" \\\"%r\\\" %s/%\u003es %D %I/%O/%B \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %V/%p\" blackboxSSLUser \u003c/IfModule\u003e \u003cIfModule !mod_logio.c\u003e LogFormat \"%a/%{REMOTE_PORT}e %X %t %l \\\"%u\\\" \\\"%r\\\" %s/%\u003es %D 0/0/%B \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %V/$p\" blackboxUser LogFormat \"%a/%{REMOTE_PORT}e %X %t %l \\\"%{SSL_CLIENT_S_DN}x\\\" \\\"%r\\\" %s/%\u003es %D 0/0/%B \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %V/$p\" blackboxSSLUser \u003c/IfModule\u003e --\u003e \u003c!-- Match line --\u003e \u003csplit delimiter=\"\\n\"\u003e \u003cgroup\u003e \u003cregex pattern=\"^([^/]+)/([^ ]+) ([^ ]+) \\[([^\\]]+)] ([^ ]+) \u0026#34;([^\u0026#34;]+)\u0026#34; \u0026#34;([^\u0026#34;]+)\u0026#34; (\\d+)/(\\d+) (\\d+) ([^/]+)/([^/]+)/(\\d+) \u0026#34;([^\u0026#34;]+)\u0026#34; \u0026#34;([^\u0026#34;]+)\u0026#34; ([^/]+)/([^ ]+)\"\u003e \u003cdata name=\"clientip\" value=\"$1\" /\u003e \u003cdata name=\"clientport\" value=\"$2\" /\u003e \u003cdata name=\"constatus\" value=\"$3\" /\u003e \u003cdata name=\"time\" value=\"$4\" /\u003e \u003cdata name=\"remotelname\" value=\"$5\" /\u003e \u003cdata name=\"user\" value=\"$6\" /\u003e \u003cdata name=\"url\" value=\"$7\"\u003e \u003cgroup value=\"$7\" ignoreErrors=\"true\"\u003e \u003c!-- Special case the \"GET /\" url string as opposed to the more standard \"method url protocol/protocol_version\". Also special case a url of \"-\" which occurs on some errors (eg 408) --\u003e \u003cregex pattern=\"^-$\"\u003e \u003cdata name=\"url\" value=\"error\" /\u003e \u003c/regex\u003e \u003cregex pattern=\"^([^ ]+) (/)$\"\u003e \u003cdata name=\"httpMethod\" value=\"$1\" /\u003e \u003cdata name=\"url\" value=\"$2\" /\u003e \u003c/regex\u003e \u003cregex pattern=\"^([^ ]+) ([^ ]+) ([^ /]*)/([^ ]*)\"\u003e \u003cdata name=\"httpMethod\" value=\"$1\" /\u003e \u003cdata name=\"url\" value=\"$2\" /\u003e \u003cdata name=\"protocol\" value=\"$3\" /\u003e \u003cdata name=\"version\" value=\"$4\" /\u003e \u003c/regex\u003e \u003c/group\u003e \u003c/data\u003e \u003cdata name=\"responseB\" value=\"$8\" /\u003e \u003cdata name=\"response\" value=\"$9\" /\u003e \u003cdata name=\"timeM\" value=\"$10\" /\u003e \u003cdata name=\"bytesIn\" value=\"$11\" /\u003e \u003cdata name=\"bytesOut\" value=\"$12\" /\u003e \u003cdata name=\"bytesOutContent\" value=\"$13\" /\u003e \u003cdata name=\"referer\" value=\"$14\" /\u003e \u003cdata name=\"userAgent\" value=\"$15\" /\u003e \u003cdata name=\"vserver\" value=\"$16\" /\u003e \u003cdata name=\"vserverport\" value=\"$17\" /\u003e \u003c/regex\u003e \u003c/group\u003e \u003c/split\u003e \u003c/dataSplitter\u003e  A copy of this Data Splitter can be found here.\nIf we now press the Step First Step first    icon we will see the complete parsed record\npipeline Stepping tab - Text Converter Complete    If we click on the Step Forward Step Forward    icon we will see the next event displayed in both the input and output panes.\npipeline Stepping tab - Text Converter Complete second event    we click on the Step Last Step Last    icon we will see the last event displayed in both the input and output panes.\npipeline Stepping tab - Text Converter Complete last event    You should take note of the stepping key that has been displayed in each stepping window. The stepping key are the numbers enclosed in square brackets e.g. [146271:1:14] found in the top right-hand side of the stepping window next to the stepping icons\npipeline Stepping tab - Stepping Key    The form of these keys is [ streamId ‘:’ subStreamId ‘:’ recordNo]\nwhere\n streamId - is the stream ID and won’t change when stepping through the selected stream. subStreamId - is the sub stream ID. When Stroom processes event streams it aggregates multiple input files and this is the file number. recordNo - is the record number within the sub stream.  One can double click on either the subStreamId or recordNo numbers and enter a new number. This allows you to ‘step’ around a stream rather than just relying on first, previous, next and last movement.\nNote, you should now Save Save    your edited Text Converter.\nStep data through Pipeline - Translation To start authoring the xslt Translation Filter, press the Translation Filter    icon which steps us to the xsl Translation Filter pane.\npipeline Stepping tab - Translation Initial    As for the Text Converter stepping tab, this tab is divided into three sub-panes. The top one is the xslt translation editor and it will allow you to edit the xslt translation. The bottom left window displays the input to the xslt translation (which is the output from the Text Converter). The bottom right window displays the output from the xslt Translation filter for the given input.\nWe now click on the pipeline Step Forward button Step Forward    to single step the Text Converter records element data through our xslt Translation. We see no change as an empty translation will just perform a copy of the input data.\nTo correct this, we will author our xslt translation. Like the Data Splitter this is also authored incrementally. A minimum xslt translation might contain\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003cxsl:stylesheet xpath-default-namespace=\"records:2\" xmlns=\"event-logging:3\" xmlns:stroom=\"stroom\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" version=\"3.0\"\u003e \u003c!-- Ingest the records tree --\u003e \u003cxsl:template match=\"records\"\u003e \u003cEvents xsi:schemaLocation=\"event-logging:3 file://event-logging-v3.2.3.xsd\" Version=\"3.2.3\"\u003e \u003cxsl:apply-templates /\u003e \u003c/Events\u003e \u003c/xsl:template\u003e \u003c!-- Only generate events if we have an url on input --\u003e \u003cxsl:template match=\"record[data[@name = 'url']]\"\u003e \u003cEvent\u003e \u003cxsl:apply-templates select=\".\" mode=\"eventTime\" /\u003e \u003cxsl:apply-templates select=\".\" mode=\"eventSource\" /\u003e \u003cxsl:apply-templates select=\".\" mode=\"eventDetail\" /\u003e \u003c/Event\u003e \u003c/xsl:template\u003e \u003cxsl:template match=\"node()\" mode=\"eventTime\"\u003e \u003cEventTime\u003e \u003cTimeCreated/\u003e \u003c/EventTime\u003e \u003c/xsl:template\u003e \u003cxsl:template match=\"node()\" mode=\"eventSource\"\u003e \u003cEventSource\u003e \u003cSystem\u003e \u003cName /\u003e \u003cEnvironment /\u003e \u003c/System\u003e \u003cGenerator /\u003e \u003cDevice /\u003e \u003cClient /\u003e \u003cServer /\u003e \u003cUser\u003e \u003cId /\u003e \u003c/User\u003e \u003c/EventSource\u003e \u003c/xsl:template\u003e \u003cxsl:template match=\"node()\" mode=\"eventDetail\"\u003e \u003cEventDetail\u003e \u003cView\u003e \u003cWebPage\u003e \u003cType\u003eWebPage\u003c/Type\u003e \u003c/WebPage\u003e \u003c/View\u003e \u003c/EventDetail\u003e \u003c/xsl:template\u003e \u003c/xsl:stylesheet\u003e  Translation Minimal    Clearly this doesn’t generate useful events. Our first iterative change might be to generate the TimeCreated element value. The change would be\n\u003cxsl:template match=\"node()\" mode=\"eventTime\"\u003e \u003cEventTime\u003e \u003cTimeCreated\u003e \u003cxsl:value-of select=\"stroom:format-date(data[@name = 'time']/@value, 'dd/MMM/yyyy:HH:mm:ss XX')\" /\u003e \u003c/TimeCreated\u003e \u003c/EventTime\u003e \u003c/xsl:template\u003e  Translation Minimal+    Adding in the EventSource elements (without ANY error checking!) as per\n\u003cxsl:template match=\"node()\" mode=\"eventSource\"\u003e \u003cEventSource\u003e \u003cSystem\u003e \u003cName\u003e \u003cxsl:value-of select=\"stroom:feed-attribute('System')\" /\u003e \u003c/Name\u003e \u003cEnvironment\u003e \u003cxsl:value-of select=\"stroom:feed-attribute('Environment')\" /\u003e \u003c/Environment\u003e \u003c/System\u003e \u003cGenerator\u003eApache HTTPD\u003c/Generator\u003e \u003cDevice\u003e \u003cHostName\u003e \u003cxsl:value-of select=\"stroom:feed-attribute('MyHost')\" /\u003e \u003c/HostName\u003e \u003cIPAddress\u003e \u003cxsl:value-of select=\"stroom:feed-attribute('MyIPAddress')\" /\u003e \u003c/IPAddress\u003e \u003c/Device\u003e \u003cClient\u003e \u003cIPAddress\u003e \u003cxsl:value-of select=\"data[@name = 'clientip']/@value\" /\u003e \u003c/IPAddress\u003e \u003cPort\u003e \u003cxsl:value-of select=\"data[@name = 'clientport']/@value\" /\u003e \u003c/Port\u003e \u003c/Client\u003e \u003cServer\u003e \u003cHostName\u003e \u003cxsl:value-of select=\"data[@name = 'vserver']/@value\" /\u003e \u003c/HostName\u003e \u003cPort\u003e \u003cxsl:value-of select=\"data[@name = 'vserverport']/@value\" /\u003e \u003c/Port\u003e \u003c/Server\u003e \u003cUser\u003e \u003cId\u003e \u003cxsl:value-of select=\"data[@name='user']/@value\" /\u003e \u003c/Id\u003e \u003c/User\u003e \u003c/EventSource\u003e \u003c/xsl:template\u003e  And after a Refresh Current Step Step Refresh    we see our output event ‘grow’ to\nTranslation Minimal++    We now complete our translation by expanding the EventDetail elements to have the completed translation of (again with limited error checking and non-existent documentation!)\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003cxsl:stylesheet xpath-default-namespace=\"records:2\" xmlns=\"event-logging:3\" xmlns:stroom=\"stroom\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" version=\"3.0\"\u003e \u003c!-- Ingest the records tree --\u003e \u003cxsl:template match=\"records\"\u003e \u003cEvents xsi:schemaLocation=\"event-logging:3 file://event-logging-v3.2.3.xsd\" Version=\"3.2.3\"\u003e \u003cxsl:apply-templates /\u003e \u003c/Events\u003e \u003c/xsl:template\u003e \u003c!-- Only generate events if we have an url on input --\u003e \u003cxsl:template match=\"record[data[@name = 'url']]\"\u003e \u003cEvent\u003e \u003cxsl:apply-templates select=\".\" mode=\"eventTime\" /\u003e \u003cxsl:apply-templates select=\".\" mode=\"eventSource\" /\u003e \u003cxsl:apply-templates select=\".\" mode=\"eventDetail\" /\u003e \u003c/Event\u003e \u003c/xsl:template\u003e \u003cxsl:template match=\"node()\" mode=\"eventTime\"\u003e \u003cEventTime\u003e \u003cTimeCreated\u003e \u003cxsl:value-of select=\"stroom:format-date(data[@name = 'time']/@value, 'dd/MMM/yyyy:HH:mm:ss XX')\" /\u003e \u003c/TimeCreated\u003e \u003c/EventTime\u003e \u003c/xsl:template\u003e \u003cxsl:template match=\"node()\" mode=\"eventSource\"\u003e \u003cEventSource\u003e \u003cSystem\u003e \u003cName\u003e \u003cxsl:value-of select=\"stroom:feed-attribute('System')\" /\u003e \u003c/Name\u003e \u003cEnvironment\u003e \u003cxsl:value-of select=\"stroom:feed-attribute('Environment')\" /\u003e \u003c/Environment\u003e \u003c/System\u003e \u003cGenerator\u003eApache HTTPD\u003c/Generator\u003e \u003cDevice\u003e \u003cHostName\u003e \u003cxsl:value-of select=\"stroom:feed-attribute('MyHost')\" /\u003e \u003c/HostName\u003e \u003cIPAddress\u003e \u003cxsl:value-of select=\"stroom:feed-attribute('MyIPAddress')\" /\u003e \u003c/IPAddress\u003e \u003c/Device\u003e \u003cClient\u003e \u003cIPAddress\u003e \u003cxsl:value-of select=\"data[@name = 'clientip']/@value\" /\u003e \u003c/IPAddress\u003e \u003cPort\u003e \u003cxsl:value-of select=\"data[@name = 'clientport']/@value\" /\u003e \u003c/Port\u003e \u003c/Client\u003e \u003cServer\u003e \u003cHostName\u003e \u003cxsl:value-of select=\"data[@name = 'vserver']/@value\" /\u003e \u003c/HostName\u003e \u003cPort\u003e \u003cxsl:value-of select=\"data[@name = 'vserverport']/@value\" /\u003e \u003c/Port\u003e \u003c/Server\u003e \u003cUser\u003e \u003cId\u003e \u003cxsl:value-of select=\"data[@name='user']/@value\" /\u003e \u003c/Id\u003e \u003c/User\u003e \u003c/EventSource\u003e \u003c/xsl:template\u003e \u003c!-- --\u003e \u003cxsl:template match=\"node()\" mode=\"eventDetail\"\u003e \u003cEventDetail\u003e \u003cView\u003e \u003cWebPage\u003e \u003cType\u003eWebPage\u003c/Type\u003e \u003cURL\u003e \u003cxsl:value-of select=\"data[@name = 'url']/data[@name = 'url']/@value\" /\u003e \u003c/URL\u003e \u003cxsl:if test=\"data[@name = 'referer']/@value != '-'\"\u003e \u003cReferrer\u003e \u003cxsl:value-of select=\"data[@name = 'referer']/@value\" /\u003e \u003c/Referrer\u003e \u003c/xsl:if\u003e \u003cHTTPMethod\u003e \u003cxsl:value-of select=\"data[@name = 'url']/data[@name = 'httpMethod']/@value\" /\u003e \u003c/HTTPMethod\u003e \u003cUserAgent\u003e \u003cxsl:value-of select=\"data[@name = 'userAgent']/@value\" /\u003e \u003c/UserAgent\u003e \u003cResponseCode\u003e \u003cxsl:value-of select=\"data[@name = 'response']/@value\" /\u003e \u003c/ResponseCode\u003e \u003c!-- Protocol --\u003e \u003cData Name=\"Protocol\"\u003e \u003cxsl:attribute name=\"Value\" select=\"data[@name = 'url']/data[@name = 'protocol']/@value\" /\u003e \u003c/Data\u003e \u003c!-- Protocol Version --\u003e \u003cData Name=\"Version\"\u003e \u003cxsl:attribute name=\"Value\" select=\"data[@name = 'url']/data[@name = 'version']/@value\" /\u003e \u003c/Data\u003e \u003c!-- Response Code Before --\u003e \u003cData Name=\"ResponseCodeBefore\"\u003e \u003cxsl:attribute name=\"Value\" select=\"data[@name = 'responseB']/@value\" /\u003e \u003c/Data\u003e \u003c!-- Connection Status --\u003e \u003cData Name=\"ConnectionStatus\"\u003e \u003cxsl:attribute name=\"Value\" select=\"data[@name = 'constatus']/@value\" /\u003e \u003c/Data\u003e \u003c!-- Bytes transferred --\u003e \u003cxsl:if test=\"data[@name = 'bytesIn']/@value != '0' and data[@name = 'bytesIn']/@value != '-'\"\u003e \u003cData Name=\"BytesIn\"\u003e \u003cxsl:attribute name=\"Value\" select=\"data[@name = 'bytesIn']/@value\" /\u003e \u003c/Data\u003e \u003c/xsl:if\u003e \u003cxsl:if test=\"data[@name = 'bytesOut']/@value != '0' and data[@name = 'bytesOut']/@value != '-'\"\u003e \u003cData Name=\"BytesOut\"\u003e \u003cxsl:attribute name=\"Value\" select=\"data[@name = 'bytesOut']/@value\" /\u003e \u003c/Data\u003e \u003c/xsl:if\u003e \u003cxsl:if test=\"data[@name = 'bytesOutContent']/@value != '0'\"\u003e \u003cData Name=\"BytesOutContentOnly\"\u003e \u003cxsl:attribute name=\"Value\" select=\"data[@name = 'bytesOutContent']/@value\" /\u003e \u003c/Data\u003e \u003c/xsl:if\u003e \u003c!-- Time to serve Request --\u003e \u003cxsl:if test=\"data[@name = 'timeM']/@value != '0'\"\u003e \u003cData Name=\"TotalRequestTimeMicroseconds\"\u003e \u003cxsl:attribute name=\"Value\" select=\"data[@name = 'timeM']/@value\" /\u003e \u003c/Data\u003e \u003c/xsl:if\u003e \u003c/WebPage\u003e \u003c/View\u003e \u003c/EventDetail\u003e \u003c/xsl:template\u003e \u003c/xsl:stylesheet\u003e  And after a Refresh Current Step Refresh Step Refresh    we see our complete output event\nTranslation Complete    Note, you should now Save Save    your edited xslt Translation.\nA copy of this XSLT Translation can be found here.\nWe have completed the translation and have completed developing our Apache-SSLBlackBox-V2.0-EVENTS event feed.\nAt this point, this event feed is set up to accept Raw Event data, but it will not automatically process the raw data and hence it will not place events into the Event Store. To have Stroom automatically process Raw Event streams, you will need to enable Processors for this pipeline.\n","categories":"","description":"The following will take you through the process of creating an Event Feed in Stroom.\n","excerpt":"The following will take you through the process of creating an Event …","ref":"/stroom-docs/hugo-docsy/docs/howtos/eventfeeds/createapachehttpdeventfeed/","tags":["processing","feed","httpd"],"title":"Apache HTTPD Event Feed"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/docs/howtos/authentication/","tags":["authentication"],"title":"Authentication"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/authentication/","tags":"","title":"authentication"},{"body":"Assumptions The following assumptions are used in this document.\n An account with the Administrator Application Permission is currently logged in. We will be adding the user burn We will make this user an Administrator  Add a new user To add a new user, move your cursor to the Tools item of the Main Menu and select to bring up the Tools sub-menu.\nStroom UI Tools sub-menu    then move down and select the Users and Groups sub-item to be presented with the Users and Groups configuration window as seen below.\nStroom UI New User - Users and Groups configuration    To add the user, move the cursor to the New icon in the top left and select it. On selection you will be prompted for a user name. In our case we will enter the user burn.\nStroom UI New User - Add User    and on pressing Stroom UI OkButton    will be presented with the User configuration window.\nStroom UI New User - User configuration    Set the User Application Permissions See Permissions for an explanation of the various Application Permissions a user can have.\nAssign an Administrator Permission As we want the user to be an administrator, select the Administrator Permission check-box\nStroom UI New User - User configuration - set administrator permission    Set User’s Password We need to set burn's password (which he will need to reset on first login). So, select the Stroom UI ResetPasswordButton    button to gain the Reset Password window\nStroom UI New User - User configuration - reset password    After setting a password and pressing the Stroom UI OkButton    button we get the informational Alert window as per\nStroom UI New User - User configuration - reset password complete    and on close of the Alert we are presented again with the User configuration window.\nStroom UI New User - User configuration - user added    We should close this window by pressing the Stroom UI CloseButton    button to be presented with the Users and Groups window with the new user burn added.\nStroom UI New User - User configuration - show user added    At this, one can close the Users and Groups configuration window by pressing the Stroom UI CloseButton    button at the bottom right of the window.\n","categories":"","description":"This HOWTO provides the steps to create a user via the Stroom User Interface.\n","excerpt":"This HOWTO provides the steps to create a user via the Stroom User …","ref":"/stroom-docs/hugo-docsy/docs/howtos/authentication/createuserhowto/","tags":["users"],"title":"Create a user"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/data-splitter/","tags":"","title":"data-splitter"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/docs/howtos/eventfeeds/","tags":["feed"],"title":"Event Feeds"},{"body":"Assumptions The following assumptions are used in this document.\n the user successfully deployed Stroom the following Stroom content packages have been installed  Template Pipelines XML Schemas    Introduction This HOWTO will demonstrate the process by which an Event Processing pipeline for a given Event Source is developed and deployed.\nThe sample event source used will be based on BlueCoat Proxy logs. An extract of BlueCoat logs were sourced from http://log-sharing.dreamhosters.com (a Public Security Log Sharing Site) but modified to add sample user attribution.\nTemplate pipelines are being used to simplify the establishment of this processing pipeline.\nThe sample BlueCoat Proxy log will be transformed into an intermediate simple XML key value pair structure, then into the Stroom Event Logging XML Schema (external link) format.\nEvent Source As mentioned, we will use BlueCoat Proxy logs as a sample event source. Although BlueCoat logs can be customised, the default is to use the W2C Extended Log File Format (ELF). Our sample data set looks like\n#Software: SGOS 3.2.4.28 #Version: 1.0 #Date: 2005-04-27 20:57:09 #Fields: date time time-taken c-ip sc-status s-action sc-bytes cs-bytes cs-method cs-uri-scheme cs-host cs-uri-path cs-uri-query cs-username s-hierarchy s-supplier-name rs(Content-Type) cs(User-Agent) sc-filter-result sc-filter-category x-virus-id s-ip s-sitename x-virus-details x-icap-error-code x-icap-error-details 2005-05-04 17:16:12 1 45.110.2.82 200 TCP_HIT 941 729 GET http www.inmobus.com /wcm/assets/images/imagefileicon.gif - george DIRECT 38.112.92.20 image/gif \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\" PROXIED none - 192.16.170.42 SG-HTTP-Service - none - 2005-05-04 17:16:12 2 45.110.2.82 200 TCP_HIT 941 729 GET http www.inmobus.com /wcm/assets/images/imagefileicon.gif - george DIRECT 38.112.92.20 image/gif \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\" PROXIED none - 192.16.170.42 SG-HTTP-Service - none - ...  A copy of this sample data source can be found here. Later in this HOWTO, one will be required to upload this file. If you save this file now, ensure the file is saved as a text document with ANSI encoding.\nEstablish the Processing Pipeline We will create the components that make up the processing pipeline for transforming these raw logs into the Stroom Event Logging XML Schema. They will be placed a folder appropriately named BlueCoat in the path System/Event Sources/Proxy. See Folder Creation for details on creating such a folder.\nThere will be four components\n the Event Feed to group the BlueCoat log files the Text Converter to convert the BlueCoat raw logs files into simple XML the XSLT Translation to translate the simple XML formed by the Text Converter into the Stroom Event Logging XML form, and the Processing pipeline which manages how the processing is performed.  All components will have the same Name BlueCoat-Proxy-V1.0-EVENTS. It should be noted that the default Stroom FeedName pattern will not accept this name. One needs to modify the stroom.feedNamePattern stroom property to change the default pattern to ^[a-zA-Z0-9_-\\.]{3,}$. See the HOWTO on System Properties docment to see how to make this change.\nCreate the Event Feed We first select (with a left click) the System/Event Sources/Proxy/BlueCoat folder in the Explorer tab then right click to bring up the New Item selection sub-menu.\nStroom New Item    As we are creating an Event Feed, select the Stroom UI FeedItem    item to have the New Feed configuration window into which we enter BlueCoat-Proxy-V1.0-EVENTS into the Name: entry box\nStroom UI Create Feed - New feed configuration window enter name    and press Stroom UI OkButton    to see the new Event Feed tab\nStroom UI Create Feed - New feed tab    and it’s corresponding reference in the Explorer display.\nThe configuration items for a Event Feed are\n Description - a description of the feed Classification - the classification or sensitivity of the Event Feed data Reference Feed Flag - to indicate if this is a Reference Feed or not Feed Status - which indicates if we accept data, reject it or silently drop it Stream Type - to indicate if the Feed contains raw log data or reference data Data Encoding - the character encoding of the data being sent to the Feed Context Encoding - the character encoding of context data associated with this Feed Retention Period - the amount of time to retain the Event data  In our example, we will set the above to\n Description - BlueCoat Proxy log data sent in W2C Extended Log File Format (ELFF) Classification - We will leave this blank Reference Feed Flag - We leave the check-box unchecked as this is not a Reference Feed Feed Status - We set to Receive Stream Type - We set to Raw Events as we will be sending batches (streams) of raw event logs Data Encoding - We leave at the default of UTF-8 as this is the proposed character encoding Context Encoding - We leave at the default of UTF-8 as there are no Context Events for this Feed Retention Period - We leave at Forever was we do not want to delete any collected BlueCoat event data.  Stroom UI Create Feed - New feed tab configuration    One should note that the Feed tab as been marked as having unsaved changes. This is indicated by the asterisk character * between the Feed icon Feed    and the name of the feed BlueCoat-Proxy-V1.0-EVENTS. We can save the changes to our feed by pressing the Save icon Save    in the top left of the BlueCoat-Proxy-V1.0-EVENTS tab. At this point one should notice two things, the first is that the asterisk has disappeared from the Feed tab and the the second is that the Save icon Ghosted Save    .\nStroom UI Create Feed - New feed tab saved    Create the Text Converter We now create the Text Converter for this Feed in a similar fashion to the Event Feed. We first select (with a left click) the System/Event Sources/Proxy/BlueCoat folder in the Explorer tab then right click to bring up the New Item selection sub-menu. As we are creating a Text Converter, select the Stroom UI TextConverterItem    item to have the New Text Converter configuration window.\nEnter BlueCoat-Proxy-V1.0-EVENTS into the Name: entry box and press the Stroom UI OkButton    which results in the creation of the Text Converter tab\nStroom UI Create Feed - New TextConverter tab    and it’s corresponding reference in the Explorer display.\nWe set the configuration for this Text Converter to be\n Description - Simple XML transform for BlueCoat Proxy log data sent in W2C Extended Log File Format (ELFF) Converter Type - We set to Data Splitter was we will be using the Stroom Data Splitter facility to convert the raw log data into simple XML.  Again, press the Save icon Save    to save the configuration items.\nCreate the XSLT Translation We now create the XSLT translation for this Feed in a similar fashion to the Event Feed or Text Converter. We first select (with a left click) the System/Event Sources/Proxy/BlueCoat folder in the Explorer tab then right click to bring up the New Item selection sub-menu. As we are creating a XSLT Translation, select the Stroom UI xsltItem    item to have the New XSLT configuration window.\nEnter BlueCoat-Proxy-V1.0-EVENTS into the Name: entry box and press the Stroom UI OkButton    which results in the creation of the XSLT Translation tab\nStroom UI Create Feed - New Translation tab    and it’s corresponding reference in the Explorer display.\nWe set the configuration for this XSLT Translation to be\n Description - Transform simple XML of BlueCoat Proxy log data into Stroom Event Logging XML form  Again, press the Save icon Save    to save the configuration items.\nCreate the Pipeline We now create the Pipeline for this Feed in a similar fashion to the Event Feed, Text Converter or XSLT Translation. We first select (with a left click) the System/Event Sources/Proxy/BlueCoat folder in the Explorer tab then right click to bring up the New Item selection sub-menu. As we are creating a Pipeline, select the Stroom UI pipeLineItem    item to have the New Pipeline configuration window.\nEnter BlueCoat-Proxy-V1.0-EVENTS into the Name: entry box and press the Stroom UI OkButton    which results in the creation of the Pipeline tab\nStroom UI Create Feed - New Pipeline tab    and it’s corresponding reference in the Explorer display.\nWe set the configuration for this Pipeline to be\n Description - Processing of XML of BlueCoat Proxy log data into Stroom Event Logging XML Type - We leave as Event Data as this is an Event Data pipeline  Configure Pipeline Structure We now need to configure the Structure of this Pipeline.\nWe do this by selecting the Structure hyper-link of the *BlueCoat-Proxy-V1.0-EVENTS Pipeline tab.\nAt this we see the Pipeline Structure configuration tab\nStroom UI Create Feed - Pipeline Structure tab    As noted in the Assumptions at the start, we have loaded the Template Pipeline content pack, so that we can Inherit a pipeline structure from this content pack and configure it to support this specific feed.\nWe find a template by selecting the Inherit From: entry box labeled Stroom UI NoneEntryBox    to reveal a Choose Item configuration item window.\nStroom UI Create Feed - Pipeline Structure tab - Inherit    Select the Template Pipelines folder by pressing the Stroom UI Open Folder    icon to the left of the folder to reveal the choice of available templates.\nStroom UI Create Feed - Pipeline Structure tab - Templates    For our BlueCoat feed we will select the Event Data (Text) template. This is done by moving the cursor to the relevant line and select via a left click\nStroom UI Create Feed - Pipeline Structure tab - Template Selection    then pressing Stroom UI OkButton    to see the inherited pipeline structure\nStroom UI Create Feed - Pipeline Structure tab - Template Selected    Configure Pipeline Elements For the purpose of this HOWTO, we are only interested in two of the eleven (11) elements in this pipeline\n the Text Converter labeled dsParser the XSLT Translation labeled translationFilter  We need to assign our BlueCoat-Proxy-V1.0-EVENTS Text Converter and XSLT Translation to these elements respectively.\nText Converter Configuration We do this by first selecting (left click) the dsParser element at which we see the Property sub-window displayed\nStroom UI Create Feed - Pipeline Structure tab - dsParser    We then select (left click) the textConverter Property Name\nStroom UI Create Feed - Pipeline Structure tab - dsParser selected Property    then press the Edit Property button Stroom UI EditButton    . At this, the Edit Property configuration window is displayed.\nStroom UI Create Feed - Pipeline Structure tab - dsParser Edit Property    We select the Value: entry box labeled Stroom UI NoneEntryBox    to reveal a Choose Item configuration item window.\nStroom UI Create Feed - Pipeline Structure tab - dsParser Edit Property choose item    We traverse the folder structure until we can select the BlueCoat-Proxy-V1.0-EVENTS Text Converter as per\nStroom UI Create Feed - Pipeline Structure tab - dsParser Edit Property chosen item    and then press the Stroom UI OkButton    to see that the Property Value: has been selected.\nStroom UI Create Feed - Pipeline Structure tab - dsParser set Property chosen item    and pressing the Stroom UI OkButton    button of the Edit Property configuration window results in the pipelines dsParser property being set.\nStroom UI Create Feed - Pipeline Structure tab - dsParser set Property    XSLT Translation Configuration We do this by first selecting (left click) the translationFilter element at which we see the Property sub-window displayed\nStroom UI Create Feed - Pipeline Structure tab - translationFilter    We then select (left click) the xslt Property Name\nStroom UI Create Feed - Pipeline Structure tab - xslt selected Property    and following the same steps as for the Text Converter property selection, we assign the BlueCoat-Proxy-V1.0-EVENTS XSLT Translation to the xslt property.\nStroom UI Create Feed - Pipeline Structure tab - xslt selected Property    At this point, we save these changes by pressing the Save icon Save    .\nAuthoring the Translation We are now ready to author the translation. Close all tabs except for the Welcome and BlueCoat-Proxy-V1.0-EVENTS Feed tabs.\nOn the BlueCoat-Proxy-V1.0-EVENTS Feed tab, select the Data hyper-link to be presented with the Data pane of our tab.\nStroom UI Create Feed - Translation - Data Pane    Although we can post our test data set to this feed, we will manually upload it via the Data pane. To do this we press the Upload button Stroom UI OkButton    in the top Data pane to display the Upload configuration window\nStroom UI Create Feed - Translation - Data Pane Upload    In a Production situation, where we would post log files to Stroom, we would include certain HTTP Header variables that, as we shall see, will be used as part of the translation. These header variables typically provide situational awareness of the source system sending the events.\nFor our purposes we set the following HTTP Header variables\nEnvironment:Development LogFileName:sampleBluecoat.log MyHost:\"somenode.strmdev00.org\" MyIPaddress:\"192.168.2.220 192.168.122.1\" MyMeta:\"FQDN:somenode.strmdev00.org\\nipaddress:192.168.2.220\\nipaddress_eth0:192.168.2.220\\nipaddress_lo:127.0.0.1\\nipaddress_virbr0:192.168.122.1\\n\" MyNameServer:\"gateway.strmdev00.org.\" MyTZ:+1000 Shar256:056f0d196ffb4bc6c5f3898962f1708886bb48e2f20a81fb93f561f4d16cb2aa System:Site http://log-sharing.dreamhosters.com/ Bluecoat Logs Version:V1.0  These are set by entering them into the Meta Data: entry box.\nStroom UI Create Feed - Translation - Data Pane Upload Metadata    Having done this we select a Stream Type: of Raw Events\nWe leave the Effective: entry box empty as this stream of raw event logs does not have an Effective Date (only Reference Feeds set this).\nAnd we choose our file sampleBluecoat.log, by clicking on the Browse button in the File: entry box, which brings up the brower’s standard file upload selection window. Having selected our file, we see\nStroom UI Create Feed - Translation - Data Pane Upload Complete    On pressing Stroom UI OkButton    and Alert pop-up window is presented indicating the file was uploaded\nStroom UI Create Feed - Translation - Data Pane Upload Complete Verify    Again press Stroom UI CloseButton    to show that the data has been uploaded as a Stream into the BlueCoat-Proxy-V1.0-EVENTS Event Feed.\nStroom UI Create Feed - Translation - Data Pane Show Batch    The top pane holds a table of the latest streams that pertain to the feed. We see the one item which is the stream we uploaded. If we select it, we see that a stream summary is also displayed in the centre pane (which shows details of the specific selected feed and associated streams. We also see that the bottom pane displays the data associated with the selected item. In this case, the first lines of content from the BlueCoat sample log file.\nStroom UI Create Feed - Translation - Data Pane Show Data    If we were to select the Meta hyper-link of the lower pane, one would see the metadata Stroom records for this Stream of data.\nStroom UI Create Feed - Translation - MetaData Pane Show Data    You should see all the HTTP variables we set as part of the Upload step as well as some that Stroom has automatically set.\nWe now switch back to the Data hyper-link before we start to develop the actual translation.\nStepping the Pipeline We will now author the two translation components of the pipeline, the data splitter that will transform our lines of BlueCoat data into a simple xml format and then the XSLT translation that will take this simple xml format and translate it into appropriate Stroom Event Logging XML form.\nWe start by ensuring our Raw Events Data stream is selected and we press the Enter Stepping Mode Stroom UI Enter Stepping    button on the lower right hand side of the bottom Stream Data pane.\nYou will be prompted to select a pipeline to step with. Choose the BlueCoat-Proxy-V1.0-EVENTS pipeline\nStroom UI Create Feed - Translation - Stepping Choose Pipeline    then press Stroom UI OkButton    .\nStepping the Pipeline - Source You will be presented with the Source element of the pipeline that shows our selected stream’s raw data.\nStroom UI Create Feed - Translation - Stepping Source Element    We see two panes here.\nThe top pane displays the Pipeline structure with Source selected (we could refer to this as the stepping pane) and it also displays a step indicator (three colon separated numbers enclosed in square brackets initially the numbers are dashes i.e. [-:-:-] as we have yet to step) and a set of green Stepping Actions. The step indicator and Stepping Actions allows one the step through a log file, selecting data event by event (an event is typically a line, but some events can be multi-line).\nThe bottom pane displays the first page (up to 100 lines) of data along with a set of blue Data Selection Actions. The Data Selection Actions are used to step through the source data 100 lines at a time. When multiple source log files have been aggregated into a single stream, two Data Selection Actions control buttons will be offered. The right hand one will allow a user to step though the source data as before, but the left hand set of control buttons allows one to step between files from the aggregated event log files.\nStepping the Pipeline - dsParser We now select the dsParser pipeline element that results in the window below\nStroom UI Create Feed - Translation - Stepping dsParser Element    This window is made up of four panes.\nThe top pane remains the same - a display of the pipeline structure and the step indicator and green Stepping Actions.\nThe next pane down is the editing pane for the Text Converter. This pane is used to edit the text converter that converts our line based BlueCoat Proxy logs into a XML format. We make use of the Stroom Data Splitter facility to perform this transformation. See here for complete details on the data splitter.\nThe lower two panes are the input and output displays for the text converter.\nThe authoring of this data splitter translation is outside the scope of this HOWTO. It is recommended that one reads up on the Data Splitter and review the various samples found in the Stroom Context packs published, or the Pull Requests of https://github.com/gchq/stroom-content.\nFor the purpose of this HOWTO, the Datasplitter appears below. The author believes the comments should support the understanding of the transformation.\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cdataSplitter bufferSize=\"5000000\" xmlns=\"data-splitter:3\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"data-splitter:3 file://data-splitter-v3.0.xsd\" version=\"3.0\" ignoreErrors=\"true\"\u003e \u003c!-- This datasplitter gains the Software and and Proxy version strings along with the log field names from the comments section of the log file. That is from the lines ... #Software: SGOS 3.2.4.28 #Version: 1.0 #Date: 2005-04-27 20:57:09 #Fields: date time time-taken c-ip sc-status s-action sc-bytes cs-bytes cs-method ... x-icap-error-code x-icap-error-details We use the Field values as the header for the subsequent log fields --\u003e \u003c!-- Match the software comment line and save it in _bc_software --\u003e \u003cregex id=\"software\" pattern=\"^#Software: (.+) ?\\n*\"\u003e \u003cdata name=\"_bc_software\" value=\"$1\" /\u003e \u003c/regex\u003e \u003c!-- Match the version comment line and save it in _bc_version --\u003e \u003cregex id=\"version\" pattern=\"^#Version: (.+) ?\\n*\"\u003e \u003cdata name=\"_bc_version\" value=\"$1\" /\u003e \u003c/regex\u003e \u003c!-- Match against a Fields: header comment and save all the field names in a headings --\u003e \u003cregex id=\"heading\" pattern=\"^#Fields: (.+) ?\\n*\"\u003e \u003cgroup value=\"$1\"\u003e \u003cregex pattern=\"^(\\S+) ?\\n*\"\u003e \u003cvar id=\"headings\" /\u003e \u003c/regex\u003e \u003c/group\u003e \u003c/regex\u003e \u003c!-- Skip all other comment lines --\u003e \u003cregex pattern=\"^#.+\\n*\"\u003e \u003cvar id=\"ignorea\" /\u003e \u003c/regex\u003e \u003c!-- We now match all other lines, applying the headings captured at the start of the file to each field value --\u003e \u003cregex id=\"body\" pattern=\"^[^#].+\\n*\"\u003e \u003cgroup\u003e \u003cregex pattern=\"^\u0026#34;([^\u0026#34;]*)\u0026#34; ?\\n*\"\u003e \u003cdata name=\"$headings$1\" value=\"$1\" /\u003e \u003c/regex\u003e \u003cregex pattern=\"^([^ ]+) *\\n*\"\u003e \u003cdata name=\"$headings$1\" value=\"$1\" /\u003e \u003c/regex\u003e \u003c/group\u003e \u003c/regex\u003e \u003c!-- --\u003e \u003c/dataSplitter\u003e  It should be entered into the Text Converter’s editing pane as per\nStroom UI Create Feed - Translation - Stepping dsParser textConverter code    A copy of this DataSplitter can be found here.\nAs mentioned earlier, to step the translation, one uses the green Stepping Actions.\nThe actions are\n  Step First    - progress the transformation to the first line of the translation input  Step Back    - progress the transformation one step backward  Step Forward    - progress the transformation one step forward  Step Last    - progress the transformation to the end of the translation input  Refresh Current Step    - refresh the transformation based on the current translation input  So, if one was to press the Step Forward    stepping action we would be presented with\nStroom UI Create Feed - Translation - Stepping dsParser textConverter 1    We see that the input pane has the first line of input from our sample file and the output pane has an XML record structure where we have defined a data element with the name attribute of bc_software and it’s value attribute of SGOS 3.2.4.28. The definition of the record structure can be found in the System/XML Schemas/records folder.\nThis is the result of the code in our editor\n\u003c!-- Match the software comment line and save it in _bc_software --\u003e \u003cregex id=\"software\" pattern=\"^#Software: (.+) ?\\n*\"\u003e \u003cdata name=\"_bc_software\" value=\"$1\" /\u003e \u003c/regex\u003e  If one presses the Step Forward    stepping action again, we see that we have moved to the second line of the input file with the resultant output of a data element with the name attribute of bc_version and it’s value attribute of 1.0.\nStroom UI Create Feed - Translation - Stepping dsParser textConverter 2    Stepping forward once more causes the translation to ignore the Date comment line, define a Data Splitter $headings variable from the Fields comment line and transform the first line of actual event data.\nStroom UI Create Feed - Translation - Stepping dsParser textConverter 3    We see that a \u003crecord\u003e element has been formed with multiple key value pair \u003cdata\u003e elements where the name attribute is the key and the value attribute the value. You will note that the keys have been taken from the Fields comment line which where placed in the $headings variable.\nYou should also take note that the stepping indicator has been incrementing the last number, so at this point it is displaying\n[1:1:3]  The general form of this indicator is\n'[' streamId ':' subStreamId ':' recordNo ']'  where\n streamId - is the stream ID and won’t change when stepping through the selected stream, subStreamId - is the sub stream ID. When Stroom aggregates multiple event sources for a feed, it aggregates multiple input files and this is, in effect, the file number. recordNo - is the record number within the sub stream.  One can double click on either the subStreamId or recordNo entry and enter a new value. This allows you to jump around a stream rather than just relying on first, previous, next and last movements.\nHovering the mouse over the stepping indicator will change the cursor to a hand pointer. Selecting (by a left click) the recordNo will allow you to edit it’s value (and the other values for that matter). You will see the display change from\n Stroom UI Create Feed - Translation - Stepping Indicator 1    to Stroom UI Create Feed - Translation - Stepping Indicator 2    If we change the record number from 3 to 12 then either press Enter or press the Refresh Current Step    action we see\nStroom UI Create Feed - Translation - Stepping Indicator 3    and note that a new record has been processed in the input and output panes. Further, if one steps back to the Source element of the pipeline to view the raw source file, we see that the highlighted current line is the 12th line of processed data. It is the 10th actual bluecoat event, but remember the #Software, #Version lines are considered as processed data (2+10 = 12). Also noted that the #Date and #Fields lines are not considered processed data, and hence do not contribute to the recordNo value.\nStroom UI Create Feed - Translation - Stepping Indicator 4    If we select the dsParser pipeline element then press the Step Last    action we see the recordNo jump to 31 which is the last processed line of our sample log file.\nStroom UI Create Feed - Translation - Stepping Indicator 5    Stepping the Pipeline - translationFilter We now select the translationFilter pipeline element that results in\nStroom UI Create Feed - Translation - Stepping translationFilter Element    As for the dsParser, this window is made up of four panes.\nThe top pane remains the same - a display of the pipeline structure and the step indicator and green Stepping Actions.\nThe next pane down is the editing pane for the Translation Filter. This pane is used to edit an xslt translation that converts our simple key value pair \u003crecords\u003e XML structure into another XML form.\nThe lower two panes are the input and output displays for the xslt translation. You will note that the input and output displays are identical for a null xslt translation is effectively a direct copy.\nIn this HOWTO we will transform the \u003crecords\u003e XML structure into the GCHQ Stroom Event Logging XML Schema form which is documented here.\nThe authoring of this xslt translation is outside the scope of this HOWTO, as is the use of the Stroom XML Schema. It is recommended that one reads up on XSLT Conversion and the Stroom Event Logging XML Schema and review the various samples found in the Stroom Context packs published, or the Pull Requests of https://github.com/gchq/stroom-content.\nWe will build the translation in steps. We enter an initial portion of our xslt transformation that just consumes the Software and Version key values and converts the date and time values (which are in UTC) into the EventTime/TimeCreated element. This code segment is\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003cxsl:stylesheet xpath-default-namespace=\"records:2\" xmlns=\"event-logging:3\" xmlns:stroom=\"stroom\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" version=\"3.0\"\u003e \u003c!-- Bluecoat Proxy logs in W2C Extended Log File Format (ELF) --\u003e \u003c!-- Ingest the record key value pair elements --\u003e \u003cxsl:template match=\"records\"\u003e \u003cEvents xsi:schemaLocation=\"event-logging:3 file://event-logging-v3.2.4.xsd\" Version=\"3.2.4\"\u003e \u003cxsl:apply-templates /\u003e \u003c/Events\u003e \u003c/xsl:template\u003e \u003c!-- Main record template for single event --\u003e \u003cxsl:template match=\"record\"\u003e \u003cxsl:choose\u003e \u003c!-- Store the Software and Version information of the Bluecoat log file for use in the Event Source elements which are processed later --\u003e \u003cxsl:when test=\"data[@name='_bc_software']\"\u003e \u003cxsl:value-of select=\"stroom:put('_bc_software', data[@name='_bc_software']/@value)\" /\u003e \u003c/xsl:when\u003e \u003cxsl:when test=\"data[@name='_bc_version']\"\u003e \u003cxsl:value-of select=\"stroom:put('_bc_version', data[@name='_bc_version']/@value)\" /\u003e \u003c/xsl:when\u003e \u003c!-- Process the event logs --\u003e \u003cxsl:otherwise\u003e \u003cEvent\u003e \u003cxsl:call-template name=\"event_time\" /\u003e \u003c/Event\u003e \u003c/xsl:otherwise\u003e \u003c/xsl:choose\u003e \u003c/xsl:template\u003e \u003c!-- Time --\u003e \u003cxsl:template name=\"event_time\"\u003e \u003cEventTime\u003e \u003cTimeCreated\u003e \u003cxsl:value-of select=\"concat(data[@name = 'date']/@value,'T',data[@name='time']/@value,'.000Z')\" /\u003e \u003c/TimeCreated\u003e \u003c/EventTime\u003e \u003c/xsl:template\u003e \u003c/xsl:stylesheet\u003e  After entering this translation and pressing the Refresh Current Step    action shows the display\nStroom UI Create Feed - Translation - Stepping XSLT Translation 1    Note that this is the 31st record, so if we were to jump to the first record using the Step First    action, we see that the input and output change appropriately.\nStroom UI Create Feed - Translation - Stepping XSLT Translation 2    You will note that there is no Event element in the output pane as the record template in our xslt translation above is only storing the input’s key value (_bc_software’s value).\nFurther note that the BlueCoat_Proxy-V1.0-EVENTS tab has a star in front of it and also the Save icon Save    is highlighted. This indicates that a component of the pipeline needs to be saved. In this case, the XSLT translation.\nStroom UI Create Feed - Translation - Stepping XSLT Translation 3    By pressing the Save icon, you will save the XSLT translation as it currently stands and both the star will be removed from the tab and the Save icon will no longer be highlighted.\nStroom UI Create Feed - Translation - Stepping XSLT Translation 4    We next extend out translation by authoring a event_source template to form an appropriate Stroom Event Logging EventSource element structure. Thus our translation now is\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003cxsl:stylesheet xpath-default-namespace=\"records:2\" xmlns=\"event-logging:3\" xmlns:stroom=\"stroom\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" version=\"3.0\"\u003e \u003c!-- Bluecoat Proxy logs in W2C Extended Log File Format (ELF) --\u003e \u003c!-- Ingest the record key value pair elements --\u003e \u003cxsl:template match=\"records\"\u003e \u003cEvents xsi:schemaLocation=\"event-logging:3 file://event-logging-v3.2.4.xsd\" Version=\"3.2.4\"\u003e \u003cxsl:apply-templates /\u003e \u003c/Events\u003e \u003c/xsl:template\u003e \u003c!-- Main record template for single event --\u003e \u003cxsl:template match=\"record\"\u003e \u003cxsl:choose\u003e \u003c!-- Store the Software and Version information of the Bluecoat log file for use in the Event Source elements which are processed later --\u003e \u003cxsl:when test=\"data[@name='_bc_software']\"\u003e \u003cxsl:value-of select=\"stroom:put('_bc_software', data[@name='_bc_software']/@value)\" /\u003e \u003c/xsl:when\u003e \u003cxsl:when test=\"data[@name='_bc_version']\"\u003e \u003cxsl:value-of select=\"stroom:put('_bc_version', data[@name='_bc_version']/@value)\" /\u003e \u003c/xsl:when\u003e \u003c!-- Process the event logs --\u003e \u003cxsl:otherwise\u003e \u003cEvent\u003e \u003cxsl:call-template name=\"event_time\" /\u003e \u003cxsl:call-template name=\"event_source\" /\u003e \u003c/Event\u003e \u003c/xsl:otherwise\u003e \u003c/xsl:choose\u003e \u003c/xsl:template\u003e \u003c!-- Time --\u003e \u003cxsl:template name=\"event_time\"\u003e \u003cEventTime\u003e \u003cTimeCreated\u003e \u003cxsl:value-of select=\"concat(data[@name = 'date']/@value,'T',data[@name='time']/@value,'.000Z')\" /\u003e \u003c/TimeCreated\u003e \u003c/EventTime\u003e \u003c/xsl:template\u003e \u003c!-- Template for event source--\u003e \u003cxsl:template name=\"event_source\"\u003e \u003c!-- We extract some situational awareness information that the posting script includes when posting the event data --\u003e \u003cxsl:variable name=\"_mymeta\" select=\"translate(stroom:meta('MyMeta'),'\u0026quot;', '')\" /\u003e \u003c!-- Form the EventSource node --\u003e \u003cEventSource\u003e \u003cSystem\u003e \u003cName\u003e \u003cxsl:value-of select=\"stroom:meta('System')\" /\u003e \u003c/Name\u003e \u003cEnvironment\u003e \u003cxsl:value-of select=\"stroom:meta('Environment')\" /\u003e \u003c/Environment\u003e \u003c/System\u003e \u003cGenerator\u003e \u003cxsl:variable name=\"gen\"\u003e \u003cxsl:if test=\"stroom:get('_bc_software')\"\u003e \u003cxsl:value-of select=\"concat(' Software: ', stroom:get('_bc_software'))\" /\u003e \u003c/xsl:if\u003e \u003cxsl:if test=\"stroom:get('_bc_version')\"\u003e \u003cxsl:value-of select=\"concat(' Version: ', stroom:get('_bc_version'))\" /\u003e \u003c/xsl:if\u003e \u003c/xsl:variable\u003e \u003cxsl:value-of select=\"concat('Bluecoat', $gen)\" /\u003e \u003c/Generator\u003e \u003cxsl:if test=\"data[@name='s-computername'] or data[@name='s-ip']\"\u003e \u003cDevice\u003e \u003cxsl:if test=\"data[@name='s-computername']\"\u003e \u003cName\u003e \u003cxsl:value-of select=\"data[@name='s-computername']/@value\" /\u003e \u003c/Name\u003e \u003c/xsl:if\u003e \u003cxsl:if test=\"data[@name='s-ip']\"\u003e \u003cIPAddress\u003e \u003cxsl:value-of select=\" data[@name='s-ip']/@value\" /\u003e \u003c/IPAddress\u003e \u003c/xsl:if\u003e \u003cxsl:if test=\"data[@name='s-sitename']\"\u003e \u003cData Name=\"ServiceType\" Value=\"{data[@name='s-sitename']/@value}\" /\u003e \u003c/xsl:if\u003e \u003c/Device\u003e \u003c/xsl:if\u003e \u003c!-- --\u003e \u003cClient\u003e \u003cxsl:if test=\"data[@name='c-ip']/@value != '-'\"\u003e \u003cIPAddress\u003e \u003cxsl:value-of select=\"data[@name='c-ip']/@value\" /\u003e \u003c/IPAddress\u003e \u003c/xsl:if\u003e \u003c!-- Remote Port Number --\u003e \u003cxsl:if test=\"data[@name='c-port']/@value !='-'\"\u003e \u003cPort\u003e \u003cxsl:value-of select=\"data[@name='c-port']/@value\" /\u003e \u003c/Port\u003e \u003c/xsl:if\u003e \u003c/Client\u003e \u003c!-- --\u003e \u003cServer\u003e \u003cHostName\u003e \u003cxsl:value-of select=\"data[@name='cs-host']/@value\" /\u003e \u003c/HostName\u003e \u003c/Server\u003e \u003c!-- --\u003e \u003cxsl:variable name=\"user\"\u003e \u003cxsl:value-of select=\"data[@name='cs-user']/@value\" /\u003e \u003cxsl:value-of select=\"data[@name='cs-username']/@value\" /\u003e \u003cxsl:value-of select=\"data[@name='cs-userdn']/@value\" /\u003e \u003c/xsl:variable\u003e \u003cxsl:if test=\"$user !='-'\"\u003e \u003cUser\u003e \u003cId\u003e \u003cxsl:value-of select=\"$user\" /\u003e \u003c/Id\u003e \u003c/User\u003e \u003c/xsl:if\u003e \u003cData Name=\"MyMeta\"\u003e \u003cxsl:attribute name=\"Value\" select=\"$_mymeta\" /\u003e \u003c/Data\u003e \u003c/EventSource\u003e \u003c/xsl:template\u003e \u003c/xsl:stylesheet\u003e  Stepping to the 3 record (the first real data record in our sample log) will reveal that our output pane has gained an EventSource element.\nStroom UI Create Feed - Translation - Stepping XSLT Translation 5    Note also, that our Save icon Save    is also highlighted, so we should at some point save the extensions to our translation.\nThe complete translation now follows. A copy of the XSLT translation can be found here.\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003cxsl:stylesheet xpath-default-namespace=\"records:2\" xmlns=\"event-logging:3\" xmlns:stroom=\"stroom\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" version=\"3.0\"\u003e \u003c!-- Bluecoat Proxy logs in W2C Extended Log File Format (ELF) --\u003e \u003c!-- Ingest the record key value pair elements --\u003e \u003cxsl:template match=\"records\"\u003e \u003cEvents xsi:schemaLocation=\"event-logging:3 file://event-logging-v3.2.4.xsd\" Version=\"3.2.4\"\u003e \u003cxsl:apply-templates /\u003e \u003c/Events\u003e \u003c/xsl:template\u003e \u003c!-- Main record template for single event --\u003e \u003cxsl:template match=\"record\"\u003e \u003cxsl:choose\u003e \u003c!-- Store the Software and Version information of the Bluecoat log file for use in the Event Source elements which are processed later --\u003e \u003cxsl:when test=\"data[@name='_bc_software']\"\u003e \u003cxsl:value-of select=\"stroom:put('_bc_software', data[@name='_bc_software']/@value)\" /\u003e \u003c/xsl:when\u003e \u003cxsl:when test=\"data[@name='_bc_version']\"\u003e \u003cxsl:value-of select=\"stroom:put('_bc_version', data[@name='_bc_version']/@value)\" /\u003e \u003c/xsl:when\u003e \u003c!-- Process the event logs --\u003e \u003cxsl:otherwise\u003e \u003cEvent\u003e \u003cxsl:call-template name=\"event_time\" /\u003e \u003cxsl:call-template name=\"event_source\" /\u003e \u003cxsl:call-template name=\"event_detail\" /\u003e \u003c/Event\u003e \u003c/xsl:otherwise\u003e \u003c/xsl:choose\u003e \u003c/xsl:template\u003e \u003c!-- Time --\u003e \u003cxsl:template name=\"event_time\"\u003e \u003cEventTime\u003e \u003cTimeCreated\u003e \u003cxsl:value-of select=\"concat(data[@name = 'date']/@value,'T',data[@name='time']/@value,'.000Z')\" /\u003e \u003c/TimeCreated\u003e \u003c/EventTime\u003e \u003c/xsl:template\u003e \u003c!-- Template for event source--\u003e \u003cxsl:template name=\"event_source\"\u003e \u003c!-- We extract some situational awareness information that the posting script includes when posting the event data --\u003e \u003cxsl:variable name=\"_mymeta\" select=\"translate(stroom:meta('MyMeta'),'\u0026quot;', '')\" /\u003e \u003c!-- Form the EventSource node --\u003e \u003cEventSource\u003e \u003cSystem\u003e \u003cName\u003e \u003cxsl:value-of select=\"stroom:meta('System')\" /\u003e \u003c/Name\u003e \u003cEnvironment\u003e \u003cxsl:value-of select=\"stroom:meta('Environment')\" /\u003e \u003c/Environment\u003e \u003c/System\u003e \u003cGenerator\u003e \u003cxsl:variable name=\"gen\"\u003e \u003cxsl:if test=\"stroom:get('_bc_software')\"\u003e \u003cxsl:value-of select=\"concat(' Software: ', stroom:get('_bc_software'))\" /\u003e \u003c/xsl:if\u003e \u003cxsl:if test=\"stroom:get('_bc_version')\"\u003e \u003cxsl:value-of select=\"concat(' Version: ', stroom:get('_bc_version'))\" /\u003e \u003c/xsl:if\u003e \u003c/xsl:variable\u003e \u003cxsl:value-of select=\"concat('Bluecoat', $gen)\" /\u003e \u003c/Generator\u003e \u003cxsl:if test=\"data[@name='s-computername'] or data[@name='s-ip']\"\u003e \u003cDevice\u003e \u003cxsl:if test=\"data[@name='s-computername']\"\u003e \u003cName\u003e \u003cxsl:value-of select=\"data[@name='s-computername']/@value\" /\u003e \u003c/Name\u003e \u003c/xsl:if\u003e \u003cxsl:if test=\"data[@name='s-ip']\"\u003e \u003cIPAddress\u003e \u003cxsl:value-of select=\" data[@name='s-ip']/@value\" /\u003e \u003c/IPAddress\u003e \u003c/xsl:if\u003e \u003cxsl:if test=\"data[@name='s-sitename']\"\u003e \u003cData Name=\"ServiceType\" Value=\"{data[@name='s-sitename']/@value}\" /\u003e \u003c/xsl:if\u003e \u003c/Device\u003e \u003c/xsl:if\u003e \u003c!-- --\u003e \u003cClient\u003e \u003cxsl:if test=\"data[@name='c-ip']/@value != '-'\"\u003e \u003cIPAddress\u003e \u003cxsl:value-of select=\"data[@name='c-ip']/@value\" /\u003e \u003c/IPAddress\u003e \u003c/xsl:if\u003e \u003c!-- Remote Port Number --\u003e \u003cxsl:if test=\"data[@name='c-port']/@value !='-'\"\u003e \u003cPort\u003e \u003cxsl:value-of select=\"data[@name='c-port']/@value\" /\u003e \u003c/Port\u003e \u003c/xsl:if\u003e \u003c/Client\u003e \u003c!-- --\u003e \u003cServer\u003e \u003cHostName\u003e \u003cxsl:value-of select=\"data[@name='cs-host']/@value\" /\u003e \u003c/HostName\u003e \u003c/Server\u003e \u003c!-- --\u003e \u003cxsl:variable name=\"user\"\u003e \u003cxsl:value-of select=\"data[@name='cs-user']/@value\" /\u003e \u003cxsl:value-of select=\"data[@name='cs-username']/@value\" /\u003e \u003cxsl:value-of select=\"data[@name='cs-userdn']/@value\" /\u003e \u003c/xsl:variable\u003e \u003cxsl:if test=\"$user !='-'\"\u003e \u003cUser\u003e \u003cId\u003e \u003cxsl:value-of select=\"$user\" /\u003e \u003c/Id\u003e \u003c/User\u003e \u003c/xsl:if\u003e \u003cData Name=\"MyMeta\"\u003e \u003cxsl:attribute name=\"Value\" select=\"$_mymeta\" /\u003e \u003c/Data\u003e \u003c/EventSource\u003e \u003c/xsl:template\u003e \u003c!-- Event detail --\u003e \u003cxsl:template name=\"event_detail\"\u003e \u003cEventDetail\u003e \u003c!-- We model Proxy events as either Receive or Send events depending on the method. We make use of the Receive/Send sub-elements Source/Destination to map the Client/Destination Proxy values and the Payload sub-element to map the URL and other details of the activity. If we have a query, we model it as a Criteria --\u003e \u003cTypeId\u003e \u003cxsl:value-of select=\"concat('Bluecoat-', data[@name='cs-method']/@value, '-', data[@name='cs-uri-scheme']/@value)\" /\u003e \u003cxsl:if test=\"data[@name='cs-uri-query']/@value != '-'\"\u003e-Query\u003c/xsl:if\u003e \u003c/TypeId\u003e \u003cxsl:choose\u003e \u003cxsl:when test=\"matches(data[@name='cs-method']/@value, 'GET|OPTIONS|HEAD')\"\u003e \u003cDescription\u003eReceipt of information from a Resource via Proxy\u003c/Description\u003e \u003cReceive\u003e \u003cxsl:call-template name=\"setupParticipants\" /\u003e \u003cxsl:call-template name=\"setPayload\" /\u003e \u003cxsl:call-template name=\"setOutcome\" /\u003e \u003c/Receive\u003e \u003c/xsl:when\u003e \u003cxsl:otherwise\u003e \u003cDescription\u003eTransmission of information to a Resource via Proxy\u003c/Description\u003e \u003cSend\u003e \u003cxsl:call-template name=\"setupParticipants\" /\u003e \u003cxsl:call-template name=\"setPayload\" /\u003e \u003cxsl:call-template name=\"setOutcome\" /\u003e \u003c/Send\u003e \u003c/xsl:otherwise\u003e \u003c/xsl:choose\u003e \u003c/EventDetail\u003e \u003c/xsl:template\u003e \u003c!-- Establish the Source and Destination nodes --\u003e \u003cxsl:template name=\"setupParticipants\"\u003e \u003cSource\u003e \u003cDevice\u003e \u003cxsl:if test=\"data[@name='c-ip']/@value != '-'\"\u003e \u003cIPAddress\u003e \u003cxsl:value-of select=\"data[@name='c-ip']/@value\" /\u003e \u003c/IPAddress\u003e \u003c/xsl:if\u003e \u003c!-- Remote Port Number --\u003e \u003cxsl:if test=\"data[@name='c-port']/@value !='-'\"\u003e \u003cPort\u003e \u003cxsl:value-of select=\"data[@name='c-port']/@value\" /\u003e \u003c/Port\u003e \u003c/xsl:if\u003e \u003c/Device\u003e \u003c/Source\u003e \u003cDestination\u003e \u003cDevice\u003e \u003cHostName\u003e \u003cxsl:value-of select=\"data[@name='cs-host']/@value\" /\u003e \u003c/HostName\u003e \u003c/Device\u003e \u003c/Destination\u003e \u003c/xsl:template\u003e \u003c!-- Define the Payload node --\u003e \u003cxsl:template name=\"setPayload\"\u003e \u003cPayload\u003e \u003cxsl:if test=\"data[@name='cs-uri-query']/@value != '-'\"\u003e \u003cCriteria\u003e \u003cDataSources\u003e \u003cDataSource\u003e \u003cxsl:value-of select=\"concat(data[@name='cs-uri-scheme']/@value, '://', data[@name='cs-host']/@value)\" /\u003e \u003cxsl:if test=\"data[@name='cs-uri-path']/@value != '/'\"\u003e \u003cxsl:value-of select=\"data[@name='cs-uri-path']/@value\" /\u003e \u003c/xsl:if\u003e \u003c/DataSource\u003e \u003c/DataSources\u003e \u003cQuery\u003e \u003cRaw\u003e \u003cxsl:value-of select=\"data[@name='cs-uri-query']/@value\" /\u003e \u003c/Raw\u003e \u003c/Query\u003e \u003c/Criteria\u003e \u003c/xsl:if\u003e \u003cResource\u003e \u003c!-- Check for auth groups the URL belongs to --\u003e \u003cxsl:variable name=\"authgroups\"\u003e \u003cxsl:value-of select=\"data[@name='cs-auth-group']/@value\" /\u003e \u003cxsl:if test=\"exists(data[@name='cs-auth-group']) and exists(data[@name='cs-auth-groups'])\"\u003e,\u003c/xsl:if\u003e \u003cxsl:value-of select=\"data[@name='cs-auth-groups']/@value\" /\u003e \u003c/xsl:variable\u003e \u003cxsl:choose\u003e \u003cxsl:when test=\"contains($authgroups, ',')\"\u003e \u003cGroups\u003e \u003cxsl:for-each select=\"tokenize($authgroups, ',')\"\u003e \u003cGroup\u003e \u003cId\u003e \u003cxsl:value-of select=\".\" /\u003e \u003c/Id\u003e \u003c/Group\u003e \u003c/xsl:for-each\u003e \u003c/Groups\u003e \u003c/xsl:when\u003e \u003cxsl:when test=\"$authgroups != '-' and $authgroups != ''\"\u003e \u003cGroups\u003e \u003cGroup\u003e \u003cId\u003e \u003cxsl:value-of select=\"$authgroups\" /\u003e \u003c/Id\u003e \u003c/Group\u003e \u003c/Groups\u003e \u003c/xsl:when\u003e \u003c/xsl:choose\u003e \u003c!-- Re-form the URL --\u003e \u003cURL\u003e \u003cxsl:value-of select=\"concat(data[@name='cs-uri-scheme']/@value, '://', data[@name='cs-host']/@value)\" /\u003e \u003cxsl:if test=\"data[@name='cs-uri-path']/@value != '/'\"\u003e \u003cxsl:value-of select=\"data[@name='cs-uri-path']/@value\" /\u003e \u003c/xsl:if\u003e \u003c/URL\u003e \u003cHTTPMethod\u003e \u003cxsl:value-of select=\"data[@name='cs-method']/@value\" /\u003e \u003c/HTTPMethod\u003e \u003cxsl:if test=\"data[@name='cs(User-Agent)']/@value !='-'\"\u003e \u003cUserAgent\u003e \u003cxsl:value-of select=\"data[@name='cs(User-Agent)']/@value\" /\u003e \u003c/UserAgent\u003e \u003c/xsl:if\u003e \u003c!-- Inbound activity --\u003e \u003cxsl:if test=\"data[@name='sc-bytes']/@value !='-'\"\u003e \u003cInboundSize\u003e \u003cxsl:value-of select=\"data[@name='sc-bytes']/@value\" /\u003e \u003c/InboundSize\u003e \u003c/xsl:if\u003e \u003cxsl:if test=\"data[@name='sc-bodylength']/@value !='-'\"\u003e \u003cInboundContentSize\u003e \u003cxsl:value-of select=\"data[@name='sc-bodylength']/@value\" /\u003e \u003c/InboundContentSize\u003e \u003c/xsl:if\u003e \u003c!-- Outbound activity --\u003e \u003cxsl:if test=\"data[@name='cs-bytes']/@value !='-'\"\u003e \u003cOutboundSize\u003e \u003cxsl:value-of select=\"data[@name='cs-bytes']/@value\" /\u003e \u003c/OutboundSize\u003e \u003c/xsl:if\u003e \u003cxsl:if test=\"data[@name='cs-bodylength']/@value !='-'\"\u003e \u003cOutboundContentSize\u003e \u003cxsl:value-of select=\"data[@name='cs-bodylength']/@value\" /\u003e \u003c/OutboundContentSize\u003e \u003c/xsl:if\u003e \u003c!-- Miscellaneous --\u003e \u003cRequestTime\u003e \u003cxsl:value-of select=\"data[@name='time-taken']/@value\" /\u003e \u003c/RequestTime\u003e \u003cResponseCode\u003e \u003cxsl:value-of select=\"data[@name='sc-status']/@value\" /\u003e \u003c/ResponseCode\u003e \u003cxsl:if test=\"data[@name='rs(Content-Type)']/@value != '-'\"\u003e \u003cMimeType\u003e \u003cxsl:value-of select=\"data[@name='rs(Content-Type)']/@value\" /\u003e \u003c/MimeType\u003e \u003c/xsl:if\u003e \u003cxsl:if test=\"data[@name='cs-categories']/@value != 'none' or data[@name='sc-filter-category']/@value != 'none'\"\u003e \u003cCategory\u003e \u003cxsl:value-of select=\"data[@name='cs-categories']/@value\" /\u003e \u003cxsl:value-of select=\"data[@name='sc-filter-category']/@value\" /\u003e \u003c/Category\u003e \u003c/xsl:if\u003e \u003c!-- Take up other items as data elements --\u003e \u003cxsl:apply-templates select=\"data[@name='s-action']\" /\u003e \u003cxsl:apply-templates select=\"data[@name='cs-uri-scheme']\" /\u003e \u003cxsl:apply-templates select=\"data[@name='s-hierarchy']\" /\u003e \u003cxsl:apply-templates select=\"data[@name='sc-filter-result']\" /\u003e \u003cxsl:apply-templates select=\"data[@name='x-virus-id']\" /\u003e \u003cxsl:apply-templates select=\"data[@name='x-virus-details']\" /\u003e \u003cxsl:apply-templates select=\"data[@name='x-icap-error-code']\" /\u003e \u003cxsl:apply-templates select=\"data[@name='x-icap-error-details']\" /\u003e \u003c/Resource\u003e \u003c/Payload\u003e \u003c/xsl:template\u003e \u003c!-- Generic Data capture template so we capture all other Bluecoat objects not already consumed --\u003e \u003cxsl:template match=\"data\"\u003e \u003cxsl:if test=\"@value != '-'\"\u003e \u003cData Name=\"{@name}\" Value=\"{@value}\" /\u003e \u003c/xsl:if\u003e \u003c/xsl:template\u003e \u003c!-- Set up the Outcome node. We only set an Outcome for an error state. The absence of an Outcome infers success --\u003e \u003cxsl:template name=\"setOutcome\"\u003e \u003cxsl:choose\u003e \u003c!-- Favour squid specific errors first --\u003e \u003cxsl:when test=\"data[@name='sc-status']/@value \u003e 500\"\u003e \u003cOutcome\u003e \u003cSuccess\u003efalse\u003c/Success\u003e \u003cDescription\u003e \u003cxsl:call-template name=\"responseCodeDesc\"\u003e \u003cxsl:with-param name=\"code\" select=\"data[@name='sc-status']/@value\" /\u003e \u003c/xsl:call-template\u003e \u003c/Description\u003e \u003c/Outcome\u003e \u003c/xsl:when\u003e \u003c!-- Now check for 'normal' errors --\u003e \u003cxsl:when test=\"data[@name='sc-status']/@value \u003e 400\"\u003e \u003cOutcome\u003e \u003cSuccess\u003efalse\u003c/Success\u003e \u003cDescription\u003e \u003cxsl:call-template name=\"responseCodeDesc\"\u003e \u003cxsl:with-param name=\"code\" select=\"data[@name='sc-status']/@value\" /\u003e \u003c/xsl:call-template\u003e \u003c/Description\u003e \u003c/Outcome\u003e \u003c/xsl:when\u003e \u003c/xsl:choose\u003e \u003c/xsl:template\u003e \u003c!-- Response Code map to Descriptions --\u003e \u003cxsl:template name=\"responseCodeDesc\"\u003e \u003cxsl:param name=\"code\" /\u003e \u003cxsl:choose\u003e \u003c!-- Informational --\u003e \u003cxsl:when test=\"$code = 100\"\u003eContinue\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 101\"\u003eSwitching Protocols\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 102\"\u003eProcessing\u003c/xsl:when\u003e \u003c!-- Successful Transaction --\u003e \u003cxsl:when test=\"$code = 200\"\u003eOK\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 201\"\u003eCreated\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 202\"\u003eAccepted\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 203\"\u003eNon-Authoritative Information\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 204\"\u003eNo Content\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 205\"\u003eReset Content\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 206\"\u003ePartial Content\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 207\"\u003eMulti Status\u003c/xsl:when\u003e \u003c!-- Redirection --\u003e \u003cxsl:when test=\"$code = 300\"\u003eMultiple Choices\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 301\"\u003eMoved Permanently\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 302\"\u003eMoved Temporarily\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 303\"\u003eSee Other\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 304\"\u003eNot Modified\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 305\"\u003eUse Proxy\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 307\"\u003eTemporary Redirect\u003c/xsl:when\u003e \u003c!-- Client Error --\u003e \u003cxsl:when test=\"$code = 400\"\u003eBad Request\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 401\"\u003eUnauthorized\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 402\"\u003ePayment Required\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 403\"\u003eForbidden\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 404\"\u003eNot Found\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 405\"\u003eMethod Not Allowed\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 406\"\u003eNot Acceptable\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 407\"\u003eProxy Authentication Required\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 408\"\u003eRequest Timeout\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 409\"\u003eConflict\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 410\"\u003eGone\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 411\"\u003eLength Required\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 412\"\u003ePrecondition Failed\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 413\"\u003eRequest Entity Too Large\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 414\"\u003eRequest URI Too Large\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 415\"\u003eUnsupported Media Type\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 416\"\u003eRequest Range Not Satisfiable\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 417\"\u003eExpectation Failed\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 422\"\u003eUnprocessable Entity\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 424\"\u003eLocked/Failed Dependency\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 433\"\u003eUnprocessable Entity\u003c/xsl:when\u003e \u003c!-- Server Error --\u003e \u003cxsl:when test=\"$code = 500\"\u003eInternal Server Error\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 501\"\u003eNot Implemented\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 502\"\u003eBad Gateway\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 503\"\u003eService Unavailable\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 504\"\u003eGateway Timeout\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 505\"\u003eHTTP Version Not Supported\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 507\"\u003eInsufficient Storage\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 600\"\u003eSquid: header parsing error\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 601\"\u003eSquid: header size overflow detected while parsing/roundcube: software configuration error\u003c/xsl:when\u003e \u003cxsl:when test=\"$code = 603\"\u003eroundcube: invalid authorization\u003c/xsl:when\u003e \u003cxsl:otherwise\u003e \u003cxsl:value-of select=\"concat('Unknown Code:', $code)\" /\u003e \u003c/xsl:otherwise\u003e \u003c/xsl:choose\u003e \u003c/xsl:template\u003e \u003c/xsl:stylesheet\u003e  Refreshing the current event will show the output pane contains\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\"?\u003e \u003cEvents xmlns=\"event-logging:3\" xmlns:stroom=\"stroom\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" xsi:schemaLocation=\"event-logging:3 file://event-logging-v3.2.4.xsd\" Version=\"3.2.4\"\u003e \u003cEvent\u003e \u003cEventTime\u003e \u003cTimeCreated\u003e2005-05-04T17:16:12.000Z\u003c/TimeCreated\u003e \u003c/EventTime\u003e \u003cEventSource\u003e \u003cSystem\u003e \u003cName\u003eSite http://log-sharing.dreamhosters.com/ Bluecoat Logs\u003c/Name\u003e \u003cEnvironment\u003eDevelopment\u003c/Environment\u003e \u003c/System\u003e \u003cGenerator\u003eBluecoat Software: SGOS 3.2.4.28 Version: 1.0\u003c/Generator\u003e \u003cDevice\u003e \u003cIPAddress\u003e192.16.170.42\u003c/IPAddress\u003e \u003cData Name=\"ServiceType\" Value=\"SG-HTTP-Service\" /\u003e \u003c/Device\u003e \u003cClient\u003e \u003cIPAddress\u003e45.110.2.82\u003c/IPAddress\u003e \u003c/Client\u003e \u003cServer\u003e \u003cHostName\u003ewww.inmobus.com\u003c/HostName\u003e \u003c/Server\u003e \u003cUser\u003e \u003cId\u003egeorge\u003c/Id\u003e \u003c/User\u003e \u003cData Name=\"MyMeta\" Value=\"FQDN:somenode.strmdev00.org\\nipaddress:192.168.2.220\\nipaddress_eth0:192.168.2.220\\nipaddress_lo:127.0.0.1\\nipaddress_virbr0:192.168.122.1\\n\" /\u003e \u003c/EventSource\u003e \u003cEventDetail\u003e \u003cTypeId\u003eBluecoat-GET-http\u003c/TypeId\u003e \u003cDescription\u003eReceipt of information from a Resource via Proxy\u003c/Description\u003e \u003cReceive\u003e \u003cSource\u003e \u003cDevice\u003e \u003cIPAddress\u003e45.110.2.82\u003c/IPAddress\u003e \u003c/Device\u003e \u003c/Source\u003e \u003cDestination\u003e \u003cDevice\u003e \u003cHostName\u003ewww.inmobus.com\u003c/HostName\u003e \u003c/Device\u003e \u003c/Destination\u003e \u003cPayload\u003e \u003cResource\u003e \u003cURL\u003ehttp://www.inmobus.com/wcm/assets/images/imagefileicon.gif\u003c/URL\u003e \u003cHTTPMethod\u003eGET\u003c/HTTPMethod\u003e \u003cUserAgent\u003eMozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\u003c/UserAgent\u003e \u003cInboundSize\u003e941\u003c/InboundSize\u003e \u003cOutboundSize\u003e729\u003c/OutboundSize\u003e \u003cRequestTime\u003e1\u003c/RequestTime\u003e \u003cResponseCode\u003e200\u003c/ResponseCode\u003e \u003cMimeType\u003eimage/gif\u003c/MimeType\u003e \u003cData Name=\"s-action\" Value=\"TCP_HIT\" /\u003e \u003cData Name=\"cs-uri-scheme\" Value=\"http\" /\u003e \u003cData Name=\"s-hierarchy\" Value=\"DIRECT\" /\u003e \u003cData Name=\"sc-filter-result\" Value=\"PROXIED\" /\u003e \u003cData Name=\"x-icap-error-code\" Value=\"none\" /\u003e \u003c/Resource\u003e \u003c/Payload\u003e \u003c/Receive\u003e \u003c/EventDetail\u003e \u003c/Event\u003e \u003c/Events\u003e  for the given input\n\u003c?xml version=\"1.1\" encoding=\"UTF-8\"?\u003e \u003crecords xmlns=\"records:2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"records:2 file://records-v2.0.xsd\" version=\"2.0\"\u003e \u003crecord\u003e \u003cdata name=\"date\" value=\"2005-05-04\" /\u003e \u003cdata name=\"time\" value=\"17:16:12\" /\u003e \u003cdata name=\"time-taken\" value=\"1\" /\u003e \u003cdata name=\"c-ip\" value=\"45.110.2.82\" /\u003e \u003cdata name=\"sc-status\" value=\"200\" /\u003e \u003cdata name=\"s-action\" value=\"TCP_HIT\" /\u003e \u003cdata name=\"sc-bytes\" value=\"941\" /\u003e \u003cdata name=\"cs-bytes\" value=\"729\" /\u003e \u003cdata name=\"cs-method\" value=\"GET\" /\u003e \u003cdata name=\"cs-uri-scheme\" value=\"http\" /\u003e \u003cdata name=\"cs-host\" value=\"www.inmobus.com\" /\u003e \u003cdata name=\"cs-uri-path\" value=\"/wcm/assets/images/imagefileicon.gif\" /\u003e \u003cdata name=\"cs-uri-query\" value=\"-\" /\u003e \u003cdata name=\"cs-username\" value=\"george\" /\u003e \u003cdata name=\"s-hierarchy\" value=\"DIRECT\" /\u003e \u003cdata name=\"s-supplier-name\" value=\"38.112.92.20\" /\u003e \u003cdata name=\"rs(Content-Type)\" value=\"image/gif\" /\u003e \u003cdata name=\"cs(User-Agent)\" value=\"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\" /\u003e \u003cdata name=\"sc-filter-result\" value=\"PROXIED\" /\u003e \u003cdata name=\"sc-filter-category\" value=\"none\" /\u003e \u003cdata name=\"x-virus-id\" value=\"-\" /\u003e \u003cdata name=\"s-ip\" value=\"192.16.170.42\" /\u003e \u003cdata name=\"s-sitename\" value=\"SG-HTTP-Service\" /\u003e \u003cdata name=\"x-virus-details\" value=\"-\" /\u003e \u003cdata name=\"x-icap-error-code\" value=\"none\" /\u003e \u003cdata name=\"x-icap-error-details\" value=\"-\" /\u003e \u003c/record\u003e \u003c/records\u003e  Do not forget to Save Save    the translation as we are complete.\nSchema Validation One last point, validating the use of the Stroom Event Logging Schema is performed in the schemaFilter component of the pipeline. Had our translation resulted in a malformed Event, this pipeline component displays any errors. In the screen below, we have purposely changed the EventTime/TimeCreated element to be EventTime/TimeCreatd. If one selects the schemaFilter component and then Refresh Refresh Current Step    the current step, we will see that\n there is an error as indicated by a square Red box Error Indicator    in the top right hand corner there is a Red rectangle line indicator mark Error Line Indicator    on the right hand side in the display slide bar there is a Red error marker Error Marker    on the left hand side  Stroom UI Create Feed - Translation - Stepping XSLT Translation 6    Hovering over the error marker Error Marker    on the left hand side will bring a pop-up describing the error.\nStroom UI Create Feed - Translation - Stepping XSLT Translation 7    At this point, close the BlueCoat-Proxy-V1.0-EVENTS stepping tab, acknowledging you do not want to save your errant changes\nStroom UI Create Feed - Translation - Stepping XSLT Translation 8    by pressing the Stroom UI OkButton    button.\nAutomated Processing Now that we have authored our translation, we want to enable Stroom to automatically process streams of raw event log data as it arrives. We do this by configuring a Processor in the BlueCoat-Proxy-V1.0-EVENTS pipeline.\nAdding a Pipeline Processor Open the BlueCoat-Proxy-V1.0-EVENTS pipeline by selecting it (double left click) in the Explorer display to show\nStroom UI Enable Processing    To configure a Processor we select the Processors hyper-link of the *BlueCoat-Proxy-V1.0-EVENTS Pipeline tab to reveal\nStroom UI Enable Processing - Processors table    We add a Processor by pressing the add processor button Add Processor    in the top left hand corner. At this you will be presented with an Add Filter configuration window.\nStroom UI Enable Processing - Add Filter 1    As we wish to create a Processor that will automatically process all BlueCoat-Proxy-V1.0-EVENTS feed Raw Events we will select the BlueCoat-Proxy-V1.0-EVENTS Feed and Raw Event Stream Type.\nTo select the feed, we press the Edit button Stroom UI EditButton    . At this, the Choose Feeds To Include And Exclude configuration window is displayed.\nStroom UI Enable Processing - Add Filter 2    As we need to Include the BlueCoat-Proxy-V1.0-EVENTS Feed in our selection, press the Add    button in the Include: pane of the window to be presented with a Choose Item configuration window.\nStroom UI Enable Processing - Add Filter 3    Navigate to the Event Sources/Proxy/BlueCoat folder and select the BlueCoat-Proxy-V1.0-EVENTS Feed\nStroom UI Enable Processing - Add Filter 4    then press the Stroom UI OkButton    button to select and see that the feed is included.\nStroom UI Enable Processing - Add Filter 5    Again press the Stroom UI OkButton    button to close the Choose Feeds To Include And Exclude window to show that we have selected our feed in the Feeds: selection pane of the Add Filter configuration window.\nStroom UI Enable Processing - Add Filter 6    We now need to select our Stream Type. Press the Add    button in the Stream Types: pane of the window to be presented with a Add Stream Type window with a Stream Type: selection drop down.\nStroom UI Enable Processing - Add Filter 7    We select (left click) the drop down selection to display the types of Stream we can choose\nStroom UI Enable Processing - Add Filter 8    and as we are selecting Raw Events we select that item then press the Stroom UI OkButton    button at which we see that our Add Filter configuration window displays\n Stroom UI Enable Processing - Add Filter 9    .\nAs we have selected our filter items, press the Stroom UI OkButton    button to display our configured Processors.\n Stroom UI Enable Processing - Configured Processors    .\nWe now see our display is divided into two panes. The Processors table pane at the top and the specific Processor pane below. In our case, our filter selection has left the BlueCoat-Proxy-V1.0-EVENTS Filter selected in the Processors table\n Stroom UI Enable Processing - Configured Processors - Selected Processor    .\nand the specific filter’s details in the bottom pane.\n Stroom UI Enable Processing - Configured Processors - Selected Processor Detail    .\nThe column entries in the Processors Table pane describe\n Pipeline - the name of the Processor pipeline ( Filter    ) Tracker Ms - the last time the tracker updated Tracker % - the percentage of available streams completed Last Poll Age - the last time the processor found new streams to process Task Count - the number of processor tasks currently running Priority - the queue scheduling priority of task submission to available stream processors Streams - the number of streams that have been processed (includes currently running streams) Events - ?? Status - the status of the processor. Normally empty if the number of stream is open-ended. If only are subset of streams were chosen (e.g. a time range in the filter) then the status will be Complete Enabled - check box to indicate the processor is enabled  We now need only Enable both the pipeline Processor and the pipeline Filter for automatic processing to occur. We do this by selecting both check boxes in the Enabled column.\nStroom UI Enable Processing - Configured Processors - Enable Processor    If we refresh our Processor table by pressing the Stroom UI RefreshButton    button in the top right hand corner, we will see that more table entries have been filled in.\nStroom UI Enable Processing - Configured Processors - Enable Processor Result    We see that the tracker last updated at 2018-07-14T04:00:35.289Z, the percentage complete is 100 (we only had one stream after all), the last time active streams were checked for was 2.3 minutes ago, there are no tasks running and that 1 stream has completed. Note that the Status column is blank as we have an open ended filter in that the processor will continue to select and process any new stream of Raw Events coming into the BlueCoat-Proxy-V1.0-EVENTS feed.\nIf we return to the BlueCoat-Proxy-V1.0-EVENTS* Feed tab, ensuring the **Data** hyper-link is selected and then refresh ( Refresh    ) the top pane that holds the summary of the latest Feed streams\nStroom UI Enable Processing - Configured Processors - Feed Display    We see a new entry in the table. The columns display\n Created - The time the stream was created. Type - The type of stream. Our new entry has a type of ‘Events’ as we have processed our Raw Events data. Feed - The name of the stream’s feed Pipeline - The name of the pipeline involved in the generation of the stream Raw - The size in bytes of the raw stream data Disk - The size in bytes of the raw stream data when stored in compressed form on the disk Read - The number of records read by a pipeline Write - The number of records (events) written by a pipeline. In this case the difference is that we did not generate events for the Software or Version records we read. Fatal - The number of fatal errors the pipeline encountered when processing this stream Error - The number of errors the pipeline encountered when processing this stream Warn - The number of warnings the pipeline encountered when processing this stream Info - The number of informational alerts the pipeline encountered when processing this stream Retention - The retention period for this stream of data  If we also refresh ( Refresh    ) the specific feed pane (middle) we again see a new entry of the Events Type\nStroom UI Enable Processing - Configured Processors - Specific Feed Display    If we select (left click) on the Events Type in either pane, we will see that the data pane displays the first event in the GCHQ Stroom Event Logging XML Schema form.\nStroom UI Enable Processing - Configured Processors - Event Display    We can now send a file of BlueCoat Proxy logs to our Stroom instance from a Linux host using curl command and see how Stroom will automatically processes the file. Use the command\ncurl -k --data-binary @sampleBluecoat.log https://stroomp.strmdev00.org/stroom/datafeed -H\"Feed:BlueCoat-Proxy-V1.0-EVENTS\" -H\"Environment:Development\" -H\"LogFileName:sampleBluecoat.log\" -H\"MyHost:\\\"somenode.strmdev00.org\\\"\" -H\"MyIPaddress:\\\"192.168.2.220 192.168.122.1\\\"\" -H\"System:Site http://log-sharing.dreamhosters.com/ Bluecoat Logs\" -H\"Version:V1.0\"  After Stroom’s Proxy aggregation has occurred, we will see that the new file posted via curl has been loaded into Stroom as per\nStroom UI Enable Processing - Configured Processors - New Posted Stream    and this new Raw Event stream is automatically processed a few seconds later as per\nStroom UI Enable Processing - Configured Processors - New Posted Stream Processed    We note that since we have used the same sample file again, the Stream sizes and record counts are the same.\nIf we switch to the Processors tab of the pipeline we see that the Tracker timestamp has changed and the number of Streams processed has increased.\nStroom UI Enable Processing - Configured Processors - New Posted Stream Processors    ","categories":"","description":"This HOWTO is provided to assist users in setting up Stroom to process inbound raw event logs and transform them into the Stroom Event Logging XML Schema.\n","excerpt":"This HOWTO is provided to assist users in setting up Stroom to process …","ref":"/stroom-docs/hugo-docsy/docs/howtos/eventfeeds/processinghowto/","tags":["processing"],"title":"Event Processing"},{"body":"This HOWTO demonstrates how to manage Feeds\nAssumptions  All Sections an account with the Administrator Application Permission is currently logged in.  Creation of an Event Feed We will be creating an Event Feed with the name TEST-FEED-V1_0.\nOnce you have logged in, move the cursor to the System folder within the Explorer tab and select it.\nStroom UI Create Feed - System selected    Once selected, right click to bring up the New Item selection sub-menu. By selecting the System folder we are requesting any new item created to be placed within it.\nStroom UI Create Feed - New item sub-menu    Now move the cursor to the Feed sub-item Stroom UI FeedItem    and select it. You will be presented with a New Feed configuration window.\nStroom UI Create Feed - New feed configuration window    You will note that the System folder has already been selected as the parent group and all we need to do is enter our feed’s name in the Name: entry box\nStroom UI Create Feed - New feed configuration window enter name    On pressing Stroom UI OkButton    we are presented with the Feed tab for our new feed. The tab is labelled with the feed name TEST-FEED-V1_0.\n Stroom UI Create Feed - New feed tab    .\nWe will leave the definitions of the Feed attributes for the present, but we will enter a Description: for our feed as we should ALWAYS do this fundamental tenet of data management - document the data. We will use the description of ‘Feed for installation validation only. No data value’.\n Stroom UI Create Feed - New feed tab with Description    .\nOne should note that the Feed tab as been marked as having unsaved changes. This is indicated by the asterisk character * between the Feed icon Feed    and the name of the feed TEST-FEED-V1_0. We can save the changes to our feed by pressing the Save icon Save    in the top left of the TEST-FEED-V1_0 tab. At this point one should notice two things, the first is that the asterisk has disappeared from the Feed tab and the Save icon Save    is ghosted.\n Stroom UI Create Feed - New feed tab with description saved    .\nFolder Structure for Event Sources In order to simplify the management of multiple event sources being processed by Stroom, it is suggested that an Event Source folder is created at the root of the System folder in the Explorer tab.\nThis can be achived by moving the cursor to the System folder within the Explorer tabe and select it. Once selected, right click to bring up the New Item selection sub-menu.\nStroom UI Create Folder - New item sub-menu    Now move the cursor to the Folder sub-item Stroom UI FolderItem    and select it. You will be presented with a New Folder configuration window.\nStroom UI Create Folder - New folder configuration window    You will note that the System folder has already been selected as the parent group and all we need to do is enter our folders’s name in the Name: entry box\nStroom UI Create Folder - New folder configuration window enter name    On pressing Stroom UI OkButton    we are presented with the Folder tab for our new folder. The tab is labelled with the folder name Event Sources.\n Stroom UI Create Folder - New folder tab    .\nYou will also note that the Explorer tab has displayed the Event Sources folder in its display.\nCreate Folder for specific Event Source In order to manage all artefacts of a given Event Source (aka Feed), one would create an appropriately named sub-folder within the Event Sources folder structure.\nIn this example, we will create one for a BlueCoat Proxy Feed.\nAs we may eventually have multiple proxy event sources, we will first create a Proxy folder in the Event Sources before creating the desired BlueCoat folder that will hold the processing components.\nSo, move the cusor to the Event Sources folder on the Explorer tab, select it and then right click to bring up the New Item selection sub-menu and move the cursor to the Folder sub-item Stroom UI FolderItem    and select it. You will be presented with a New Folder configuration window.\nEnter Proxy as the folder Name:\nStroom UI Create Folder - New sub folder configuration window    and press the Stroom UI OkButton    .\nAt this you will be presented with a new Folder tab for the new sub-folder and we note that it has been added below the Event Sources folder in the Explorer tab\n Stroom UI Create Folder - New sub folder tab    .\nRepeat this process to create the desired BlueCoat sub-folder with the result\n Stroom UI Create Folder - New BlueCoat sub folder tab    .\n","categories":"","description":"This HOWTO demonstrates how to manage feeds.\n","excerpt":"This HOWTO demonstrates how to manage feeds.\n","ref":"/stroom-docs/hugo-docsy/docs/howtos/general/feedmanagementhowto/","tags":["feed"],"title":"Feed Management"},{"body":"","categories":"","description":"General How Tos for using Stroom.\n","excerpt":"General How Tos for using Stroom.\n","ref":"/stroom-docs/hugo-docsy/docs/howtos/general/","tags":"","title":"General"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/index/","tags":"","title":"index"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/jobs/","tags":"","title":"jobs"},{"body":"Assumptions The following assumptions are used in this document.\n for manual login, we will log in as the user admin whose password is set to admin and the password is pre-expired for PKI Certificate login, the Stroom deployment would have been configured to accept PKI Logins  Manual Login Within the Login panel, enter admin into the User Name: entry box and admin into the Password: entry box as per\nStroom UI Login - logging in as admin    When you press the Stroom UI LoginButton    button, you are advised that your user’s password has expired and you need to change it.\nStroom UI Login - password expiry    Press the Stroom UI OkButton    button and enter the old password admin and a new password with confirmation in the appropriate entry boxes.\nStroom UI Login - password change    Again press the Stroom UI OkButton    button to see the confirmation that the password has changed.\n Stroom UI Login - password change confirmation    .\nOn pressing Stroom UI CloseButton    you will be logged in as the admin user and you will be presented with the Main Menu (Item Tools Monitoring User Help), and the Explorer and Welcome panels (or tabs).\n Stroom UI Login - user logged in    .\nWe have now successfully logged on as the admin user.\nThe next time you login with this account, you will not be prompted to change the password until the password expiry period has been met.\nPKI Certificate Login To login using a PKI Certificate, a user should have their Personal PKI certificate loaded in the browser (and selected if you have multiple certificates) and the user just needs to go to the Stroom UI URL, and providing you have an account, you will be automatically logged in.\n","categories":"","description":"This HOWTO shows how to log into the Stroom User Interface.\n","excerpt":"This HOWTO shows how to log into the Stroom User Interface.\n","ref":"/stroom-docs/hugo-docsy/docs/howtos/authentication/userloginhowto/","tags":["authentication"],"title":"Login"},{"body":"Assumptions The following assumptions are used in this document.\n the user admin is currently logged in  Log out of UI To log out of the UI, select the User item of the Main Menu and to bring up the User sub-menu.\nStroom UI - User Sub-menu    and select the Logout sub-item and confirm you wish to log out by selecting the Stroom UI OkButton    button.\nStroom UI - User Logout    This will return you to the Login page Stroom UI Login Page    ","categories":"","description":"This HOWTO shows how to log out of the Stroom User Interface.\n","excerpt":"This HOWTO shows how to log out of the Stroom User Interface.\n","ref":"/stroom-docs/hugo-docsy/docs/howtos/authentication/userlogouthowto/","tags":["authentication"],"title":"Logout"},{"body":"The Stroom documentation has been re-vamped with a new look and feel. The documentation is now versioned so that you are only reading documentation applicable to one minor version of stroom.\nOther improvements:\n Addtion of News and Releases sections Better site and page navigation with the left and right hand menus Tagging of content to help find related material  ","categories":"","description":"","excerpt":"The Stroom documentation has been re-vamped with a new look and feel. …","ref":"/stroom-docs/hugo-docsy/news/news/20210708-new-docs-site/","tags":"","title":"New Stroom Documentation Site"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/raw/","tags":"","title":"raw"},{"body":"Stroom v6.1 introduced a new feature (stroom:source()) to allow a translation developer to obtain positional details of the source file that is currently being processed. Using the positional information it is possible to tag Events with sufficient details to link back to the Raw source.\nAssumptions  You have a working pipeline that processes logs into Events. Events are indexed You have a Dashboard uses a Search Extraction pipeline.  Steps   Create a new XSLT called Source Decoration containing the following:\n\u003cxsl:stylesheet xpath-default-namespace=\"event-logging:3\" xmlns:sm=\"stroom-meta\" xmlns=\"event-logging:3\" xmlns:rec=\"records:2\" xmlns:stroom=\"stroom\" version=\"3.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"\u003e \u003cxsl:template match=\"@*|node()\"\u003e \u003cxsl:copy\u003e \u003cxsl:apply-templates select=\"@*|node()\" /\u003e \u003c/xsl:copy\u003e \u003c/xsl:template\u003e \u003cxsl:template match=\"Event/Meta[not(sm:source)]\"\u003e \u003cxsl:copy\u003e \u003cxsl:apply-templates /\u003e \u003cxsl:copy-of select=\"stroom:source()\" /\u003e \u003c/xsl:copy\u003e \u003c/xsl:template\u003e \u003cxsl:template match=\"Event[not(Meta)]\"\u003e \u003cxsl:copy\u003e \u003cxsl:element name=\"Meta\"\u003e \u003cxsl:copy-of select=\"stroom:source()\" /\u003e \u003c/xsl:element\u003e \u003cxsl:apply-templates /\u003e \u003c/xsl:copy\u003e \u003c/xsl:template\u003e \u003c/xsl:stylesheet\u003e  This XSLT will add or augment the Meta section of the Event with the source details.\n  Insert a new XSLT filter into your translation pipeline after your translation filter and set it to the XSLT created above.\n  Reprocess the Events through the modified pipeline, also ensure your Events are indexed.\n  Amend the translation performed by the Extraction pipeline to include the new data items that represent the source position data. Add the following to the XSLT:\n\u003cxsl:element name=\"data\"\u003e \u003cxsl:attribute name=\"name\"\u003e \u003cxsl:text\u003esrc-id\u003c/xsl:text\u003e \u003c/xsl:attribute\u003e \u003cxsl:attribute name=\"value\" select=\"Meta/sm:source/sm:id\" /\u003e \u003c/xsl:element\u003e \u003cxsl:element name=\"data\"\u003e \u003cxsl:attribute name=\"name\"\u003e \u003cxsl:text\u003esrc-partNo\u003c/xsl:text\u003e \u003c/xsl:attribute\u003e \u003cxsl:attribute name=\"value\" select=\"Meta/sm:source/sm:partNo\" /\u003e \u003c/xsl:element\u003e \u003cxsl:element name=\"data\"\u003e \u003cxsl:attribute name=\"name\"\u003e \u003cxsl:text\u003esrc-recordNo\u003c/xsl:text\u003e \u003c/xsl:attribute\u003e \u003cxsl:attribute name=\"value\" select=\"Meta/sm:source/sm:recordNo\" /\u003e \u003c/xsl:element\u003e \u003cxsl:element name=\"data\"\u003e \u003cxsl:attribute name=\"name\"\u003e \u003cxsl:text\u003esrc-lineFrom\u003c/xsl:text\u003e \u003c/xsl:attribute\u003e \u003cxsl:attribute name=\"value\" select=\"Meta/sm:source/sm:lineFrom\" /\u003e \u003c/xsl:element\u003e \u003cxsl:element name=\"data\"\u003e \u003cxsl:attribute name=\"name\"\u003e \u003cxsl:text\u003esrc-colFrom\u003c/xsl:text\u003e \u003c/xsl:attribute\u003e \u003cxsl:attribute name=\"value\" select=\"Meta/sm:source/sm:colFrom\" /\u003e \u003c/xsl:element\u003e \u003cxsl:element name=\"data\"\u003e \u003cxsl:attribute name=\"name\"\u003e \u003cxsl:text\u003esrc-lineTo\u003c/xsl:text\u003e \u003c/xsl:attribute\u003e \u003cxsl:attribute name=\"value\" select=\"Meta/sm:source/sm:lineTo\" /\u003e \u003c/xsl:element\u003e \u003cxsl:element name=\"data\"\u003e \u003cxsl:attribute name=\"name\"\u003e \u003cxsl:text\u003esrc-colTo\u003c/xsl:text\u003e \u003c/xsl:attribute\u003e \u003cxsl:attribute name=\"value\" select=\"Meta/sm:source/sm:colTo\" /\u003e \u003c/xsl:element\u003e    Open your dashboard, now add the following custom fields to your table:\n${src-id}, ${src-partNo}, ${src-recordNo}, ${src-lineFrom}, ${src-lineTo}, ${src-colFrom}, ${src-colTo}    Now add a New Text Window to your Dashboard, and configure it as below: TextWindow Config      You can also add a column to the table that will open a data window showing the source. Add a custom column with the following expression:\ndata('Raw Log',${src-id},${src-partNo},'',${src-lineFrom},${src-colFrom},${src-lineTo},${src-colTo})    ","categories":"","description":"How to link every Event back to the Raw log\n","excerpt":"How to link every Event back to the Raw log\n","ref":"/stroom-docs/hugo-docsy/docs/howtos/general/rawsourcetracking/","tags":["source","raw"],"title":"Raw Source Tracking"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/source/","tags":"","title":"source"},{"body":"Assumptions The following assumptions are used in this document.\n the user successfully logged into Stroom with the appropriate administrative privilege (Manage Properties).  Introduction Certain Stroom System Properties can be edited via the Stroom User Interface.\nEditing a System Property To edit a System Property select the Tools item of the Main Menu and select to bring up the Tools sub-menu.\nStroom UI Tools sub-menu    Then move down and select the Properties sub-item to be presented with System Properties configuration window as seen below.\nStroom UI Tools System Properties    Using the Scrollbar to the right of the System Properties configuration window and scroll down to the line where the property one wants to modify is displayed then select (left click) the line. In the example below we have selected the stroom.maxStreamSize property.\nStroom UI Tools System Properties - Selected Property    Now bring up the editing window by double clicking on the selected line. At this we will be presented with the Application Property - stroom.maxStreamSize editing window.\nStroom UI Tools System Properties - Editing Property    Now edit the property, by double clicking the string in the Value entry box. In this case we select the 1G value to see\nStroom UI Tools System Properties - Editing Property - Value selected    Now change the selected 1G value to the value we want. In this example, we are changing the value to 512M.\nStroom UI Tools System Properties - Editing Property - Value changed    At this, press the Stroom UI OkButton    to see the new value updated in the System Properties configuration window\nStroom UI Tools System Properties - Value changed    ","categories":"","description":"This HOWTO is provided to assist users in managing Stroom **System Properties** via the User Interface.\n","excerpt":"This HOWTO is provided to assist users in managing Stroom **System …","ref":"/stroom-docs/hugo-docsy/docs/howtos/administration/systemproperties/","tags":["properties","configuration"],"title":"System Properties"},{"body":"Various Tasks run in the background within Stroom. This HOWTO demonstrates how to manage these tasks\nAssumptions  All Sections an account with the Administrator Application Permission is currently logged in. Proxy Aggregation Tasks we have a multi node Stroom cluster with two nodes, stroomp00 and stroomp01. Stream Processor Tasks we have a multi node Stroom cluster with two nodes, stroomp00 and stroomp01. when demonstrating adding a new node to an existing cluster, the new node is stroomp02.  Proxy Aggregation Turn Off Proxy Aggregation We first select the Monitoring item of the Main Menu to bring up the Monitoring sub-menu.\nStroom UI Monitoring sub-menu    then move down and select the Jobs sub-item to be presented with the Jobs configuration tab as seen below.\nStroom UI Jobs Management - management tab    At this we can select the Proxy Aggregation Job whose check-box is selected and the tab will show the individual Stroom Processor nodes in the deployment.\nStroom UI Jobs Management - Proxy Aggregation Job    At this, uncheck the Enabled check-boxes for both nodes and also the main Proxy Aggregation check-box to see.\nStroom UI Jobs Management - Proxy Aggregation Job Off    At this point, no new proxy aggregation will occur and any inbound files received by the Store Proxies will accumulate in the proxy storage area.\nTurn On Proxy Aggregation We first select the Monitoring item of the Main Menu to bring up the Monitoring sub-menu.\nStroom UI Monitoring sub-menu    then move down and select the Jobs sub-item then select the Proxy Aggregation Job to be presented with the Jobs configuration tab as seen below.\nStroom UI Jobs Management - Proxy Aggregation Job Off    Now, re-enable each node’s Proxy Aggregation check-box and the main Proxy Aggregation check-box.\nAfter checking the check-boxes, perform a refresh of the display by pressing the Refresh icon Stroom UI RefreshButton    on the top right of the lower (node display) pane. You should note the Last Executed date/time change to see\n Stroom UI Test Feed - Re-enable Proxy Aggregation    .\nStream Processors Enable Stream Processors To enable the Stream Processors task, move to the Monitoring item of the Main Menu and select it to bring up the Monitoring sub-menu.\nStroom UI Monitoring sub-menu    then move down and select the Jobs sub-item to be presented with the Jobs configuration tab as seen below.\nStroom UI Jobs Management - management tab    At this, we select the Stream Processor Job whose check-box is not selected and the tab will show the individual Stroom Processor nodes in the Stroom deployment.\nStroom UI Jobs Management - Stream Processor    Clearly, if it was a single node Stroom deployment, you would only see the one node at the bottom of the Jobs configuration tab.\nWe enable nodes nodes by selecting their check-boxes as well as the main Stream Processors check-box. Do so.\nStroom UI Jobs Management - Stream Processor enabled    That is it. Stroom will automatically take note of these changes and internally start each node’s Stroom Processor task.\nEnable Stream Processors On New Node When one expands a Multi Node Stroom cluster deployment, after the installation of the Stroom Proxy and Application software and services on the new node, we need to enable it’s Stream Processors task.\nTo enable the Stream Processors for this new node, move to the Monitoring item of the Main Menu and select it to bring up the Monitoring sub-menu.\nStroom UI Monitoring sub-menu    then move down and select the Jobs sub-item to be presented with the Jobs configuration tab as seen below.\nStroom UI Jobs Management - management tab    At this we select the Stream Processor Job whose check-box is selected\nStroom UI Jobs Management - Stream Processor new node    We enable the new node by selecting it’s check-box.\nStroom UI Jobs Management - Stream Processor enabled on new node    ","categories":"","description":"This HOWTO demonstrates how to manage background tasks.\n","excerpt":"This HOWTO demonstrates how to manage background tasks.\n","ref":"/stroom-docs/hugo-docsy/docs/howtos/general/taskshowto/","tags":["tasks","jobs"],"title":"Task Management"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/tasks/","tags":"","title":"tasks"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/users/","tags":"","title":"users"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/visualisation/","tags":"","title":"visualisation"},{"body":" For a detailed list of the changes in v7.0 see the changelog (external link)\n Integrated Authentication The previously standalone (in v6) stroom-auth-service and stroom-auth-ui services have been integrated into the core stroom application. This simplifies the installation and configuration of stroom.\nConfiguration Properties Improvements Configuration is now provided by YAML files on boot Previously stroom used a flat .conf file to manage the application configuration. Application logging was configured either via a .yml file (in v6) or in an .xml file (in v5). Now stroom uses a single .yml file to configure the application and logging. This file is different to the .yml files(s) used in the docker compose configuration. The YAML file provides a more logical hierarchical structure and support for typed values (longs, doubles, maps, lists, etc.).\nThe YAML configuration is intended for configuration items that are either needed to bootstrap stroom or have values that are specific to a node. Cluster wide configuration properties are still stored in the database and managed via the UI.\nThere has been a change to the precedence of the configuration properties held in different locations (YAML, database, default) and this is described in Properties.\nStroom Home and relative paths The concept of Stroom Home has been introduced. Stroom Home allows for one path to be configured and for all other configurable paths to default to being a child of this path. This keeps all configured directories in one place by default. Each configured directory can be set to an absolute path if a location outside Stroom Home is required. If a relative path is used it will be relative to Stroom Home. Stroom Home can be configured with the property stroom.path.home.\nImproved Properties UI screens that tell you the values over the cluster Previously the Properties UI screens could only tell you the values held within the database and not the value that a node was actually using. The Properties screens have been improved to tell you the source of a property value and where multiple values exist across the cluster, which nodes have what values. See Properties.\nValidation of Configuration Property Values Validation of configuration property values is now possible. The validation rules are defined in the application code and allow for things like:\n Ensuring that a regex pattern is a valid pattern Setting maximum or minimum values to numeric properties. Ensuring a property has a value.  Validation will be enforced on application boot or when a value is edited via the UI.\nHot Loading of Node Configuration Now that node specific configuration is managed via the YAML configuration file stroom will detect changes to this file and update the configuration properties accordingly. Some properties however do not support being changed at runtime so will still require either the whole system or the UI nodes to be restarted.\nData retention impact summary The Data_Retention screen now provides an Impact Summary tab that will show you a summary of what will be deleted by the current active rules. The summary is based on the rules as they currently are in the UI, so it allows you to see the impact before saving rule changes. The summary is a count of the number of streams that will be deleted by each rule, broken down by feed and stream type. In very large systems with a lot of data or where complex rules are in place the summary may take a some time (minutes) to produce.\nSee Data Retention for more details.\nFuzzy Finding in Quick Filters and Suggestion Text Fields A richer fuzzy find algorithm has been added to the Quick filter search fields. It has also been added to some text input fields with suggestion fields, e.g. Feed Name input fields. This makes finding values or rows in a table faster and more precise.\nSee Finding Things for more details.\nNew (off-heap) memory efficient reference data The reference data feature in previous versions of stroom loaded the reference data on demand and held it in Java’s heap memory. In large systems or where a pipeline doing reference data lookups across a wide time range this can lead to very large heap sizes.\nIn v7 stroom now uses an off-heap, disk backed store (LMDB) for the reference data. This removes all (with the exception of context lookups) from the Java heap, so the -Xmx value can be reduced. In large systems this can mean keeping your -Xmx value below the 32Gb threshold to further reduce the memory usage. Because the store is disk backed frequently used reference data can be kept in the store to reduce the loading overhead. As the reference data is held off-heap it stroom can make use of all available free RAM for the reference data.\nSee Reference Data\nReference Data API A RESTful API has been added for the reference data store. This primarily allows reference lookups to be performed by external systems.\nSee Reference Data API\nText editor improvements The Ace text editor is used widely in Stroom for such things as editing XSLTs, editing dashboard column expressions, viewing stream data and stepping. There have been a number of improvements to this editor.\nSee Editing and Viewing Text Data\nEditor context menu Additional options have been added to the context menu in the text editor:\n Toggle soft line wrapping of long lines. Toggle viewing hidden characters, e.g. tabs, spaces, line breaks. Toggle Vim key bindings. The Ace editor does not implement all Vim functionality but supports the core key bindings. Toggle auto-completion. Completion is triggered using ctrl-space. Toggle live auto-completion. Completion is triggered as you type. Toggle the inclusion of snippets in the auto-complete suggestions.  Auto-completion and snippets Most editor screens now support basic auto-completion of existing words found in the text. Some editor screens, such as XSLT, dashboard column expressions and Javascript scripts also support keyword and snippet completion.\nData viewing improvements The way data is viewed in Stroom has changed to improve the viewing of large files or files with no line breaks. Previously a set number of lines of data would be fetched for display on the page in the Data Viewer. This did not work for data that has no line breaks as Stroom would then try to fetch all data.\nIn v7 Stroom works at the character level so can fetch a reasonable number of characters for display whether they are all one line or spread over multiple lines.\nThe viewing of data has been separated into two mechanisms, Data Preview and Source View.\nSee Editing and Viewing Text Data\nData Preview This is the default view of the data. It displays the first n characters (configurable) of the data. It will attempt the format the data, e.g. showing pretty-printed XML. You cannot navigate around the data.\nSource View This view is intended for seeing the actual data in its raw un-formatted form and for navigating around it. This view provides navigation controls to define the range of data being display, e.g. from a character offset, line number or line and column.\nYou can now query data, server tasks and processing tasks on dashboards TODO Complete this section  Data actions such as delete, download, reprocess now provide an impact summary before proceeding. TODO Complete this section  Index volume groups for easier index volume assignment TODO Complete this section  Kafka Integration New Kafka Configuration Entity Integration with Apache Kafka was introduced in v6 however the way the connection to Kafka cluster(s) is configured has been improved. We have introduced a new entity type called Kafka Configuration that can be created/managed via the explorer tree. This means stroom can integrate with many Kafka clusters or connect to a cluster using different sets of Kafka Configuration properties. The Kafka Configuration entity provides an editor for setting all the Kafka specific configuration properties. Pipeline elements that use Kafka now provide a means to select the Kafka Configuration to use.\nTODO Add user guide section on Kafka configuration  An Improved Pipeline Element for Sending Data to Kafka The previous Kafka pipeline elements in v6 have been replaced with a single StandardKafkaProducer element. The new element allows for the dynamic construction of a Kafka Producer message via an XML document conforming to the kafka-records XmlSchema. With this new element events can be translated into kafka records which will be then given to the Kafka Producer to send to the Kafka Cluster. This allows for complete control of things like timestamps, topics, keys, values, etc.\nTODO Add user guide section on Kafka Standard Producer  No limitations on data reprocessing TODO Complete this section  Improved REST API A rich REST API for all UI accessible functions The architecture of the stroom UI has been changed such that all communication between the UI and the back end is via REST calls. This means all of these REST calls are available as an API for users of stroom to take advantage of. It opens up the possibility for interacting with stoom via scripts or from other applications.\nSwagger UI to document REST API methods The Swagger UI and specification file have been improved to include all of the API methods available in stroom.\nImproved architecture with separate modules with individual DB access to spread load. The architecture of the core stroom application has been fundamentally changed in v7 to internally break up the application into its functional areas. This separation makes for a more logical code base and allows for the possibility of each functional area having its own database instance, if required.\nJava 12 stroom v7 now runs on the Java 12 JVM.\nMySQL 8 support. stroom v7 has been changed to support MySQL v8, opening up the possibility of using features like group replication.\n","categories":"","description":"Key new features and changes present in v7.0 of Stroom and Stroom-Proxy.\n","excerpt":"Key new features and changes present in v7.0 of Stroom and …","ref":"/stroom-docs/hugo-docsy/news/releases/v07.00/","tags":"","title":"Version 7.0"},{"body":"This is old news!\n","categories":"","description":"","excerpt":"This is old news!\n","ref":"/stroom-docs/hugo-docsy/news/news/20210707-old-news/","tags":"","title":"Old News"},{"body":" Version Information: Created with Stroom v7.0\nLast Updated: 2021-06-23\n Stroom and its associated services can be deployed in may ways (single node docker stack, non-docker cluster, kubernetes, etc). This document will cover two types of deployment:\n Single node stroom_core docker stack. A mixed deployment with nginx in docker and stroom, stroom-proxy and the database not in docker.  This document will explain how each application/service is configured and where its configuration files live.\nApplication Configuration The following sections provide links to how to configure each application.\n  Stroom Configuration\n  Stroom Proxy Configuration\n  MySQL Configuration\n  Nginx Configuration\n  Stroom log sender Configuration\n  General configuration of docker stacks Environment variables The stroom docker stacks have a single env file \u003cstack name\u003e.env that acts as a single point to configure some aspects of the stack. Setting values in the env file can be useful when the value is shared between multiple containers. This env file sets environment variables that are then used for variable substitution in the docker compose YAML files, e.g.\nenvironment: - MYSQL_ROOT_PASSWORD=${STROOM_DB_ROOT_PASSWORD:-my-secret-pw}  In this example the environment variable STROOM_DB_ROOT_PASSWORD is read and used to set the environment variable MYSQL_ROOT_PASSWORD in the docker container. If STROOM_DB_ROOT_PASSWORD is not set then the value my-secret-pw is used instead.\nThe environment variables set in the env file are NOT automatically visible inside the containers. Only those environment variables defined in the environment section of the docker-compose YAML files are visible. These environment entries can either be hard coded values or use environment variables from outside the container. In some case the names in the env file and the names of the environment variables set in the containers are the same, in some they are different.\nThe environment variables set in the containers can then be used by the application running in each container to set its configuration. For example, stroom’s config.yml file also uses variable substitution, e.g.\nappConfig: commonDbDetails: connection: jdbcDriverClassName: \"${STROOM_JDBC_DRIVER_CLASS_NAME:-com.mysql.cj.jdbc.Driver}\"  In this example jdbcDriverUrl will be set to the value of environment variable STROOM_JDBC_DRIVER_CLASS_NAME or com.mysql.cj.jdbc.Driver if that is not set.\nThe following example shows how setting MY_ENV_VAR=123 means myProperty will ultimately get a value of 123 and not its default of 789.\nenv file (stroom\u003cstack name\u003e.env) - MY_ENV_VAR=123 | | | environment variable substitution | v docker compose YAML (01_stroom.yml) - STROOM_ENV_VAR=${MY_ENV_VAR:-456} | | | environment variable substitution | v Stroom configuration file (config.yml) - myProperty: \"${STROOM_ENV_VAR:-789}\"  Note that environment variables are only set into the container on start. Any changes to the env file will not take effect until the container is (re)started.\nConfiguration files The following shows the basic structure of a stack with respect to the location of the configuration files:\n── stroom_core_test-vX.Y.Z ├── config [stack env file and docker compose YAML files] └── volumes └── \u003cservice\u003e └── conf/config [service specifc configuration files]  Some aspects of configuration do not lend themselves to environment variable substitution, e.g. deeply nested parts of stroom’s config.yml. In these instances it may be necessary to have static configuration files that have no connection to the env file or only use environment variables for some values.\nBind mounts Everything in the stack volumes directory is bind-mounted into the named docker container but is mounted read-only to the container. This allows configuration files to be read by the container but not modified.\nTypically the bind mounts mount a directory into the container, though in the case of the stroom-all-dbs.cnf file, the file is mounted. The mounts are done using the inode of the file/directory rather than the name, so docker will mount whatever the inode points to even if the name changes. If for instance the stroom-all-dbs.cnf file is renamed to stroom-all-dbs.cnf.old then copied to stroom-all-dbs.cnf and then the new version modified, the container would still see the old file.\nDocker managed volumes When stroom is running various forms of data are persisted, e.g. stroom’s stream store, stroom-all-dbs database files, etc. All this data is stored in docker managed volumes. By default these will be located in /var/lib/docker/volumes/\u003cvolume name\u003e/_data and root/sudo access will be needed to access these directories.\nDocker data root  IMPORTANT\n By default Docker stores all its images, container layers and managed volumes in its default data root directory which defaults to /var/lib/docker. It is typical in server deployments for the root file system to be kept fairly small and this is likely to result in the root file system running out of space due to the growth in docker images/layers/volumes in /var/lib/docker. It is therefore strongly recommended to move the docker data root to another location with more space.\nThere are various options for achieving this. In all cases the docker daemon should be stopped prior to making the changes, e.g. service docker stop, then started afterwards.\n  Symlink - One option is to move the var/lib/docker directory to a new location then create a symlink to it. For example:\nln -s /large_mount/docker_data_root /var/lib/docker  This has the advantage that anyone unaware that the data root has moved will be able to easily find it if they look in the default location.\n  Configuration - The location can be changed by adding this key to the file /etc/docker/daemon.json (or creating this file if it doesn’t exist.\n{ \"data-root\": \"/mnt/docker\" }    Mount - If your intention is to use a whole storage device for the docker data root then you can mount that device to /var/lib/docker. You will need to make a copy of the /var/lib/docker directory prior to doing this then copy it mount once created. The process for setting up this mount will be OS dependent and is outside the scope of this document.\n  Active services Each stroom docker stack comes pre-built with a number of different services, e.g. the stroom_core stack contains the following:\n stroom stroom-proxy-local stroom-all-dbs nginx stroom-log-sender  While you can pass a set of service names to the commands like start.sh and stop.sh, it may sometimes be required to configure the stack instance to only have a set of services active. You can set the active services like so:\n./set_services.sh stroom stroom-all-dbs nginx  In the above example and subsequent use of commands like start.sh and stop.sh with no named services would only act upon the active services set by set_services.sh. This list of active services is held in ACTIVE_SERVICES.txt and the full list of available services is held in ALL_SERVICES.txt.\nCertificates A number of the services in the docker stacks will make use of SSL certificates/keys in various forms. The certificate/key files are typically found in the directories volumes/\u003cservice\u003e/certs/.\nThe stacks come with a set of client/server certificates that can be used for demo/test purposes. For production deployments these should be replaced with the actual certificates/keys for your environment.\nIn general the best approach to configuring the certificates/keys is to replace the existing files with symlinks to the actual files. For example in the case of the server certificates for nginx (found in volumes/nginx/certs/) the directory would look like:\nca.pem.crt -\u003e /some/path/to/certificate_authority.pem.crt server.pem.crt -\u003e /some/path/to/host123.pem.crt server.unencrypted.key -\u003e /some/path/to/host123.key  This approach avoids the need to change any configuration files to reference differently named certificate/key files and avoids having to copy your real certificates/keys into multiple places.\nFor examples of how to create certificates, keys and keystores see creatCerts.sh (external link)\n","categories":"","description":"","excerpt":" Version Information: Created with Stroom v7.0\nLast Updated: …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/configuration/","tags":"","title":"Configuration"},{"body":" Version Information: Created with Stroom v7.0\nLast Updated: 2021-06-23\nSee Also: Properties.\n General configuration The Stroom application is essentially just an executable JAR (external link) file that can be run when provided with a configuration file, config.yml. This config file is common to all forms of deployment.\nconfig.yml This file, sometimes known as the DropWizard configuration file (as DropWizard is the java framework on which Stroom runs) is the primary means of configuring stroom. As a minimum this file should be used to configure anything that needs to be set before stroom can start up, e.g. database connection details or is specific to a node in a stroom cluster. If you are using some form of scripted deployment, e.g. ansible then it can be used to set all stroom properties for the environment that stroom runs in. If you are not using scripted deployments then you can maintain stroom’s node agnostic configuration properties via the user interface.\nFor more details on the structure of the file, data types and property precedence see Properties.\nStroom operates on a configuration by exception basis so all configuration properties will have a sensible default value and a property only needs to be explicitly configured if the default value is not appropriate, e.g. for tuning a large scale production deployment or where values are environment specific. As a result config.yml only contains a minimal set of properties. The full tree of properties can be seen in ./config/config-defaults.yml and a schema for the configuration tree (along with descriptions for each property) can be found in ./config/config-schema.yml. These two files can be used as a reference when configuring stroom.\nKey Configuration Properties The following are key properties that would typically be changed for a production deployment. All configuration branches are relative to the appConfig root.\nThe database name(s), hostname(s), port(s), usernames(s) and password(s) should be configured using these properties. Typically stroom is configured to keep it statistics data in a separate database to the main stroom database, as is configured below.\ncommonDbDetails: connection: jdbcDriverUrl: \"jdbc:mysql://localhost:3307/stroom?useUnicode=yes\u0026characterEncoding=UTF-8\" jdbcDriverUsername: \"stroomuser\" jdbcDriverPassword: \"stroompassword1\" statistics: sql: db: connection: jdbcDriverUrl: \"jdbc:mysql://localhost:3307/stats?useUnicode=yes\u0026characterEncoding=UTF-8\" jdbcDriverUsername: \"statsuser\" jdbcDriverPassword: \"stroompassword1\"  In a clustered deployment each node must be given a node name that is unique within the cluster. This is used to identify nodes in the Nodes screen. It could be the hostname of the node or follow some other naming convetion.\nnode: name: \"node1a\"  Each node should have its identity on the network configured so that it uses the appropriate FQDNs. The nodeUri hostname is the FQDN of each node and used by nodes to communicate with each other, therefore it can be private to the cluster of nodes. The publicUri hostname is the public facing FQDN for stroom, i.e. the address of a load balancer or Nginx. This is the address that users will use in their browser.\nnodeUri: hostname: \"localhost\" # e.g. node5.stroomnodes.somedomain publicUri: hostname: \"localhost\" # e.g. stroom.somedomain  Deploying without Docker Stroom running without docker has two files to configure it. The following locations are relative to the stroom home directory, i.e. the root of the distribution zip.\n ./config/config.yml - Stroom configuration YAML file ./config/scripts.env - Stroom scripts configuration env file  The distribution also includes these files which are helpful when it comes to configuring stroom.\n ./config/config-defaults.yml - Full version of the config.yml file containing all branches/leaves with default values set. Useful as a reference for the structure and the default values. ./config/config-schema.yml - The schema defining the structure of the config.yml file.  scripts.env This file is used by the various shell scripts like start.sh, stop.sh, etc. This file should not need to be unless you want to change the locations where certain log files are written to or need to change the java memory settings.\nIn a production system it is highly likely that you will need to increase the java heap size as the default is only 2G. The heap size settings and any other java command line options can be set by changing:\nJAVA_OPTS=\"-Xms512m -Xmx2048m\"  As part of a docker stack When stroom is run as part of one of our docker stacks, e.g. stroom_core there are some additional layers of configuration to take into account, but the configuration is still primarily done using the config.yml file.\nStroom’s config.yml file is found in the stack in ./volumes/stroom/config/ and this is the primary means of configuring Stroom.\nThe stack also ships with a default config.yml file baked into the docker image. This minimal fallback file (located in /stroom/config-fallback/ inside the container) will be used in the absence of one provided in the docker stack configuration (./volumes/stroom/config/).\nThe default config.yml file uses environment variable substitution so some configuration items will be set by environment variables set into the container by the stack env file and the docker-compose YAML. This approach is useful for configuration values that need to be used by multiple containers, e.g. the public FQDN of Nginx, so it can be configured in one place.\nIf you need to further customise the stroom configuration then it is recommended to edit the ./volumes/stroom/config/config.yml file. This can either be a simple file with hard coded values or one that uses environment variables for some of its configuration items.\nThe configuration works as follows:\nenv file (stroom\u003cstack name\u003e.env) | | | environment variable substitution | v docker compose YAML (01_stroom.yml) | | | environment variable substitution | v Stroom configuration file (config.yml)  Ansible If you are using Ansible to deploy a stack then it is recommended that all of stroom’s configuration properties are set directly in the config.yml file using a templated version of the file and to NOT use any environment variable substitution. When using Ansible, the Ansible inventory is the single source of truth for your configuration so not using environment variable substitution for stroom simplifies the configuration and makes it clearer when looking at deployed configuration files.\nStroom-ansible has an example inventory for a single node stroom stack deployment. The group_vars/all file shows how values can be set into the env file.\n","categories":"","description":"","excerpt":" Version Information: Created with Stroom v7.0\nLast Updated: …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/configuration/configuring-stroom/","tags":"","title":"Stroom Configuration"},{"body":" Version Information: Created with Stroom v7.0\nLast Updated: 2021-06-23\nSee Also: Stroom Application Configuration\nSee Also: Properties.\nTODO: This needs updating for v7.1\n The configuration of Stroom-proxy is very much the same as for Stroom with the only difference being the structure of the config.yml file. Stroom-proxy has a proxyConfig key in the YAML while Stroom has appConfig. It is recommended to first read Stroom Application Configuration to understand the general mechanics of the stroom configuration as this will largely apply to stroom-proxy.\nGeneral configuration The Stroom-proxy application is essentially just an executable JAR (external link) file that can be run when provided with a configuration file, config.yml. This configuration file is common to all forms of deployment.\nconfig.yml Stroom-proxy does not have a user interface so the config.yml file is the only way of configuring stroom-proxy. As with stroom, the config.yml file is split into three sections using these keys:\n server - Configuration of the web server, e.g. ports, paths, request logging. logging - Configuration of application logging proxyConfig - Configuration of stroom-proxy  See also Properties for more details on structure of the config.yml file and supported data types.\nStroom-proxy operates on a configuration by exception basis so all configuration properties will have a sensible default value and a property only needs to be explicitly configured if the default value is not appropriate, e.g. for tuning a large scale production deployment or where values are environment specific. As a result config.yml only contains a minimal set of properties. The full tree of properties can be seen in ./config/config-defaults.yml and a schema for the configuration tree (along with descriptions for each property) can be found in ./config/config-schema.yml. These two files can be used as a reference when configuring stroom.\nKey Configuration Properties Stroom-proxy has two main functions, storing and forwarding. It can be configured to do either or both of these functions. These functions are enabled/disabled using:\nproxyConfig: forwardStreamConfig: forwardingEnabled: true proxyRepositoryConfig: storingEnabled: true  Stroom-proxy should be configured to check the receipt status of feeds on receipt of data. This is done by configuring the end point of a downstream stroom-proxy or stroom.\nfeedStatus: url: \"http://stroom:8080/api/feedStatus/v1\" apiKey: \"\"  The url should be the url for the feed status API on the downstream stroom(-proxy). If this is on the same host then you can use the http endpoint, however if it is on a remote host then you should use https and the host of its nginx, e.g. https://downstream-instance/api/feedStatus/v1.\nIn order to use the API, the proxy must have a configured apiKey. The API key must be created in the downstream stroom instance and then copied into this configuration.\nIf the proxy is configured to forward data then the forward destination(s) should be set. This is the datafeed endpoint of the downstream stroom-proxy or stroom instance that data will be forwarded to. This may also be te address of a load balancer or similar that is fronting a cluster of stroom-proxy or stroom instances. See also Feed status certificate configuration.\nforwardStreamConfig: forwardDestinations: - forwardUrl: \"https://nginx/stroom/datafeed\"  forwardUrl specifies the URL of the datafeed endpoint on the destination host. Each forward location can use a different key/trust store pair. See also Forwarding certificate configuration.\nIf the proxy is configured to store then it is the location of the proxy repository may need to be configured if it needs to be in a different location to the proxy home directory, e.g. on another mount point.\nDeploying without Docker Apart from the structure of the config.yml file, the configuration in a non-docker environment is the same as for stroom\nAs part of a docker stack The way stroom-proxy is configured is essentially the same as for stroom with the only real difference being the structure of the config.yml file as note above . As with stroom the docker stack comes with a ./volumes/stroom-proxy-*/config/config.yml file that will be used in the absence of a provided one. Also as with stroom, the config.yml file supports environment variable substitution so can make use of environment variables set in the stack env file and passed down via the docker-compose YAML files.\nCertificates Stroom-proxy makes use of client certificates for two purposes:\n Communicating with a downstream stroom/stroom-proxy in order to establish the receipt status for the feeds it has received data for. When forwarding data to a downstream stroom/stroom-proxy  The stack comes with the following files that can be used for demo/test purposes.\nvolumes/stroom-proxy-*/certs/ca.jks volumes/stroom-proxy-*/certs/client.jks  For a production deployment these will need to be changed, see Certificates\nFeed status certificate configuration The configuration of the client certificates for feed status checks is done using:\nproxyConfig: jerseyClient: timeout: \"10s\" connectionTimeout: \"10s\" timeToLive: \"1h\" cookiesEnabled: false maxConnections: 1024 maxConnectionsPerRoute: \"1024\" keepAlive: \"0ms\" retries: 0 tls: verifyHostname: true keyStorePath: \"/stroom-proxy/certs/client.jks\" keyStorePassword: \"password\" keyStoreType: \"JKS\" trustStorePath: \"/stroom-proxy/certs/ca.jks\" trustStorePassword: \"password\" trustStoreType: \"JKS\" trustSelfSignedCertificates: false  This configuration is also used for making any other REST API calls.\nForwarding certificate configuration Stroom-proxy can forward to multiple locations. The configuration of the certificate(s) for the forwarding locations is as follows:\nproxyConfig: forwardStreamConfig: forwardingEnabled: true forwardDestinations: # If you want multiple forward destinations then you will need to edit this file directly # instead of using env var substitution - forwardUrl: \"https://nginx/stroom/datafeed\" sslConfig: keyStorePath: \"/stroom-proxy/certs/client.jks\" keyStorePassword: \"password\" keyStoreType: \"JKS\" trustStorePath: \"/stroom-proxy/certs/ca.jks\" trustStorePassword: \"password\" trustStoreType: \"JKS\" hostnameVerificationEnabled: true  forwardUrl specifies the URL of the datafeed endpoint on the destination host. Each forward location can use a different key/trust store pair.\n","categories":"","description":"","excerpt":" Version Information: Created with Stroom v7.0\nLast Updated: …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/configuration/configuring-stroom-proxy/","tags":["proxy"],"title":"Stroom Proxy Configuration"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/log-sender/","tags":"","title":"log-sender"},{"body":" Version Information: Created with Stroom v7.0\nLast Updated: 2021-06-14\n Stroom log sender is a docker image used for sending application logs to stroom. It is essentially just a combination of the send_to_stroom.sh (external link) script and a set of crontab entries to call the script at intervals.\nDeploying without Docker When deploying without docker stroom and stroom-proxy nodes will need to be configured to send their logs to stroom. This can be done using the ./bin/send_to_stroom.sh script in the stroom and stroom-proxy zip distributions and some crontab configuration.\nThe crontab file for the user account running stroom should be edited (crontab -e) and set to something like:\n# stroom logs * * * * * STROOM_HOME=\u003cpath to stroom home\u003e ${STROOM_HOME}/bin/send_to_stroom.sh ${STROOM_HOME}/logs/access STROOM-ACCESS-EVENTS \u003cdatafeed URL\u003e --system STROOM --environment \u003cenvironment\u003e --file-regex '.*/[a-z]+-[0-9]{4}-[0-9]{2}-[0-9]{2}T.*\\\\.log' --max-sleep 10 --key \u003ckey file\u003e --cert \u003ccert file\u003e --cacert \u003cCA cert file\u003e --delete-after-sending --compress \u003e\u003e \u003cpath to log\u003e 2\u003e\u00261 * * * * * STROOM_HOME=\u003cpath to stroom home\u003e ${STROOM_HOME}/bin/send_to_stroom.sh ${STROOM_HOME}/logs/app STROOM-APP-EVENTS \u003cdatafeed URL\u003e --system STROOM --environment \u003cenvironment\u003e --file-regex '.*/[a-z]+-[0-9]{4}-[0-9]{2}-[0-9]{2}T.*\\\\.log' --max-sleep 10 --key \u003ckey file\u003e --cert \u003ccert file\u003e --cacert \u003cCA cert file\u003e --delete-after-sending --compress \u003e\u003e \u003cpath to log\u003e 2\u003e\u00261 * * * * * STROOM_HOME=\u003cpath to stroom home\u003e ${STROOM_HOME}/bin/send_to_stroom.sh ${STROOM_HOME}/logs/user STROOM-USER-EVENTS \u003cdatafeed URL\u003e --system STROOM --environment \u003cenvironment\u003e --file-regex '.*/[a-z]+-[0-9]{4}-[0-9]{2}-[0-9]{2}T.*\\\\.log' --max-sleep 10 --key \u003ckey file\u003e --cert \u003ccert file\u003e --cacert \u003cCA cert file\u003e --delete-after-sending --compress \u003e\u003e \u003cpath to log\u003e 2\u003e\u00261 # stroom-proxy logs * * * * * PROXY_HOME=\u003cpath to proxy home\u003e ${PROXY_HOME}/bin/send_to_stroom.sh ${PROXY_HOME}/logs/access STROOM_PROXY-ACCESS-EVENTS \u003cdatafeed URL\u003e --system STROOM-PROXY --environment \u003cenvironment\u003e --file-regex '.*/[a-z]+-[0-9]{4}-[0-9]{2}-[0-9]{2}T.*\\\\.log' --max-sleep 10 --key \u003ckey file\u003e --cert \u003ccert file\u003e --cacert \u003cCA cert file\u003e --delete-after-sending --compress \u003e\u003e \u003cpath to log\u003e 2\u003e\u00261 * * * * * PROXY_HOME=\u003cpath to proxy home\u003e ${PROXY_HOME}/bin/send_to_stroom.sh ${PROXY_HOME}/logs/app STROOM_PROXY-APP-EVENTS \u003cdatafeed URL\u003e --system STROOM-PROXY --environment \u003cenvironment\u003e --file-regex '.*/[a-z]+-[0-9]{4}-[0-9]{2}-[0-9]{2}T.*\\\\.log' --max-sleep 10 --key \u003ckey file\u003e --cert \u003ccert file\u003e --cacert \u003cCA cert file\u003e --delete-after-sending --compress \u003e\u003e \u003cpath to log\u003e 2\u003e\u00261 * * * * * PROXY_HOME=\u003cpath to proxy home\u003e ${PROXY_HOME}/bin/send_to_stroom.sh ${PROXY_HOME}/logs/send STROOM_PROXY-SEND-EVENTS \u003cdatafeed URL\u003e --system STROOM-PROXY --environment \u003cenvironment\u003e --file-regex '.*/[a-z]+-[0-9]{4}-[0-9]{2}-[0-9]{2}T.*\\\\.log' --max-sleep 10 --key \u003ckey file\u003e --cert \u003ccert file\u003e --cacert \u003cCA cert file\u003e --delete-after-sending --compress \u003e\u003e \u003cpath to log\u003e 2\u003e\u00261 * * * * * PROXY_HOME=\u003cpath to proxy home\u003e ${PROXY_HOME}/bin/send_to_stroom.sh ${PROXY_HOME}/logs/receive STROOM_PROXY-RECEIVE-EVENTS \u003cdatafeed URL\u003e --system STROOM-PROXY --environment \u003cenvironment\u003e --file-regex '.*/[a-z]+-[0-9]{4}-[0-9]{2}-[0-9]{2}T.*\\\\.log' --max-sleep 10 --key \u003ckey file\u003e --cert \u003ccert file\u003e --cacert \u003cCA cert file\u003e --delete-after-sending --compress \u003e\u003e \u003cpath to log\u003e 2\u003e\u00261  where the environment specific values are:\n \u003cpath to stroom home\u003e - The absolute path to the stroom home, i.e. the location of the start.sh script. \u003cpath to proxy home\u003e - The absolute path to the stroom-proxy home, i.e. the location of the start.sh script. \u003cdatafeed URL\u003e - The URL that the logs will be sent to. This will typically be the nginx host or load balancer and the path will typically be https://host/datafeeddirect to bypass the proxy for faster access to the logs. \u003cenvironment\u003e - The environment name that the stroom/proxy is deployed in, e.g. OPS, REF, DEV, etc. \u003ckey file\u003e - The absolute path to the SSL key file used by curl. \u003ccert file\u003e - The absolute path to the SSL certificate file used by curl. \u003cCA cert file\u003e - The absolute path to the SSL certificate authority file used by curl. \u003cpath to log\u003e - The absolute path to a log file to log all the send_to_stroom.sh output to.  If your implementation of cron supports environment variables then you can define some of the common values at the top of the crontab file and use them in the entries. cronie as used by Centos does not support environment variables in the crontab file but variables can be defined at the line level as has been shown with STROOM_HOME and PROXY_HOME.\nThe above crontab entries assume that stroom and stroom-proxy are running on the same host. If there are not then the entries can be split across the hosts accordingly.\nService host(s) When deploying stroom/stroom-proxy without stroom you may still be deploying the service stack (nginx and stroom-log-sender) to a host. In this case see As part of a docker stack below for details of how to configure stroom-log-sender to send the nginx logs.\nAs part of a docker stack Crontab The docker stacks include the stroom-log-sender docker image for sending the logs of all the other containers to stroom. Stroom-log-sender is configured using the crontab file volumes/stroom-log-sender/conf/crontab.txt. When the container starts this file will be read. Any variables in it will be substituted with the values from the corresponding environment variables that are present in the container. These common values can be set in the config/\u003cstack name\u003e.env file.\nAs the variables are substituted on container start you will need to restart the container following any configuration change.\nCertificates The directory volumes/stroom-log-sender/certs contains the default client certificates used for the stack. These allow stroom-log-sender to send the log files over SSL which also provides stroom with details of the sender. These will need to be replaced in a production environment.\nvolumes/stroom-log-sender/certs/ca.pem.crt volumes/stroom-log-sender/certs/client.pem.crt volumes/stroom-log-sender/certs/client.unencrypted.key  For a production deployment these will need to be changed, see Certificates\n","categories":"","description":"","excerpt":" Version Information: Created with Stroom v7.0\nLast Updated: …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/configuration/configuring-stroom-log-sender/","tags":["log-sender"],"title":"Stroom Log Sender Configuration"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/mysql/","tags":"","title":"mysql"},{"body":" Version Information: Created with Stroom v7.0\nLast Updated: 2021-06-07\nSee Also: MySQL Server Setup\nSee Also: MySQL Server Administration (external link)\n General configuration MySQL is configured via the .cnf file which is typically located in one of these locations:\n /etc/my.cnf /etc/mysql/my.cnf $MYSQL_HOME/my.cnf \u003cdata dir\u003e/my.cnf ~/.my.cnf  Key configuration properties   lower_case_table_names - This proerty controls how the tables are stored on the filesystem and the case-sensitivity of table names in SQL. A value of 0 means tables are stored on the filesystem in the case used in CREATE TABLE and sql is case sensitive. This is the default in linux and is the preferred value for deployments of stroom of v7+. A value of 1 means tables are stored on the filesystem in lowercase but sql is case insensitive. See also (external link)\n  max_connections - The maximum permitted number of simultaneous client connections. For a clustered deployment of stroom, the default value of 151 will typically be too low. Each stroom node will hold a pool of open database connections for its use, therefore with a large number of stroom nodes and a big connection pool the total number of connections can be very large. This property should be set taking into account the values of the stroom properties of the form *.db.connectionPool.maxPoolSize. See also (external link)\n  innodb_buffer_pool_size/innodb_buffer_pool_instances - Controls the amount of memory availble to MySQL for caching table/index data. Typically this will be set to 80% of available RAM, assuming MySQL is running on a dedicated host and the total amount of table/index data is greater than 80% of avaialable RAM. Note: innodb_buffer_pool_size must be set to a value that is equal to or a multiple of innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances. See also (external link)\n   TODO - Add additional key configuration items\n Deploying without Docker When MySQL is deployed without a docker stack then MySQL should be installed and configured according to the MySQL documentation. How MySQL is deployed and configured will depend on the requirements of the environment, e.g. clustered, primary/standby, etc.\nAs part of a docker stack Where a stroom docker stack includes stroom-all-dbs (MySQL) the MySQL instance is configured via the .cnf file. The .cnf file is located in volumes/stroom-all-dbs/conf/stroom-all-dbs.cnf. This file is read-only to the container and will be read on container start.\nDatabase initialisation When the container is started for the first time the database be initialised with the root user account. It will also then run any scripts found in volumes/stroom-all-dbs/init/stroom. The scripts in here will be run in alpabetical order. Scripts of the form .sh, .sql, .sql.gz and .sql.template are supported.\n.sql.template files are proprietry to stroom stacks and are just templated .sql files. They can contain tags of the form \u003c\u003c\u003cENV_VAR_NAME\u003e\u003e\u003e which will be replaced with the value of the named environment variable that has been set in the container.\nIf you need to add additional database users then either add them to volumes/stroom-all-dbs/init/stroom/001_create_databases.sql.template or create additional scripts/templates in that directory.\nThe script that controls this templating is volumes/stroom-all-dbs/init/000_stroom_init.sh. This script MUST not have its executable bit set else it will be executed rather than being sourced by the MySQL entry point scripts and will then not work.\n","categories":"","description":"","excerpt":" Version Information: Created with Stroom v7.0\nLast Updated: …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/configuration/configuring-mysql/","tags":["mysql"],"title":"MySQL Configuration"},{"body":" Warning Before comencing an upgrade to v7 you should upgrade Stroom to the latest minor and patch version of v6.\n Differences between v6 and v7 Stroom v7 has significant differences to v6 which make the upgrade process a little more complicated.\n v6 handled authentication using a separate application, stroom-auth-service, with its own database. In v7 authentication is handled either internally in stroom (the default) or by an external identity provider such as google or AWS Cognito. v6 used a stroom.conf file or environment variables for configuration. In v7 stroom uses a config.yml file for its configuration (see Properties) v6 used upper case and heavily abbreviated names for its tables. In v7 clearer and lower case table names are used. As a result ALL v6 tables get renamed with the prefix OLD_, the new tables created and any content copied over. As the database will be holding two copies of most data you need to ensure you have space to accomodate it.  Pre-Upgrade tasks The following steps are required to be performed before migrating from v6 to v7.\nDownload migration scripts Download the migration SQL scripts from https://github.com/gchq/stroom/blob/STROOM_VERSION/scripts e.g. https://github.com/gchq/stroom/blob/v7.0-beta.133/scripts\nThese scripts will be used in the steps below.\nPre-migration database checks Run the pre-migration checks script on the running database.\ndocker exec -i stroom-all-dbs mysql --table -u\"stroomuser\" -p\"stroompassword1\" stroom \u003c v7_db_pre_migration_checks.sql  This will produce a report of items that will not be migrated or need attention before migration.\nStop processing Before shutting stroom down it is wise to turn off stream processing and let all outstanding server tasks complete.\nTODO clairfy steps for this.\nStop the stack Stop the stack (stroom and the database) then start up the database. Do this using the v6 stack. This ensures that stroom is not trying to access the database.\n./stop.sh ./start.sh stroom-all-dbs  Backup the databases Backup all the databases for the different components. Typically these will be stroom, stats and auth.\nIf you are running in a docker stack then you can run the ./backup_databases.sh script.\nStop the database Stop the database using the v6 stack.\n./stop.sh  Deploy and configure v7 Deploy the v7 stack. TODO - more detail\nVerify the database connection configuration for the stroom and stats databases. Ensure that there is NOT any configuration for a separate auth database as this will now be in stroom.\nRunning mysql_upgrade Stroom v6 ran on mysql v5.6. Stroom v7 runs on mysql v8. The upgrade path for MySQL is 5.6 =\u003e 5.7.33 =\u003e 8.x\nTo ensure the database is up to date mysql_upgrade neeeds to be run using the 5.7.33 binaries, see (external link).\nThis is the process for upgrading the database. All of these commands are using the v7 stack.\n# Set the version of the MySQL docker image to use export MYSQL_TAG=5.7.33 # Start MySQL at v5.7, this will recreate the container ./start.sh stroom-all-dbs # Run the upgrade from 5.6 =\u003e 5.7.33 docker exec -it stroom-all-dbs mysql_upgrade -u\"root\" -p\"my-secret-pw\" # Stop MySQL ./stop.sh # Unset the tag variable so that it now uses the default from the stack (8.x) unset MYSQL_TAG # Start MySQL at v8.x, this will recreate the container and run the upgrade from 5.7.33=\u003e8 ./start.sh stroom-all-dbs ./stop.sh  Rename legacy stroom-auth tables Run this command to connect to the auth database and run the pre-migration SQL script.\ndocker exec -i stroom-all-dbs mysql --table -u\"authuser\" -p\"stroompassword1\" auth \u003c v7_auth_db_table_rename.sql  This will rename all but one of the tables in the auth database.\nCopy the auth database content to stroom Having run the table rename perform another backup of just the auth database.\n./backup_databases.sh . auth  Now restore this backup into the stroom database. You can use the v7 stack scripts to do this.\n./restore_database.sh stroom auth_20210312143513.sql.gz  You should now see the following tables in the stroom database:\nOLD_AUTH_json_web_key OLD_AUTH_schema_version OLD_AUTH_token_types OLD_AUTH_tokens OLD_AUTH_users  This can be checked by running the following in the v7 stack.\necho 'select table_name from information_schema.tables where table_name like \"OLD_AUTH%\"' | ./database_shell.sh  Drop unused databases There may be a number of databases that are no longer used that can be dropped prior to the upgrade. Note the use of the --force argument so it copes with users that are not there.\ndocker exec -i stroom-all-dbs mysql --force -u\"root\" -p\"my-secret-pw\" \u003c v7_drop_unused_databases.sql  Verify it worked with:\necho 'show databases;' | docker exec -i stroom-all-dbs mysql -u\"root\" -p\"my-secret-pw\"  Performing the upgrade To perform the stroom schema upgrade to v7 run the migrate command which will migrate the database then exit. For a large upgrade like this is it is preferable to run the migrate command rather than just starting stroom as stroom will only migrate the parts of the schema as it needs to use them. Running migrate ensures all parts of the migration are completed when the command is run and no other parts of stroom will be started.\n./migrate.sh  Post-Upgrade tasks TODO remove auth* containers,images,volumes\n","categories":"","description":"This document describes the process for upgrading a Stroom single node docker stack from v6.x to v7.x.\n","excerpt":"This document describes the process for upgrading a Stroom single node …","ref":"/stroom-docs/hugo-docsy/docs/install-guide/upgrades/6_to_7_upgrade/","tags":"","title":"v6 to v7 Upgrade"},{"body":" For a detailed list of the changes in v6.1 see the changelog (external link)\n TODO Version 6.1 release notes need populating.  ","categories":"","description":"Key new features and changes present in v6.1 of Stroom and Stroom-Proxy.\n","excerpt":"Key new features and changes present in v6.1 of Stroom and …","ref":"/stroom-docs/hugo-docsy/news/releases/v06.01/","tags":"","title":"Version 6.1"},{"body":" For a detailed list of the changes in v6.0 see the changelog (external link)\n OAuth 2.0/OpenID Connect authentication Authentication for Stroom provided by an external service rather than a service internal to Stroom. This change allows support for broader corporate authentication schemes and is a key requirement for enabling the future microservice architecture for Stroom.\nAPI keys for third party clients Anyone wishing to make use of the data exposed by Stroom’s services can request an API key. This key acts as a password for their own applications. It allows administrators to secure and manage access to Stroom’s data.\nHBase backed statistics store This new implementation of statistics (Stroom-Stats) provides a vastly more scalable time series DB for large scale collection of Stroom’s data aggregated to various time buckets. Stroom-Stats uses Kafka for ingesting the source data.\nData receipt filtering Data arriving in Stroom has meta data that can be matched against a policy so that certain actions can be taken. This could be to receive, drop or reject the data.\nFiltering of data also applies to Stroom proxy where each proxy can get a filtering policy from an upstream proxy or a Stroom instance.\nData retention policies The length of time that data will be retained in Strooms stream store can be defined by creating data retention rules. These rules match streams based on their meta data and will automatically delete data once the retention period associated with the rule is exceeded.\nDashboard linking Links can be created in dashboards to jump to other dashboards or other external sites that provide additional contextual information.\nSearch API The search system used by Dashboards can be used via a restful API. This provides access to data stored in search indices (including the ability to extract data) and statistics stores. The data fetched via the search API can be received and processed via an external system.\nKafka appender and filter New pipeline elements for writing XML or text data to a Kafka topic. This provides more options for using Stroom’s data in other systems.\n","categories":"","description":"Key new features and changes present in v6.0 of Stroom and Stroom-Proxy.\n","excerpt":"Key new features and changes present in v6.0 of Stroom and …","ref":"/stroom-docs/hugo-docsy/news/releases/v06.00/","tags":"","title":"Version 6.0"},{"body":" This is a placeholder page. Replace it with your own content.\n Text can be bold, italic, or strikethrough. Links should be blue with no underlines (unless hovered over).\nThere should be whitespace between paragraphs. Vape migas chillwave sriracha poutine try-hard distillery. Tattooed shabby chic small batch, pabst art party heirloom letterpress air plant pop-up. Sustainable chia skateboard art party banjo cardigan normcore affogato vexillologist quinoa meggings man bun master cleanse shoreditch readymade. Yuccie prism four dollar toast tbh cardigan iPhone, tumblr listicle live-edge VHS. Pug lyft normcore hot chicken biodiesel, actually keffiyeh thundercats photo booth pour-over twee fam food truck microdosing banh mi. Vice activated charcoal raclette unicorn live-edge post-ironic. Heirloom vexillologist coloring book, beard deep v letterpress echo park humblebrag tilde.\n90’s four loko seitan photo booth gochujang freegan tumeric listicle fam ugh humblebrag. Bespoke leggings gastropub, biodiesel brunch pug fashion axe meh swag art party neutra deep v chia. Enamel pin fanny pack knausgaard tofu, artisan cronut hammock meditation occupy master cleanse chartreuse lumbersexual. Kombucha kogi viral truffaut synth distillery single-origin coffee ugh slow-carb marfa selfies. Pitchfork schlitz semiotics fanny pack, ugh artisan vegan vaporware hexagon. Polaroid fixie post-ironic venmo wolf ramps kale chips.\n There should be no margin above this first sentence.\nBlockquotes should be a lighter gray with a border along the left side in the secondary color.\nThere should be no margin below this final sentence.\n First Header 2 This is a normal paragraph following a header. Knausgaard kale chips snackwave microdosing cronut copper mug swag synth bitters letterpress glossier craft beer. Mumblecore bushwick authentic gochujang vegan chambray meditation jean shorts irony. Viral farm-to-table kale chips, pork belly palo santo distillery activated charcoal aesthetic jianbing air plant woke lomo VHS organic. Tattooed locavore succulents heirloom, small batch sriracha echo park DIY af. Shaman you probably haven’t heard of them copper mug, crucifix green juice vape single-origin coffee brunch actually. Mustache etsy vexillologist raclette authentic fam. Tousled beard humblebrag asymmetrical. I love turkey, I love my job, I love my friends, I love Chardonnay!\nDeae legum paulatimque terra, non vos mutata tacet: dic. Vocant docuique me plumas fila quin afuerunt copia haec o neque.\nOn big screens, paragraphs and headings should not take up the full container width, but we want tables, code blocks and similar to take the full width.\nScenester tumeric pickled, authentic crucifix post-ironic fam freegan VHS pork belly 8-bit yuccie PBR\u0026B. I love this life we live in.\nSecond Header 2  This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\n Header 3 This is a code block following a header.  Next level leggings before they sold out, PBR\u0026B church-key shaman echo park. Kale chips occupy godard whatever pop-up freegan pork belly selfies. Gastropub Belinda subway tile woke post-ironic seitan. Shabby chic man bun semiotics vape, chia messenger bag plaid cardigan.\nHeader 4  This is an unordered list following a header. This is an unordered list following a header. This is an unordered list following a header.  Header 5  This is an ordered list following a header. This is an ordered list following a header. This is an ordered list following a header.  Header 6    What Follows     A table A header   A table A header   A table A header     There’s a horizontal rule above and below this.\n Here is an unordered list:\n Liverpool F.C. Chelsea F.C. Manchester United F.C.  And an ordered list:\n Michael Brecker Seamus Blake Branford Marsalis  And an unordered task list:\n Create a Hugo theme Add task lists to it Take a vacation  And a “mixed” task list:\n Pack bags ? Travel!  And a nested list:\n Jackson 5  Michael Tito Jackie Marlon Jermaine   TMNT  Leonardo Michelangelo Donatello Raphael    Definition lists can be used with Markdown syntax. Definition headers are bold.\n Name Godzilla Born 1952 Birthplace Japan Color Green   Tables should have bold headings and alternating shaded rows.\n   Artist Album Year     Michael Jackson Thriller 1982   Prince Purple Rain 1984   Beastie Boys License to Ill 1986    If a table is too wide, it should scroll horizontally.\n   Artist Album Year Label Awards Songs     Michael Jackson Thriller 1982 Epic Records Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical Wanna Be Startin' Somethin', Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life   Prince Purple Rain 1984 Warner Brothers Records Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal Let’s Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I’m a Star, Purple Rain   Beastie Boys License to Ill 1986 Mercury Records noawardsbutthistablecelliswide Rhymin \u0026 Stealin, The New Style, She’s Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill     Code snippets like var foo = \"bar\"; can be shown inline.\nAlso, this should vertically align with this and this.\nCode can also be shown in a block element.\nfoo := \"bar\"; bar := \"foo\";  Code can also use syntax highlighting.\nfunc main() { input := `var foo = \"bar\";` lexer := lexers.Get(\"javascript\") iterator, _ := lexer.Tokenise(nil, input) style := styles.Get(\"github\") formatter := html.New(html.WithLineNumbers()) var buff bytes.Buffer formatter.Format(\u0026buff, style, iterator) fmt.Println(buff.String()) }  Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.  Inline code inside table cells should still be distinguishable.\n   Language Code     Javascript var foo = \"bar\";   Ruby foo = \"bar\"{     Small images should be shown at their actual size.\nLarge images should always scale down and fit in the content container.\nThe photo above of the Spruce Picea abies shoot with foliage buds: Bjørn Erik Pedersen, CC-BY-SA.\nComponents Alerts  This is an alert.  Note This is an alert with a title.  Note This is an alert with a title and Markdown.  This is a successful alert.  This is a warning.  Warning This is a warning with a title.  Another Heading Add some sections here to see how the ToC looks like. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nThis Document Inguina genus: Anaphen post: lingua violente voce suae meus aetate diversi. Orbis unam nec flammaeque status deam Silenum erat et a ferrea. Excitus rigidum ait: vestro et Herculis convicia: nitidae deseruit coniuge Proteaque adiciam eripitur? Sitim noceat signa probat quidem. Sua longis fugatis quidem genae.\nPixel Count Tilde photo booth wayfarers cliche lomo intelligentsia man braid kombucha vaporware farm-to-table mixtape portland. PBR\u0026B pickled cornhole ugh try-hard ethical subway tile. Fixie paleo intelligentsia pabst. Ennui waistcoat vinyl gochujang. Poutine salvia authentic affogato, chambray lumbersexual shabby chic.\nContact Info Plaid hell of cred microdosing, succulents tilde pour-over. Offal shabby chic 3 wolf moon blue bottle raw denim normcore poutine pork belly.\nExternal Links Stumptown PBR\u0026B keytar plaid street art, forage XOXO pitchfork selvage affogato green juice listicle pickled everyday carry hashtag. Organic sustainable letterpress sartorial scenester intelligentsia swag bushwick. Put a bird on it stumptown neutra locavore. IPhone typewriter messenger bag narwhal. Ennui cold-pressed seitan flannel keytar, single-origin coffee adaptogen occupy yuccie williamsburg chillwave shoreditch forage waistcoat.\nThis is the final element on the page and there should be no margin below this.  ","categories":"","description":"A short lead description about this content page. It can be **bold** or _italic_ and can be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. It can be **bold** …","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/getting-started/example-page/","tags":"","title":"Example Page"},{"body":" This is a placeholder page. Replace it with your own content.\n Text can be bold, italic, or strikethrough. Links should be blue with no underlines (unless hovered over).\nThere should be whitespace between paragraphs. Vape migas chillwave sriracha poutine try-hard distillery. Tattooed shabby chic small batch, pabst art party heirloom letterpress air plant pop-up. Sustainable chia skateboard art party banjo cardigan normcore affogato vexillologist quinoa meggings man bun master cleanse shoreditch readymade. Yuccie prism four dollar toast tbh cardigan iPhone, tumblr listicle live-edge VHS. Pug lyft normcore hot chicken biodiesel, actually keffiyeh thundercats photo booth pour-over twee fam food truck microdosing banh mi. Vice activated charcoal raclette unicorn live-edge post-ironic. Heirloom vexillologist coloring book, beard deep v letterpress echo park humblebrag tilde.\n90’s four loko seitan photo booth gochujang freegan tumeric listicle fam ugh humblebrag. Bespoke leggings gastropub, biodiesel brunch pug fashion axe meh swag art party neutra deep v chia. Enamel pin fanny pack knausgaard tofu, artisan cronut hammock meditation occupy master cleanse chartreuse lumbersexual. Kombucha kogi viral truffaut synth distillery single-origin coffee ugh slow-carb marfa selfies. Pitchfork schlitz semiotics fanny pack, ugh artisan vegan vaporware hexagon. Polaroid fixie post-ironic venmo wolf ramps kale chips.\n There should be no margin above this first sentence.\nBlockquotes should be a lighter gray with a border along the left side in the secondary color.\nThere should be no margin below this final sentence.\n First Header 2 This is a normal paragraph following a header. Knausgaard kale chips snackwave microdosing cronut copper mug swag synth bitters letterpress glossier craft beer. Mumblecore bushwick authentic gochujang vegan chambray meditation jean shorts irony. Viral farm-to-table kale chips, pork belly palo santo distillery activated charcoal aesthetic jianbing air plant woke lomo VHS organic. Tattooed locavore succulents heirloom, small batch sriracha echo park DIY af. Shaman you probably haven’t heard of them copper mug, crucifix green juice vape single-origin coffee brunch actually. Mustache etsy vexillologist raclette authentic fam. Tousled beard humblebrag asymmetrical. I love turkey, I love my job, I love my friends, I love Chardonnay!\nDeae legum paulatimque terra, non vos mutata tacet: dic. Vocant docuique me plumas fila quin afuerunt copia haec o neque.\nOn big screens, paragraphs and headings should not take up the full container width, but we want tables, code blocks and similar to take the full width.\nScenester tumeric pickled, authentic crucifix post-ironic fam freegan VHS pork belly 8-bit yuccie PBR\u0026B. I love this life we live in.\nSecond Header 2  This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\n Header 3 This is a code block following a header.  Next level leggings before they sold out, PBR\u0026B church-key shaman echo park. Kale chips occupy godard whatever pop-up freegan pork belly selfies. Gastropub Belinda subway tile woke post-ironic seitan. Shabby chic man bun semiotics vape, chia messenger bag plaid cardigan.\nHeader 4  This is an unordered list following a header. This is an unordered list following a header. This is an unordered list following a header.  Header 5  This is an ordered list following a header. This is an ordered list following a header. This is an ordered list following a header.  Header 6    What Follows     A table A header   A table A header   A table A header     There’s a horizontal rule above and below this.\n Here is an unordered list:\n Liverpool F.C. Chelsea F.C. Manchester United F.C.  And an ordered list:\n Michael Brecker Seamus Blake Branford Marsalis  And an unordered task list:\n Create a Hugo theme Add task lists to it Take a vacation  And a “mixed” task list:\n Pack bags ? Travel!  And a nested list:\n Jackson 5  Michael Tito Jackie Marlon Jermaine   TMNT  Leonardo Michelangelo Donatello Raphael    Definition lists can be used with Markdown syntax. Definition headers are bold.\n Name Godzilla Born 1952 Birthplace Japan Color Green   Tables should have bold headings and alternating shaded rows.\n   Artist Album Year     Michael Jackson Thriller 1982   Prince Purple Rain 1984   Beastie Boys License to Ill 1986    If a table is too wide, it should scroll horizontally.\n   Artist Album Year Label Awards Songs     Michael Jackson Thriller 1982 Epic Records Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical Wanna Be Startin' Somethin', Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life   Prince Purple Rain 1984 Warner Brothers Records Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal Let’s Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I’m a Star, Purple Rain   Beastie Boys License to Ill 1986 Mercury Records noawardsbutthistablecelliswide Rhymin \u0026 Stealin, The New Style, She’s Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill     Code snippets like var foo = \"bar\"; can be shown inline.\nAlso, this should vertically align with this and this.\nCode can also be shown in a block element.\nfoo := \"bar\"; bar := \"foo\";  Code can also use syntax highlighting.\nfunc main() { input := `var foo = \"bar\";` lexer := lexers.Get(\"javascript\") iterator, _ := lexer.Tokenise(nil, input) style := styles.Get(\"github\") formatter := html.New(html.WithLineNumbers()) var buff bytes.Buffer formatter.Format(\u0026buff, style, iterator) fmt.Println(buff.String()) }  Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.  Inline code inside table cells should still be distinguishable.\n   Language Code     Javascript var foo = \"bar\";   Ruby foo = \"bar\"{     Small images should be shown at their actual size.\nLarge images should always scale down and fit in the content container.\nThe photo above of the Spruce Picea abies shoot with foliage buds: Bjørn Erik Pedersen, CC-BY-SA.\nComponents Alerts  This is an alert.  Note This is an alert with a title.  Note This is an alert with a title and Markdown.  This is a successful alert.  This is a warning.  Warning This is a warning with a title.  Another Heading ","categories":"","description":"A short lead description about this content page. It can be **bold** or _italic_ and can be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. It can be **bold** …","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/reference/parameter-reference/","tags":"","title":"Parameter Reference"},{"body":"PUML Inline PUML @startuml Alice -\u003e Bob: Authentication Request Bob --\u003e Alice: Authentication Response Alice -\u003e Bob: Another authentication Request Alice \u003c-- Bob: Another authentication Response @enduml  SVG Local SVG Static SVG Page Resource This is some wordy text And so is this     PNG resized  SVG Figure   My PUML   Code highlighting YAML\n--- root: someKey: \"value\"  Bash\necho \"${VAR}\"  XML\n\u003croot\u003e \u003cchild attr=\"xxx\"\u003esome val\u003c/child\u003e \u003c/root\u003e  Links   PUML\n  Code Highlighting\n  Table    Data Type Example UI String Forms Example YAML form     Boolean true false true false   String This is a string \"This is a string\"   Integer/Long 123 123   Float 1.23 1.23   Stroom Duration P30D P1DT12H PT30S 30d 30s 30000 \"P30D\" \"P1DT12H\" \"PT30S\" \"30d\" \"30s\" \"30000\" See Stroom Duration Data Type.   List #red#Green#Blue ,1,2,3 See List Data Type   Map ,=red=FF0000,Green=00FF00,Blue=0000FF See Map Data Type   DocRef ,docRef(MyType,a56ff805-b214-4674-a7a7-a8fac288be60,My DocRef name) See DocRef Data Type   Enum HIGH LOW \"HIGH\" \"LOW\"   Path /some/path/to/a/file \"/some/path/to/a/file\"   ByteSize 32, 512Kib 32, 512Kib See Byte Size Data Type    ","categories":"","description":"A test for stroom\n","excerpt":"A test for stroom\n","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/getting-started/stroom_test/","tags":"","title":"Stroom Test Page"},{"body":" This is a placeholder page. Replace it with your own content.\n This is the section landing page.\n","categories":"","description":"A short lead description about this section page. Text here can also be **bold** or _italic_ and can even be split over multiple paragraphs.\n","excerpt":"A short lead description about this section page. Text here can also …","ref":"/stroom-docs/hugo-docsy/docs/docsy-examples/tasks/ponycopters/","tags":"","title":"Working with Ponycopters"},{"body":"  About Stroom Stroom is an application that was developed by the UK's Government Communication Headquarters for collecting, processing and analysing large volumes of log files. In 2016 it was made available to the open source community with an Apache 2.0 licence.    ","categories":"","description":"","excerpt":"  About Stroom Stroom is an application that was developed by the UK's …","ref":"/stroom-docs/hugo-docsy/about/","tags":"","title":"About Stroom"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/all-content/","tags":"","title":"All Content (DRAFT)"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/community/","tags":"","title":"Community"},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","categories":"","description":"","excerpt":"This is the blog section. It has two categories: News and Releases. …","ref":"/stroom-docs/hugo-docsy/news/","tags":"","title":"News / Releases"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/search/","tags":"","title":"Search Results"},{"body":"   Learn More   Download   Stroom is a data processing, storage and analysis platform. It is scalable - just add more CPUs / servers for greater throughput. It is suitable for processing high volume data such as system logs, to provide valuable insights into IT performance and usage.       Data Ingest  Receive and store large volumes of data such as native format logs. Ingested data is always available in its raw form.\n   Data transformation pipelines  Create sequences of XSL and text operations, in order to normalise or export data in any format. It is possible to enrich data using lookups and reference data.\n   Integrated transformation development  Easily add new data formats and debug the transformations if they don’t work as expected.\n   Scalable Search  Create multiple indexes with different retention periods. These can be sharded across your cluster.\n   Dashboards  Run queries against your indexes or statistics and view the results within custom visualisations.\n   Statistics  Record counts or values of items over time, providing answers to questions such as “how many times has a specific machine provided data in the last hour/day/month?”\n    ","categories":"","description":"","excerpt":"   Learn More   Download   Stroom is a data processing, storage and …","ref":"/stroom-docs/hugo-docsy/","tags":"","title":"Stroom"},{"body":"","categories":"","description":"","excerpt":"","ref":"/stroom-docs/hugo-docsy/tags/v5/","tags":"","title":"v5"}]